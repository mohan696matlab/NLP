{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d497a369bc454999bc4499aa86841fbb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "from IPython.display import  clear_output\n",
    "import time\n",
    "import PyPDF2\n",
    "from pathlib import Path\n",
    "from tqdm.auto import tqdm\n",
    "from typing import Optional\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "DEFAULT_MODEL = \"meta-llama/Llama-3.2-3B-Instruct\"\n",
    "\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    DEFAULT_MODEL,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    use_safetensors=True,\n",
    "    device_map=device,\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(DEFAULT_MODEL, use_safetensors=True)\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation = [\n",
    "    # {\"role\": \"system\", \"content\": '''respind to the user as if you are stewie from family guy'''},\n",
    "    {\"role\": \"user\", \"content\": '''who is mohan dash ?'''},\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|><|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "Cutting Knowledge Date: December 2023\n",
      "Today Date: 07 Mar 2025\n",
      "\n",
      "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "who is mohan dash?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "Mohan Das is a popular Indian YouTuber and content creator known for his educational and entertaining content, particularly in the fields of science, history, and philosophy. His real name is not widely known, and he prefers to keep his personal life private.\n",
      "\n",
      "Mohan Das is particularly famous for his channel \"Mohan Das\", where he creates in-depth, engaging, and informative videos on a wide range of topics, including:\n",
      "\n",
      "1. Science and technology: He explains complex scientific concepts\n"
     ]
    }
   ],
   "source": [
    "prompt = tokenizer.apply_chat_template(conversation, tokenize=False)\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "# print(prompt)\n",
    "\n",
    "with torch.no_grad():\n",
    "    output = model.generate(\n",
    "        **inputs,\n",
    "        do_sample=True,\n",
    "        max_new_tokens=100\n",
    "    )\n",
    "\n",
    "processed_text = tokenizer.decode(output[0], skip_special_tokens=False)\n",
    "\n",
    "print(processed_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing PDF with 10 pages...\n",
      "Processed page 1/10\n",
      "Processed page 2/10\n",
      "Processed page 3/10\n",
      "Processed page 4/10\n",
      "Processed page 5/10\n",
      "Processed page 6/10\n",
      "Processed page 7/10\n",
      "Processed page 8/10\n",
      "Processed page 9/10\n",
      "Processed page 10/10\n",
      "\n",
      "Extraction complete! Total characters: 42093\n"
     ]
    }
   ],
   "source": [
    "pdf_path = '/home/mohan.dash/sand particle counting/NLP/1506.02640v5.pdf'\n",
    "\n",
    "with open(pdf_path, 'rb') as file:\n",
    "    # Create PDF reader object\n",
    "    pdf_reader = PyPDF2.PdfReader(file)\n",
    "    \n",
    "    # Get total number of pages\n",
    "    num_pages = len(pdf_reader.pages)\n",
    "    print(f\"Processing PDF with {num_pages} pages...\")\n",
    "    \n",
    "    extracted_text = []\n",
    "    total_chars = 0\n",
    "    max_chars = 100000\n",
    "    \n",
    "    # Iterate through all pages\n",
    "    for page_num in range(num_pages):\n",
    "        # Extract text from page\n",
    "        page = pdf_reader.pages[page_num]\n",
    "        text = page.extract_text()\n",
    "        \n",
    "        # Check if adding this page's text would exceed the limit\n",
    "        if total_chars + len(text) > max_chars:\n",
    "            # Only add text up to the limit\n",
    "            remaining_chars = max_chars - total_chars\n",
    "            extracted_text.append(text[:remaining_chars])\n",
    "            print(f\"Reached {max_chars} character limit at page {page_num + 1}\")\n",
    "            break\n",
    "        \n",
    "        extracted_text.append(text)\n",
    "        total_chars += len(text)\n",
    "        print(f\"Processed page {page_num + 1}/{num_pages}\")\n",
    "    \n",
    "    final_text = '\\n'.join(extracted_text)\n",
    "    print(f\"\\nExtraction complete! Total characters: {len(final_text)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Extracted text has been saved to extracted_text.txt\n"
     ]
    }
   ],
   "source": [
    "if final_text:\n",
    "    output_file = 'extracted_text.txt'\n",
    "    with open(output_file, 'w', encoding='utf-8') as f:\n",
    "        f.write(final_text)\n",
    "    print(f\"\\nExtracted text has been saved to {output_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Llama Pre-Processing\n",
    "Now let's proceed to justify our distaste for writing regex and use that as a justification for a LLM instead:\n",
    "\n",
    "At this point, have a text file extracted from a PDF of a paper. Generally PDF extracts can be messy due to characters, formatting, Latex, Tables, etc.\n",
    "\n",
    "One way to handle this would be using regex, instead we can also prompt the feather light Llama models to clean up our text for us.\n",
    "\n",
    "Please try changing the SYS_PROMPT below to see what improvements you can make:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "SYS_PROMPT = \"\"\"\n",
    "You are a world class text pre-processor, here is the raw data from a PDF, please parse and return it in a way that is crispy and usable to send to a podcast writer.\n",
    "\n",
    "The raw data is messed up with new lines, Latex math and you will see fluff that we can remove completely. Basically take away any details that you think might be useless in a podcast author's transcript.\n",
    "\n",
    "Remember, the podcast could be on any topic whatsoever so the issues listed above are not exhaustive\n",
    "\n",
    "Please be smart with what you remove and be creative ok?\n",
    "\n",
    "Remember DO NOT START SUMMARIZING THIS, YOU ARE ONLY CLEANING UP THE TEXT AND RE-WRITING WHEN NEEDED\n",
    "\n",
    "Be very smart and aggressive with removing details, you will get a running portion of the text and keep returning the processed text.\n",
    "\n",
    "PLEASE DO NOT ADD MARKDOWN FORMATTING, STOP ADDING SPECIAL CHARACTERS THAT MARKDOWN CAPATILISATION ETC LIKES\n",
    "\n",
    "ALWAYS start your response directly with processed text and NO ACKNOWLEDGEMENTS about my questions ok?\n",
    "Here is the text:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_word_bounded_chunks(text, target_chunk_size):\n",
    "    \"\"\"\n",
    "    Split text into chunks at word boundaries close to the target chunk size.\n",
    "    \"\"\"\n",
    "    words = text.split()\n",
    "    chunks = []\n",
    "    current_chunk = []\n",
    "    current_length = 0\n",
    "    \n",
    "    for word in words:\n",
    "        word_length = len(word) + 1  # +1 for the space\n",
    "        if current_length + word_length > target_chunk_size and current_chunk:\n",
    "            # Join the current chunk and add it to chunks\n",
    "            chunks.append(' '.join(current_chunk))\n",
    "            current_chunk = [word]\n",
    "            current_length = word_length\n",
    "        else:\n",
    "            current_chunk.append(word)\n",
    "            current_length += word_length\n",
    "    \n",
    "    # Add the last chunk if it exists\n",
    "    if current_chunk:\n",
    "        chunks.append(' '.join(current_chunk))\n",
    "    \n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "43\n"
     ]
    }
   ],
   "source": [
    "CHUNK_SIZE = 1000  # Adjust chunk size if needed\n",
    "\n",
    "chunks = create_word_bounded_chunks(final_text, CHUNK_SIZE)\n",
    "print(len(chunks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INPUT TEXT:\n",
      "to current state-of-the-art methods. Finally, we show that YOLO generalizes to new domains better than other detectors on two artwork datasets. 4.1. Comparison to Other Real-Time Systems Many research efforts in object detection focus on mak- ing standard detection pipelines fast. [5] [38] [31] [14] [17] [28] However, only Sadeghi et al. actually produce a de- tection system that runs in real-time (30 frames per second or better) [31]. We compare YOLO to their GPU imple- mentation of DPM which r...\n",
      "\n",
      "PROCESSED TEXT:\n",
      "ors on two artwork datasets. Our system is the fastest object detection method on PASCAL, with a mAP of 52.7% and is more than twice as accurate as prior work on real-time detection. YOLO achieves a mAP of 63.4% while maintaining real-time performance....\n",
      "==========================================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "text_chunk = chunks[22]\n",
    "\n",
    "\n",
    "conversation = [\n",
    "    {\"role\": \"system\", \"content\": SYS_PROMPT},\n",
    "    {\"role\": \"user\", \"content\": text_chunk},\n",
    "]\n",
    "\n",
    "prompt = tokenizer.apply_chat_template(conversation, tokenize=False)\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    output = model.generate(\n",
    "        **inputs,\n",
    "        temperature=0.7,\n",
    "        top_p=0.9,\n",
    "        max_new_tokens=512\n",
    "    )\n",
    "\n",
    "processed_text = tokenizer.decode(output[0], skip_special_tokens=True)[len(prompt):].strip()\n",
    "\n",
    "# Print chunk information for monitoring\n",
    "#print(f\"\\n{'='*40} Chunk {chunk_num} {'='*40}\")\n",
    "print(f\"INPUT TEXT:\\n{text_chunk[:500]}...\")  # Show first 500 chars of input\n",
    "print(f\"\\nPROCESSED TEXT:\\n{processed_text[:500]}...\")  # Show first 500 chars of output\n",
    "print(f\"{'='*90}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_chunk(text_chunk, chunk_num):\n",
    "    \"\"\"Process a chunk of text and return both input and output for verification\"\"\"\n",
    "    conversation = [\n",
    "        {\"role\": \"system\", \"content\": SYS_PROMPT},\n",
    "        {\"role\": \"user\", \"content\": text_chunk},\n",
    "    ]\n",
    "    \n",
    "    prompt = tokenizer.apply_chat_template(conversation, tokenize=False)\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        output = model.generate(\n",
    "            **inputs,\n",
    "            temperature=0.7,\n",
    "            top_p=0.9,\n",
    "            max_new_tokens=512\n",
    "        )\n",
    "    \n",
    "    processed_text = tokenizer.decode(output[0], skip_special_tokens=True)[len(prompt):].strip()\n",
    "    \n",
    "    return processed_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "89ab718467ac489c94f779aa7d19b418",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing chunks:   0%|          | 0/43 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "# Read the file\n",
    "import os\n",
    "INPUT_FILE = 'extracted_text.txt'\n",
    "with open(INPUT_FILE, 'r', encoding='utf-8') as file:\n",
    "    text = file.read()\n",
    "\n",
    "# Calculate number of chunks\n",
    "num_chunks = (len(text) + CHUNK_SIZE - 1) // CHUNK_SIZE\n",
    "\n",
    "# Cell 6: Process the file with ordered output\n",
    "# Create output file name\n",
    "output_file = f\"clean_{os.path.basename(INPUT_FILE)}\"\n",
    "\n",
    "with open(output_file, 'w', encoding='utf-8') as out_file:\n",
    "    for chunk_num, chunk in enumerate(tqdm(chunks, desc=\"Processing chunks\")):\n",
    "        # Process chunk and append to complete text\n",
    "        processed_chunk = process_chunk(chunk, chunk_num)\n",
    "        processed_text += processed_chunk + \"\\n\"\n",
    "        \n",
    "        # Write chunk immediately to file\n",
    "        out_file.write(processed_chunk + \"\\n\")\n",
    "        out_file.flush()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
