ick, Ali Farhadi 
University of Washington, Allen Institute for AI, Facebook AI Research 
http://pjreddie.com/yolo/ 
Abstract 
We present YOLO, a new approach to object detection. 
Instead of repurposing classiﬁers to perform detection, we frame object detection as a regression problem to spatially separated bounding boxes and associated class probabilities. 
A single neural network predicts bounding boxes and class probabilities directly from full images in one evaluation. 
Since the whole detection pipeline is a single network, it can be optimized end-to-end directly on detection performance. 
Our uniﬁed architecture is extremely fast. 
Our base YOLO model processes images in real-time at 45 frames per second. 
A smaller version of the network, Fast YOLO, processes an astounding 155 frames per second while still achieving double the mAP of other real-time object detection methods.
w they interact. The human visual system is fast and accurate, allowing us to perform complex tasks like driving with little conscious thought. Fast, accurate algorithms for object detection would allow computers to drive cars without specialized sensors, enable assistive devices to convey real-time scene information to human users, and unlock the potential for general purpose, responsive robotic systems. Current detection systems repurpose classifiers to perform detection.
Systems like deformable parts models (DPM) use a sliding window approach where the classiﬁer is run at evenly spaced locations over the entire image. More recent approaches like R-CNN use region proposal methods.
box coordinates and class probabilities. Using our system, you only look once (YOLO) at an image to predict what objects are present and where they are. YOLO is refreshingly simple: see Figure 1. A single convolutional network simultaneously predicts multiple bounding boxes and class probabilities for those boxes. YOLO trains on full images and directly optimizes detection performance.
l-time systems. For a demo of our system running in real-time on a webcam, see our project webpage. Unlike other methods, YOLO reasons globally about the image during training and test time, implicitly encoding contextual information about classes and their appearance. This results in fewer background errors, with YOLO making less than half the number of background errors compared to Fast R-CNN. YOLO learns generalizable representations of objects, outperforming top detection methods even when trained on natural images and tested on artwork."
s likely to break down when applied to new domains or unexpected inputs. YOLO still lags behind state-of-the-art detection systems in accuracy. While it can quickly identify objects in images, it struggles to precisely localize some objects, especially small ones. We examine these tradeoffs further in our experiments. All of our training and testing code is open source. A variety of pre-trained models are also available to download.
s. Grid cells predict bounding boxes and confidence scores for those boxes. Confidence scores reflect the model's confidence that the box contains an object and how accurate it thinks the box is. We define confidence as the intersection over union (IOU) between the predicted box and the ground truth. If no object exists in a cell, the confidence score is zero.
e conditioned on the grid cell containing an object. We only predict one set of class probabilities per grid cell, regardless of the number of boxes B. At test time we multiply the conditional class probabilities and the individual box confidence predictions, Pr(ClassijObject) Pr(Object) IOUtruth pred= Pr(Classi) IOUtruth pred(1) which gives us class-specific confidence scores for each box.
We implement this model as a convolutional neural network and evaluate it on the PASCAL VOC detection dataset. The initial convolutional layers extract features from the image while the fully connected layers predict the output probabilities and coordinates. Our network architecture is inspired by the GoogLeNet model for image classification. Our network has 24 convolutional layers followed by 2 fully connected layers. Instead of the inception modules used by GoogLeNet, we use 1x1 reduction layers followed by 3x3 convolutional layers.
and Fast YOLO. 

448 448 3 7 7 Conv. Layer 7x7x64-s-2 Maxpool Layer 2x2-s-2 

Conv. Layer 3x3x192 Maxpool Layer 2x2-s-2Conv. Layers 1x1x128 3x3x256 1x1x256 3x3x512 

Maxpool Layer 2x2-s-2 Conv. Layers 1x1x256 3x3x512 1x1x256 3x3x512 3x3x1024 

Maxpool Layer 2x2-s-2 3 314 14 1024 Conv. Layers 1x1x512 3x3x10243x3x1024 3x3x1024-s-2 

3 37 7 10247 7 10247 7 30
t 20 convolutional layers from Figure 3 followed by an average-pooling layer and a fully connected layer. We trained this network for approximately a week and achieved a top-5 accuracy of 88% on the ImageNet 2012 validation set, comparable to GoogLeNet models.
ween 0 and 1. The bounding box x and y coordinates are parametrized to be offsets of a particular grid location, also bounded between 0 and 1. Our model uses a linear activation function for the final layer, while all other layers utilize a leaky rectified linear activation function. The optimization is based on sum-squared error in the output of our model.
and confidence predictions for boxes that don't contain objects. We set coord = 5 and noobj = 5. To reflect that small deviations in large boxes matter less than in small boxes, we predict the square root of the bounding box width and height instead of the width and height directly.
rdS2X i=0BX j=01obj ij(h^x_i - x_i)^2 + (y_i^y_i)^2i 
+ coordS2X i=0BX j=01obj ij(p_w^p - w^x_i)^2 + p_h^q(p_h^q - q^h)^2 + 
S2X i=0BX j=01obj ij(C^c(C^c - c)^2 + noobjS2X i=0BX j=01noobj ij(C^c(C^c - c)^2
r learning rate schedule starts at 10^3, then decreases to 10^2 for 75 epochs, followed by 10^3 for 30 epochs, and finally 10^4 for 30 epochs. To prevent overfitting, we use dropout and extensive data augmentation. We apply dropout with a rate of 0.5 after the first connected layer to prevent co-adaptation between layers. Data augmentation involves random scaling and translations of up to 20% of the original image size, as well as adjusting the exposure and saturation of the image by up to a factor of 5 in the HSV color space.
esign enforces spatial diversity in the bounding box predictions. Often it is clear which grid cell an object falls in to and the network only predicts one box for each object. However, some large objects or objects near the border of multiple cells can be well localized by multiple cells. Non-maximal suppression can be used to fix these multiple detections.
igurations. Our model uses coarse features for predicting bounding boxes due to multiple downsampling layers from the input image. The loss function approximates detection performance but treats errors the same for small and large bounding boxes, with small errors in large boxes being generally benign but having a greater effect on IOU. Main source of error is incorrect localizations.
ons. We compare the YOLO detection system to several top detection frameworks. Deformable parts models use a sliding window approach to object detection. Our system replaces these disparate parts with a single convolutional neural network. The network performs feature extraction, bounding box prediction, non-maximal suppression, and contextual reasoning concurrently. Instead of static features, the network trains the features in-line and optimizes them for the detection task.
tracts features, an SVM scores the boxes, a linear model adjusts the bounding boxes, and non-max suppression eliminates duplicate detections. Each stage of this complex pipeline must be precisely tuned independently. The resulting system is very slow, taking over 40 seconds per image at test time. YOLO shares some similarities with R-CNN. Each grid cell proposes potential bounding boxes and scores those boxes using convolutional features. Our system puts spatial constraints on the grid cell proposals, which helps mitigate multiple detections of the same object. We propose far fewer bounding boxes, only 98 per image, compared to about 2000 from Selective Search. Our system combines these individual components into a single, jointly optimized model.
offer speed and accuracy improvements over R-CNN, both still fall short of real-time performance. Research efforts focus on speeding up the DPM pipeline by speeding up HOG computation, using cascades, and pushing computation to GPUs. Only 30Hz DPM actually runs in real-time. YOLO throws out the pipeline entirely and is fast by design. Detectors for single classes like faces or people can be highly optimized due to less variation. YOLO is a general purpose detector that learns to detect a variety of objects simultaneously.
orm general object detection and is still just a piece in a larger detection pipeline, requiring further image patch classiﬁcation. Both YOLO and MultiBox use a convolutional network to predict bounding boxes in an image but YOLO is a complete detection system.
ion is a simpler task than object detection. MultiGrasp only needs to predict a single graspable region for an image containing one object. It doesn’t have to estimate the size, location, or boundaries of the object or predict its class, only find a region suitable for grasping. YOLO predicts both bounding boxes and class probabilities for multiple objects of multiple classes in an image. 

Experiments. We compare YOLO with other real-time detection systems on PASCAL VOC 2007. We explore the errors on VOC 2007 made by YOLO and Fast R-CNN, one of the highest performing versions of R-CNN. Based on the different error profiles we show that YOLO can be used to rescore Fast R-CNN detections and reduce the errors from background false positives, giving a significant performance boost. We also present VOC 2012 results and compare mAP
on two artwork datasets. 

Comparison to other real-time systems reveals that only Sadeghi et al. produce a detection system that runs in real-time, at 30 frames per second or better. We compare YOLO to their GPU implementation of DPM, which runs at 30Hz or 100Hz. 

YOLO is the fastest object detection method on PASCAL, with a mAP of 52.7%, more than twice as accurate as prior work on real-time detection.
t is useful for comparison to other detection systems that rely on VGG-16 but since it is slower than real-time, the rest of the paper focuses on our faster models. 

Fastest DPM effectively speeds up DPM without sacrificing much mAP but still misses real-time performance by a factor of 2. It also is limited by DPM’s relatively low accuracy on detection compared to neural network approaches.
ny other real-time detector. It is 10 mAP more accurate than the fast version and still runs at real-time speed.
YOLO and state-of-the-art detectors by analyzing VOC 2007 results. YOLO is compared to Fast R-CNN, one of the highest performing detectors on PASCAL, using the methodology and tools of Hoiem et al. [19]. At test time, we evaluate the top N predictions for each category. Each prediction is classified based on the type of error: correct, localization (IOU < 0.5), similar (IOU > 0.1), or other. The results show: 
Correct: 71.6%
Localization: 8.6%
Similar: 4.3%
Other: 1.9%
Background: 13.6%
shows the breakdown of each error type averaged across all 20 classes. YOLO struggles to localize objects correctly. Localization errors account for more of YOLO’s errors than all other sources combined. Fast R-CNN makes fewer localization errors but far more background errors.
a combined mAP of 72.4. Other versions of Fast R-CNN provide only a small benefit, while YOLO provides a significant performance boost. 

Model combination experiments on VOC 2007 show that combining various models with the best version of Fast R-CNN does not yield a significant improvement. YOLO provides a notable performance boost.
0 87.5 80.9 81.0 74.7 41.8 71.5 68.5 82.1 67.2 MR CNN SCNN 70.7 85.0 79.6 71.5 55.3 57.7 76.0 73.9 84.6 50.5 74.3 61.7 85.5 79.9 81.7 76.4 41.0 69.0 61.2 77.7 72.1 Faster R-CNN 70.4 84.9 79.8 74.3 53.9 49.8 77.5 75.9 88.5 45.6 77.1 55.3 86.9 81.7 80.9 79.6 40.1 72.6 60.9 81.2 61.5 DEEP ENS COCO 70.1 84.0 79.4 71.6 51.9 51.1 74.1 72.1 88.6 48.3 73.4 57.8 86.1 80.0 80.7 70.4 46.6 69.6 68.8 75.9 71.4 NoC 68.8 82.8 79.0 71.6 52.3 53.7 74.1 69.0 84.9 46.9 74.3 53.1 85.0 81.3 79.5 72.2 38.9 72.4 59.5 76.7 68.1 Fast R-CNN 68.4 82.3 78.4 70.8 52.3 38.7 77.8 71.6 89.3
64.0 35.3 67.9 55.7 68.7 62.6 NUS NIN 62.4 77.9 73.1 62.6 39.5 43.3 69.1 66.4 78.9 39.1 68.1 50.0 77.2 71.3 76.1 64.7 38.4 66.9 56.2 66.9 62.7 

R-CNN VGG BB 62.4 79.6 72.7 61.9 41.2 41.9 65.9 66.4 84.6 38.5 67.2 46.7 82.0 74.8 76.0 65.2 35.6 65.4 54.2 67.4 60.3 

R-CNN VGG 59.2 76.8 70.9 56.6 37.5 36.9 62.9 63.6 81.1 35.7 64.3 43.9 80.4 71.6 74.0 60.0 30.8 63.4 52.0 63.5 58.7 

YOLO 57.9 77.0 67.2 57.7 38.3 22.7 68.3 55.9 81.4 36.2 60.8 48.5 77.2 72.3 71.3 63.5 28.9 52.2 54.8 73.9 50.8 

Feature Edit 56.3 74.6 69.1 54.4 39.1 33.1 65.2 62.7 69.7 30.8 56.0 44.6 70.0 64.4
. Mean average precision and per-class average precision are shown for a variety of detection methods. YOLO is the only real-time detector. Fast R-CNN + YOLO is the forth highest scoring method, with a 2.3% boost over Fast R-CNN. mAP increases by 3.2% to 75.0%.
ine the results. However, since YOLO is so fast it doesn’t add any significant computational time compared to Fast R-CNN. 
On the VOC 2012 test set, YOLO scores 57.9% mAP. This is lower than the current state of the art, closer to the original R-CNN using VGG-16. Our system struggles with small objects compared to its closest competitors. On categories like bottle, sheep, and tv/monitor YOLO scores 8-10% lower than R-CNN or Feature Edit. However, on other categories like cat and train YOLO achieves higher performance. Our combined Fast R-CNN + YOLO model is one of the highest performing detection methods. Fast R-CNN gets a 2.3% improvement from the combination with YOLO, boosting it 5 spots up on the public leaderboard.
from what the system has seen before. We compare YOLO to other detection systems on the Picasso Dataset and the People-Art Dataset, two datasets for testing person detection on artwork. Figure 5 shows comparative performance between YOLO and other detection methods. For reference, we give VOC 2007 detection AP on person where all models are trained on VOC 2007 data. R-CNN has high AP on VOC 2007, but drops off considerably when applied to artwork. R-CNN's performance is limited by its reliance on selective search for bounding box proposals, which is tuned for natural images.
formance on VOC 2007 and its AP degrades less than other methods when applied to artwork. Like DPM, YOLO models the size and shape of objects, as well as relationships between objects and where objects commonly appear. Artwork and natural images are very different on a pixel level but they are similar in terms of the size and shape of objects, thus YOLO can still predict good bounding boxes and detections. YOLO is a fast, accurate object detector, making it ideal for computer vision applications.
5: Generalization results on Picasso and People-Art datasets. Figure 6: Qualitative Results. YOLO running on sample artwork and natural images from the internet. It is mostly accurate although it sometimes misidentifies a person as an airplane. The system is interactive and engaging. When attached to a webcam, it functions like a tracking system, detecting objects as they move around and change in appearance.
art in real-time object detection. It generalizes well to new domains, making it ideal for applications that rely on fast and robust object detection.
n et al. Fast detection of 100,000 object classes on a single machine. CVPR 2013. 1814–1821. 
J Donahue et al. Decaf: A deep activation feature for generic visual recognition. arXiv preprint arXiv:1310.1531. 
J Dong et al. Towards unified object detection and semantic segmentation. ECCV 2014. 299–314. 
D Erhan et al. Scalable object detection using deep neural networks.
A. Eslami, L. Van Gool, C. K. I. Williams, J. Winn, and A. Zisserman. The Pascal Visual Object Classes Challenge: A Retrospective. International Journal of Computer Vision, 111(1):98–136, Jan. 2015. 
P. F. Felzenszwalb, R. B. Girshick, D. McAllester, and D. Ramman. Object Detection with Discriminatively Trained Part Based Models. IEEE Transactions on Pattern Analysis and Machine Intelligence, 32(9):1627–1645, 2010. 
S. Gidaris and N. Komodakis. Object Detection via a Multi-Region & Semantic Segmentation-Aware CNN Model. CoRR, abs/1505.01749, 2015. 
S. Ginosar, D. Haas, T. Brown, and J. Malik. Detecting People in Cubist Art. In Computer Vision-ECCV 2014 Workshops, pages 101–116. Springer, 2014. 
R. Girshick, J. Donahue, T. Darrell, and J. Malik. Rich Feature Hierarchies for Accurate Object Detection and Semantic Segmentation.
ck. Fast R-CNN. CoRR, abs/1504.08083, 2015. 
S. Gould, T. Gao, and D. Koller. Region-based segmentation and object detection. In Advances in neural information processing systems, pages 655–663, 2009. 
B. Hariharan, P. Arbel´aez, R. Girshick, and J. Malik. Simultaneous detection and segmentation. In Computer Vision– ECCV 2014, pages 297–312. Springer, 2014. 
K. He, X. Zhang, S. Ren, and J. Sun. Spatial pyramid pooling in deep convolutional networks for visual recognition. arXiv preprint arXiv:1406.4729, 2014. 
G. E. Hinton, N. Srivastava, A. Krizhevsky, I. Sutskever, and R. R. Salakhutdinov. Improving neural networks by preventing co-adaptation of feature detectors. arXiv preprint arXiv:1207.0580, 2012. 
D. Hoiem, Y. Chodpathumwan, and Q. Dai. Diagnosing error in object detectors. In Computer Vision–ECCV 2012
ke features for rapid object detection. In Image Processing. 2002. Proceedings. 2002 International Conference on, volume 1, pages I–900. IEEE, 2002. 
M. Lin, Q. Chen, and S. Yan. Network in network. CoRR, abs/1312.4400, 2013. 
Object recognition from local scale-invariant features. In Computer vision, 1999. The proceedings of the seventh IEEE international conference on, volume 2, pages 1150–1157. Ieee, 1999. 
Models accuracy on imagenet 2012 val. https://github.com/BVLC/caffe/wiki/Models-accuracy-on-ImageNet-2012-val. 
C. P. Papageorgiou, M. Oren, and T. Poggio. A general framework for object detection. In Computer vision, 1998. sixth international conference on, pages 555–562. IEEE, 1998. 
Darknet: Open source
s/1412.3128, 2014. 
Faster r-cnn: To-wards real-time object detection with region proposal net- works. arXiv preprint arXiv:1506.01497, 2015. 
Object detection networks on convolutional feature maps. CoRR, abs/1504.06066, 2015. 
ImageNet Large Scale Visual Recognition Challenge. International Journal of Computer Vision (IJCV), 2015. 
30hz object detection with dpm v5. In Computer Vision–ECCV 2014, pages 65–79. Springer, 2014. 
Overfeat:
ore dropouts in pool5 feature maps for better object detection. 
C. Szegedy et al. Going deeper with convolutions. 
J. R. Uijlings et al. Selective search for object recognition. 
P. Viola and M. Jones. Robust real-time object detection. 
P. Viola and M. J. Jones. Robust real-time face detection. 
J. Yan et al. The fastest deformable part model for object detection.

