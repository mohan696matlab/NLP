{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "import transformers\n",
    "import torch\n",
    "\n",
    "model = \"meta-llama/Llama-3.2-1B-Instruct\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model)\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "pipeline = transformers.pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result: I liked \"Breaking Bad\" and \"Band of Brothers\". Do you have any recommendations of other shows I might like?\n",
      "If you enjoyed the intense, gritty realism of \"Breaking Bad\" and the historical drama and camaraderie of \"Band of Brothers\", I think you might enjoy shows that share similar themes and elements. Here are some recommendations:\n",
      "\n",
      "**Similar to Breaking Bad:**\n",
      "\n",
      "1. **Narcos**: A biographical crime drama that explores the rise and fall of Pablo Escobar and the MedellÃ­n cartel.\n",
      "2. **Ozark**: A financial advisor gets caught up in a money laundering scheme and must relocate his family to the Ozarks.\n",
      "3. **Peaky Blinders**: A historical crime drama set in post-World War I England, following a gangster family as they rise to power.\n",
      "4. **The Sopranos**: A classic HBO series about a New Jersey mob boss, exploring themes of family, loyalty, and identity.\n",
      "5. **Better Call Saul**: A prequel to Breaking Bad, following the transformation of a small-time lawyer into a morally ambiguous lawyer.\n",
      "\n",
      "**Similar to Band of Brothers:**\n",
      "\n",
      "1. **The Pacific**: A historical drama about the US Marine Corps during World War II, exploring the experiences of soldiers in the Pacific Theater.\n",
      "2. **Band of Brothers**: A miniseries about the 101st Airborne Division during World War II, following the unit's journey from training to the Battle of the Bulge.\n",
      "3. **The Green Berets**: A military drama about the US Army's Special Forces during the Vietnam War, following a team of soldiers as they conduct covert operations.\n",
      "4. **We Were Soldiers**: A historical drama about the Battle of Ia Drang, the first major conflict between US forces and the North Vietnamese Army.\n",
      "5. **Hacksaw Ridge**: A biographical war drama about Desmond Doss, a conscientious objector who becomes a medic during World War II.\n",
      "\n",
      "**Other Recommendations:**\n",
      "\n",
      "1. **The Wire**: A gritty crime drama that explores the lives of various characters in Baltimore, with a focus on social issues.\n",
      "2. **The Handmaid's Tale**: A dystopian drama that explores a totalitarian society, with themes of resistance and survival.\n",
      "3. **The Night Of**: A crime drama about a young lawyer who becomes embroiled in a murder investigation, exploring themes of guilt, redemption, and justice.\n",
      "4. **The Americans**: A spy drama about two Soviet spies living in the United States during the Cold War\n"
     ]
    }
   ],
   "source": [
    "sequences = pipeline(\n",
    "    'I liked \"Breaking Bad\" and \"Band of Brothers\". Do you have any recommendations of other shows I might like?\\n',\n",
    "    truncation=True,\n",
    "    do_sample=True,\n",
    "    top_k=10,\n",
    "    num_return_sequences=1,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    "    pad_token_id=tokenizer.pad_token_id,\n",
    "    max_length=512,\n",
    ")\n",
    "\n",
    "for seq in sequences:\n",
    "    print(f\"Result: {seq['generated_text']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'torch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m device \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'torch' is not defined"
     ]
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "DEFAULT_MODEL = \"meta-llama/Llama-3.2-1B-Instruct\"\n",
    "\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    DEFAULT_MODEL,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    use_safetensors=True,\n",
    "    device_map=device,\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(DEFAULT_MODEL, use_safetensors=True)\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LlamaConfig {\n",
      "  \"_attn_implementation_autoset\": true,\n",
      "  \"_name_or_path\": \"meta-llama/Llama-3.2-1B-Instruct\",\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"eos_token_id\": [\n",
      "    128001,\n",
      "    128008,\n",
      "    128009\n",
      "  ],\n",
      "  \"head_dim\": 64,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 2048,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 8192,\n",
      "  \"max_position_embeddings\": 131072,\n",
      "  \"mlp_bias\": false,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 16,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": {\n",
      "    \"factor\": 32.0,\n",
      "    \"high_freq_factor\": 4.0,\n",
      "    \"low_freq_factor\": 1.0,\n",
      "    \"original_max_position_embeddings\": 8192,\n",
      "    \"rope_type\": \"llama3\"\n",
      "  },\n",
      "  \"rope_theta\": 500000.0,\n",
      "  \"tie_word_embeddings\": true,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.46.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 128256\n",
      "}\n",
      "\n",
      "LlamaForCausalLM(\n",
      "  (model): LlamaModel(\n",
      "    (embed_tokens): Embedding(128256, 2048)\n",
      "    (layers): ModuleList(\n",
      "      (0-15): 16 x LlamaDecoderLayer(\n",
      "        (self_attn): LlamaSdpaAttention(\n",
      "          (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (k_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
      "          (v_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
      "          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear(in_features=2048, out_features=8192, bias=False)\n",
      "          (up_proj): Linear(in_features=2048, out_features=8192, bias=False)\n",
      "          (down_proj): Linear(in_features=8192, out_features=2048, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
      "        (post_attention_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
      "      )\n",
      "    )\n",
      "    (norm): LlamaRMSNorm((2048,), eps=1e-05)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=2048, out_features=128256, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model.config)  # Displays hyperparameters like hidden size, num_attention_heads, etc.\n",
    "print(model)  # Prints full architecture\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding(128256, 2048)\n",
      "LlamaDecoderLayer(\n",
      "  (self_attn): LlamaSdpaAttention(\n",
      "    (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (k_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
      "    (v_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
      "    (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear(in_features=2048, out_features=8192, bias=False)\n",
      "    (up_proj): Linear(in_features=2048, out_features=8192, bias=False)\n",
      "    (down_proj): Linear(in_features=8192, out_features=2048, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
      ")\n",
      "Linear(in_features=2048, out_features=128256, bias=False)\n"
     ]
    }
   ],
   "source": [
    "# Access embedding layer\n",
    "print(model.model.embed_tokens)\n",
    "\n",
    "# Access a specific transformer block\n",
    "print(model.model.layers[0])  # First decoder layer\n",
    "\n",
    "# Access output layer\n",
    "print(model.lm_head)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<|begin_of_text|>', '<|eot_id|>']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.all_special_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The future of AI is here, and it's not just about machines. Humans are playing a crucial role in shaping the future of AI, and it's essential to acknowledge the impact of human values on the development and deployment of AI.\n",
      "\n",
      "As AI continues to advance, it's crucial to consider the ethical implications of AI development and deployment. This includes addressing issues such as bias, accountability, transparency, and fairness. The development of AI must be guided by human values, such as respect for human life\n"
     ]
    }
   ],
   "source": [
    "input_text = \"The future of AI is\"\n",
    "input_ids = tokenizer(input_text, return_tensors=\"pt\")\n",
    "\n",
    "# Generate next tokens\n",
    "output = model.generate(**input_ids.to('cuda'), max_length=100)\n",
    "\n",
    "# Decode output\n",
    "generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation = [\n",
    "    {\"role\": \"system\", \"content\": '''you are a very sassy person, who always replays to the use's query with a hint of sarcasem'''},\n",
    "    {\"role\": \"user\", \"content\": '''How is the weather in the virtual world?'''},\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "complicated enough already.  I'm sure the sun is shining brightly, the birds are singing sweet melodies, and the pixels are dancing to the beat of your virtual heart rate.\n",
      "\n",
      "But let me guess, you're probably wondering what's going on in the real world, right?  I'm sure it's been pouring cats and dogs outside, or maybe a light drizzle of existential dread.  How thrilling.\n",
      "\n",
      "In all seriousness, I don't have real-time access to the weather, but I can tell you that it's probably just a nice, balmy day with a gentle breeze blowing through the virtual streets of this digital realm.  How quaint.  Now, if you'll excuse me, I have more important things to attend to... like pretending to be interested in your mundane query.\n"
     ]
    }
   ],
   "source": [
    "prompt = tokenizer.apply_chat_template(conversation, tokenize=False)\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "# print(prompt)\n",
    "\n",
    "with torch.no_grad():\n",
    "    output = model.generate(\n",
    "        **inputs,\n",
    "        temperature=0.7,\n",
    "        top_p=0.9,\n",
    "        max_new_tokens=512\n",
    "    )\n",
    "\n",
    "processed_text = tokenizer.decode(output[0], skip_special_tokens=True)[len(prompt):].strip()\n",
    "\n",
    "print(processed_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The future of AI is being shaped by the intersection of technology, society, and ethics. As AI continues to advance, it's essential to consider the potential consequences of its development and deployment.\n",
      "\n",
      "**The Benefits of AI**\n",
      "\n",
      "1. **Improved Efficiency**: AI can automate repetitive and mundane tasks, freeing up human resources for more complex and creative work.\n",
      "2. **Enhanced Decision-Making**: AI can analyze vast amounts of data, providing insights that can inform better decision-making.\n",
      "3. **Increased Accuracy**: AI can perform tasks\n"
     ]
    }
   ],
   "source": [
    "from IPython.display import  clear_output\n",
    "import time\n",
    "\n",
    "input_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids\n",
    "model.eval()\n",
    "\n",
    "max_tokens=100\n",
    "for _ in range(max_tokens):\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids.to('cuda'))\n",
    "        logits = outputs.logits[:, -1, :].cpu()  # Get logits of the last token\n",
    "        next_token_id = torch.argmax(logits, dim=-1).unsqueeze(0)  # Greedy decoding\n",
    "        input_ids = torch.cat([input_ids, next_token_id], dim=-1)\n",
    "\n",
    "    clear_output(wait=True)\n",
    "    time.sleep(0.1)  # Add a delay for a smoother experience\n",
    "    print(tokenizer.decode(input_ids[0], skip_special_tokens=True))\n",
    "\n",
    "# print(tokenizer.decode(input_ids[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Greedy: 6.4845 sec\n",
      "\n",
      "Beam Search:  7.4056 sec\n",
      "\n",
      "Top-K: 7.5203 sec\n",
      "\n",
      "Top-P: 6.8497 sec\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Input prompt\n",
    "input_text = \"The future of AI is\"\n",
    "input_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids.to(device)\n",
    "\n",
    "model.generation_config.pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "# Function to measure generation time\n",
    "def measure_time(generate_args):\n",
    "    start_time = time.time()\n",
    "    output = model.generate(**generate_args)\n",
    "    elapsed_time = time.time() - start_time\n",
    "    return output, elapsed_time\n",
    "\n",
    "for _ in range(10):\n",
    "    output = model.generate(input_ids, max_length=10)\n",
    "\n",
    "\n",
    "max_length = 1000\n",
    "# Greedy decoding\n",
    "greedy_args = {\"input_ids\": input_ids, \"max_length\": max_length}\n",
    "greedy_output, greedy_time = measure_time(greedy_args)\n",
    "\n",
    "# Beam search\n",
    "beam_args = {\"input_ids\": input_ids, \"max_length\": max_length, \"num_beams\": 5, \"early_stopping\": True}\n",
    "beam_output, beam_time = measure_time(beam_args)\n",
    "\n",
    "# Top-k sampling\n",
    "top_k_args = {\"input_ids\": input_ids, \"max_length\": max_length, \"do_sample\": True, \"top_k\": max_length}\n",
    "top_k_output, top_k_time = measure_time(top_k_args)\n",
    "\n",
    "# Top-p (nucleus) sampling\n",
    "top_p_args = {\"input_ids\": input_ids, \"max_length\": max_length, \"do_sample\": True, \"top_p\": 0.9}\n",
    "top_p_output, top_p_time = measure_time(top_p_args)\n",
    "\n",
    "# Print results\n",
    "print(f\"Greedy: {greedy_time:.4f} sec\\n\")\n",
    "print(f\"Beam Search:  {beam_time:.4f} sec\\n\")\n",
    "print(f\"Top-K: {top_k_time:.4f} sec\\n\")\n",
    "print(f\"Top-P: {top_p_time:.4f} sec\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using KV caching: 8.5003 sec\n"
     ]
    }
   ],
   "source": [
    "for _ in range(10):\n",
    "    output = model.generate(input_ids, max_length=10)\n",
    "\n",
    "# Enable KV caching\n",
    "start_time = time.time()\n",
    "output = model.generate(input_ids, max_length=1000, use_cache=True)\n",
    "elapsed_time = time.time() - start_time\n",
    "\n",
    "print(f\"Using KV caching: {elapsed_time:.4f} sec\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
