{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ea33f6eaa27419bb6086cb5bb092ec7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "import torch\n",
    "from IPython.display import  clear_output\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "DEFAULT_MODEL = \"meta-llama/Llama-3.2-3B-Instruct\"\n",
    "\n",
    "# Configure 4-bit quantization\n",
    "# bnb_config = BitsAndBytesConfig(\n",
    "#     load_in_8bit=True)\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True, \n",
    "    bnb_4bit_quant_type=\"nf4\",  # Normalized float 4-bit (recommended)\n",
    "    bnb_4bit_compute_dtype=torch.float16,  \n",
    "    bnb_4bit_use_double_quant=True  # Improves performance by applying second quantization\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    DEFAULT_MODEL,\n",
    "    quantization_config=bnb_config,\n",
    "    # torch_dtype=torch.bfloat16,\n",
    "    device_map=device,\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(DEFAULT_MODEL, use_safetensors=True)\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "model.generation_config.pad_token_id = 128001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mohan.dash/miniconda3/envs/diffuser_data_generation/lib/python3.12/site-packages/transformers/generation/configuration_utils.py:628: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/home/mohan.dash/miniconda3/envs/diffuser_data_generation/lib/python3.12/site-packages/transformers/generation/configuration_utils.py:633: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\"Modi\" can refer to several individuals, but I'll provide information on the most well-known ones:\n",
      "\n",
      "1. **Narendra Modi**: He is the current Prime Minister of India, serving since 2014. Born on September 17, 1950, in Vadnagar, Madhya Pradesh, India, Modi is a member of the Bharatiya Janata Party (BJP). He has been a prominent figure in Indian politics for over three decades, serving as the Chief Minister of Gujarat from 2001 to 2014.\n",
      "2. **Suresh Modi**: An Indian politician from the Bharatiya Janata Party (BJP), Suresh Modi is the current Deputy Chief Minister of Uttar Pradesh, serving since 2017. He is the brother of Narendra Modi.\n",
      "3. **Dilip Modi**: An Indian politician from the Bharatiya Janata Party (BJP), Dilip Modi is a Member of the Legislative Assembly (MLA) from the Gujarat Legislative Assembly, representing the Surat North constituency.\n",
      "\n",
      "If you could provide more context or specify which Modi you are referring to, I'd be happy to provide more information.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "conversation = [\n",
    "    {\"role\": \"system\", \"content\": f'''You are a helpful assistant'''},\n",
    "    {\"role\": \"user\", \"content\": f'''who is Modi'''},\n",
    "]\n",
    "# \n",
    "prompt = tokenizer.apply_chat_template(conversation, tokenize=False)\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    output = model.generate(\n",
    "        **inputs,\n",
    "        do_sample=False,\n",
    "        max_new_tokens=256\n",
    "    )\n",
    "\n",
    "\n",
    "processed_text = tokenizer.decode(output[0][len(inputs.input_ids[0])+3:], skip_special_tokens=True)\n",
    "\n",
    "print(processed_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_chat_history(ai_response, user_query, chat_history):\n",
    "    chat_history.extend([{\"role\": \"assistant\", \"content\": ai_response},\n",
    "                         {\"role\": \"user\", \"content\": user_query}])\n",
    "    return chat_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation2 = update_chat_history(ai_response=processed_text,user_query='who is Narendra Modi', chat_history=conversation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "token length: 289\n",
      "\n",
      "\n",
      "**Narendra Damodardas Modi** (born September 17, 1950) is the current Prime Minister of India, serving since 2014. He is a member of the Bharatiya Janata Party (BJP) and is widely known for his conservative and nationalist policies.\n",
      "\n",
      "**Early Life and Education:**\n",
      "\n",
      "Modi was born in Vadnagar, Madhya Pradesh, India, to a Hindu family. His father, Damodardas Modi,\n"
     ]
    }
   ],
   "source": [
    "prompt = tokenizer.apply_chat_template(conversation2, tokenize=False)\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "\n",
    "print(f'token length: {len(inputs.input_ids[0])}')\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    output = model.generate(\n",
    "        **inputs,\n",
    "        do_sample=True,\n",
    "        max_new_tokens=100\n",
    "    )\n",
    "\n",
    "\n",
    "processed_text = tokenizer.decode(output[0][len(inputs.input_ids[0])+3:], skip_special_tokens=True)\n",
    "\n",
    "print(processed_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "proce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;31mSignature:\u001b[0m\n",
      "\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0minputs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mgeneration_config\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtransformers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgeneration\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfiguration_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGenerationConfig\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mlogits_processor\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtransformers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgeneration\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogits_process\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLogitsProcessorList\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mstopping_criteria\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtransformers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgeneration\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstopping_criteria\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mStoppingCriteriaList\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mprefix_allowed_tokens_fn\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mCallable\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0msynced_gpus\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0massistant_model\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mForwardRef\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'PreTrainedModel'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mstreamer\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mForwardRef\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'BaseStreamer'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mnegative_prompt_ids\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mnegative_prompt_attention_mask\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtransformers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgeneration\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGenerateDecoderOnlyOutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransformers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgeneration\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGenerateEncoderDecoderOutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransformers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgeneration\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGenerateBeamDecoderOnlyOutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransformers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgeneration\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGenerateBeamEncoderDecoderOutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLongTensor\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mDocstring:\u001b[0m\n",
      "Generates sequences of token ids for models with a language modeling head.\n",
      "\n",
      "<Tip warning={true}>\n",
      "\n",
      "Most generation-controlling parameters are set in `generation_config` which, if not passed, will be set to the\n",
      "model's default generation configuration. You can override any `generation_config` by passing the corresponding\n",
      "parameters to generate(), e.g. `.generate(inputs, num_beams=4, do_sample=True)`.\n",
      "\n",
      "For an overview of generation strategies and code examples, check out the [following\n",
      "guide](../generation_strategies).\n",
      "\n",
      "</Tip>\n",
      "\n",
      "Parameters:\n",
      "    inputs (`torch.Tensor` of varying shape depending on the modality, *optional*):\n",
      "        The sequence used as a prompt for the generation or as model inputs to the encoder. If `None` the\n",
      "        method initializes it with `bos_token_id` and a batch size of 1. For decoder-only models `inputs`\n",
      "        should be in the format of `input_ids`. For encoder-decoder models *inputs* can represent any of\n",
      "        `input_ids`, `input_values`, `input_features`, or `pixel_values`.\n",
      "    generation_config ([`~generation.GenerationConfig`], *optional*):\n",
      "        The generation configuration to be used as base parametrization for the generation call. `**kwargs`\n",
      "        passed to generate matching the attributes of `generation_config` will override them. If\n",
      "        `generation_config` is not provided, the default will be used, which has the following loading\n",
      "        priority: 1) from the `generation_config.json` model file, if it exists; 2) from the model\n",
      "        configuration. Please note that unspecified parameters will inherit [`~generation.GenerationConfig`]'s\n",
      "        default values, whose documentation should be checked to parameterize generation.\n",
      "    logits_processor (`LogitsProcessorList`, *optional*):\n",
      "        Custom logits processors that complement the default logits processors built from arguments and\n",
      "        generation config. If a logit processor is passed that is already created with the arguments or a\n",
      "        generation config an error is thrown. This feature is intended for advanced users.\n",
      "    stopping_criteria (`StoppingCriteriaList`, *optional*):\n",
      "        Custom stopping criteria that complements the default stopping criteria built from arguments and a\n",
      "        generation config. If a stopping criteria is passed that is already created with the arguments or a\n",
      "        generation config an error is thrown. If your stopping criteria depends on the `scores` input, make\n",
      "        sure you pass `return_dict_in_generate=True, output_scores=True` to `generate`. This feature is\n",
      "        intended for advanced users.\n",
      "    prefix_allowed_tokens_fn (`Callable[[int, torch.Tensor], List[int]]`, *optional*):\n",
      "        If provided, this function constraints the beam search to allowed tokens only at each step. If not\n",
      "        provided no constraint is applied. This function takes 2 arguments: the batch ID `batch_id` and\n",
      "        `input_ids`. It has to return a list with the allowed tokens for the next generation step conditioned\n",
      "        on the batch ID `batch_id` and the previously generated tokens `inputs_ids`. This argument is useful\n",
      "        for constrained generation conditioned on the prefix, as described in [Autoregressive Entity\n",
      "        Retrieval](https://arxiv.org/abs/2010.00904).\n",
      "    synced_gpus (`bool`, *optional*):\n",
      "        Whether to continue running the while loop until max_length. Unless overridden, this flag will be set\n",
      "        to `True` if using `FullyShardedDataParallel` or DeepSpeed ZeRO Stage 3 with multiple GPUs to avoid\n",
      "        deadlocking if one GPU finishes generating before other GPUs. Otherwise, defaults to `False`.\n",
      "    assistant_model (`PreTrainedModel`, *optional*):\n",
      "        An assistant model that can be used to accelerate generation. The assistant model must have the exact\n",
      "        same tokenizer. The acceleration is achieved when forecasting candidate tokens with the assistant model\n",
      "        is much faster than running generation with the model you're calling generate from. As such, the\n",
      "        assistant model should be much smaller.\n",
      "    streamer (`BaseStreamer`, *optional*):\n",
      "        Streamer object that will be used to stream the generated sequences. Generated tokens are passed\n",
      "        through `streamer.put(token_ids)` and the streamer is responsible for any further processing.\n",
      "    negative_prompt_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n",
      "        The negative prompt needed for some processors such as CFG. The batch size must match the input batch\n",
      "        size. This is an experimental feature, subject to breaking API changes in future versions.\n",
      "    negative_prompt_attention_mask (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n",
      "        Attention_mask for `negative_prompt_ids`.\n",
      "    kwargs (`Dict[str, Any]`, *optional*):\n",
      "        Ad hoc parametrization of `generation_config` and/or additional model-specific kwargs that will be\n",
      "        forwarded to the `forward` function of the model. If the model is an encoder-decoder model, encoder\n",
      "        specific kwargs should not be prefixed and decoder specific kwargs should be prefixed with *decoder_*.\n",
      "\n",
      "Return:\n",
      "    [`~utils.ModelOutput`] or `torch.LongTensor`: A [`~utils.ModelOutput`] (if `return_dict_in_generate=True`\n",
      "    or when `config.return_dict_in_generate=True`) or a `torch.LongTensor`.\n",
      "\n",
      "        If the model is *not* an encoder-decoder model (`model.config.is_encoder_decoder=False`), the possible\n",
      "        [`~utils.ModelOutput`] types are:\n",
      "\n",
      "            - [`~generation.GenerateDecoderOnlyOutput`],\n",
      "            - [`~generation.GenerateBeamDecoderOnlyOutput`]\n",
      "\n",
      "        If the model is an encoder-decoder model (`model.config.is_encoder_decoder=True`), the possible\n",
      "        [`~utils.ModelOutput`] types are:\n",
      "\n",
      "            - [`~generation.GenerateEncoderDecoderOutput`],\n",
      "            - [`~generation.GenerateBeamEncoderDecoderOutput`]\n",
      "\u001b[0;31mFile:\u001b[0m      ~/miniconda3/envs/diffuser_data_generation/lib/python3.12/site-packages/transformers/generation/utils.py\n",
      "\u001b[0;31mType:\u001b[0m      method"
     ]
    }
   ],
   "source": [
    "model.generate?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
