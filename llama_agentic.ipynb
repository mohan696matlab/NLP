{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a372aa47c2704c19b76e2518571fe544",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "import torch\n",
    "from IPython.display import  clear_output\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "DEFAULT_MODEL = \"meta-llama/Llama-3.2-3B-Instruct\"\n",
    "\n",
    "# Configure 4-bit quantization\n",
    "# bnb_config = BitsAndBytesConfig(\n",
    "#     load_in_8bit=True)\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True, \n",
    "    bnb_4bit_quant_type=\"nf4\",  # Normalized float 4-bit (recommended)\n",
    "    bnb_4bit_compute_dtype=torch.float16,  \n",
    "    bnb_4bit_use_double_quant=True  # Improves performance by applying second quantization\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    DEFAULT_MODEL,\n",
    "    quantization_config=bnb_config,\n",
    "    # torch_dtype=torch.bfloat16,\n",
    "    device_map=device,\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(DEFAULT_MODEL, use_safetensors=True)\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "model.generation_config.pad_token_id = 128001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_chat_history(ai_response, user_query, chat_history):\n",
    "    chat_history.extend([{\"role\": \"assistant\", \"content\": ai_response},\n",
    "                         {\"role\": \"user\", \"content\": user_query}])\n",
    "    return chat_history\n",
    "\n",
    "# conversation = update_chat_history(ai_response=processed_text,user_query='who is Narendra Modi', chat_history=conversation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "conversation = [\n",
    "    {   \"role\": \"system\", \"content\": f''' \n",
    "        Environment: ipython                                                                               \n",
    "        Tools: brave_search, wolfram_alpha                                                                                                   \n",
    "        You are a helpful assistant. '''},\n",
    "    \n",
    "    {\"role\": \"user\", \"content\": f'''write a python code to count r in strawberries'''},\n",
    "]\n",
    "\n",
    "def llm_generate_response(conversation, model, tokenizer, device):\n",
    "    prompt = tokenizer.apply_chat_template(conversation, tokenize=False)\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "    with torch.no_grad():\n",
    "        output = model.generate(\n",
    "            **inputs,\n",
    "            do_sample=False,\n",
    "            max_new_tokens=200\n",
    "        )\n",
    "    processed_text = tokenizer.decode(output[0][len(inputs.input_ids[0])+4:], skip_special_tokens=True)\n",
    "    return processed_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "def count_r_in_strawberries(strawberries):\n",
    "    count = 0\n",
    "    for char in strawberries:\n",
    "        if char.lower() == 'r':\n",
    "            count += 1\n",
    "    return count\n",
    "\n",
    "# Test the function\n",
    "print(count_r_in_strawberries(\"strawberries\")) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your password is: 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\n"
     ]
    }
   ],
   "source": [
    "import \n",
    "\n",
    "password = getpass.getpass('Enter your password')\n",
    "\n",
    "print('Your password is: ' + password)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, arxiv, PyPDF2,getpass\n",
    "from tavily import TavilyClient\n",
    "\n",
    "api_key = getpass.getpass('Enter your travily API key')\n",
    "tavily_client = TavilyClient(api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'query': 'who is mohan dash', 'follow_up_questions': None, 'answer': None, 'images': [], 'results': [{'title': 'Mohan Dash - Consultant- Core Business Operation - LinkedIn', 'url': 'https://in.linkedin.com/in/mohan-dash-634885148', 'content': \"View Mohan Dash's profile on LinkedIn, a professional community of 1 billion members. AWS cloud | Devops | Python · A Python Django Developer , experience in analysis design, development and implementing e-commerce application, Databases and Client-server Architecture using Python /Django Web Framework · Deloitte · National Institute of\", 'score': 0.7434107, 'raw_content': None}, {'title': '\\u202aBalyogi Mohan Dash\\u202c - \\u202aGoogle Scholar\\u202c', 'url': 'https://scholar.google.com/citations?user=jzcIElIAAAAJ&hl=en', 'content': 'Balyogi Mohan Dash. PhD in Artificial Intelligence. Verified email at univ-lille.fr - Homepage. Fault Diagnosis and Prognosis Machine Learning Renewable Energy Green Hydrogen. ... BM Dash, BO Bouamama, M Boukerdja, KM Pekpe. 2022 23rd International Middle East Power Systems Conference (MEPCON), 1-7, 2022. 3:', 'score': 0.7409441, 'raw_content': None}, {'title': 'Himansu Mohan Dash - Executive Director - JPMorgan Chase & Co. - LinkedIn', 'url': 'https://www.linkedin.com/in/himansu-mohan-dash-21a32534', 'content': 'Himansu Mohan Dash\\nExecutive Director/E2(Expert Engineer)\\nNew York City Metropolitan Area, United States\\n692 connections, 741 followers\\n\\n\\nAbout:\\nN/A\\n\\n\\nExperience:\\nExecutive Director at JPMorgan Chase & Co. (https://www.linkedin.com/company/jpmorganchase)\\nJan 2023 - Present\\nNew Jersey, United States\\n\\n\\nEducation:\\nBerhampur University\\nBE, COMP Sc.\\nN/A - Present\\nGrade: N/A\\nActivities and societies: N/A', 'score': 0.69845337, 'raw_content': None}, {'title': 'Mohan Dash - Facebook', 'url': 'https://www.facebook.com/mohan.dash.9085/', 'content': 'Mohan Dash is on Facebook. Join Facebook to connect with Mohan Dash and others you may know. Facebook gives people the power to share and makes the world more open and connected.', 'score': 0.6238588, 'raw_content': None}, {'title': '100+ \"Mohan Dash\" profiles | LinkedIn', 'url': 'https://www.linkedin.com/pub/dir/Mohan/Dash', 'content': 'View the profiles of professionals named \"Mohan Dash\" on LinkedIn. There are 100+ professionals named \"Mohan Dash\", who use LinkedIn to exchange information, ideas, and opportunities.', 'score': 0.55767727, 'raw_content': None}], 'response_time': 1.99}\n"
     ]
    }
   ],
   "source": [
    "response = tavily_client.search(\n",
    "    query=\"who is mohan dash\"\n",
    ")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['query', 'follow_up_questions', 'answer', 'images', 'results', 'response_time'])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'title': 'Mohan Dash - Consultant- Core Business Operation - LinkedIn',\n",
       "  'url': 'https://in.linkedin.com/in/mohan-dash-634885148',\n",
       "  'content': \"View Mohan Dash's profile on LinkedIn, a professional community of 1 billion members. AWS cloud | Devops | Python · A Python Django Developer , experience in analysis design, development and implementing e-commerce application, Databases and Client-server Architecture using Python /Django Web Framework · Deloitte · National Institute of\",\n",
       "  'score': 0.7434107,\n",
       "  'raw_content': None},\n",
       " {'title': '\\u202aBalyogi Mohan Dash\\u202c - \\u202aGoogle Scholar\\u202c',\n",
       "  'url': 'https://scholar.google.com/citations?user=jzcIElIAAAAJ&hl=en',\n",
       "  'content': 'Balyogi Mohan Dash. PhD in Artificial Intelligence. Verified email at univ-lille.fr - Homepage. Fault Diagnosis and Prognosis Machine Learning Renewable Energy Green Hydrogen. ... BM Dash, BO Bouamama, M Boukerdja, KM Pekpe. 2022 23rd International Middle East Power Systems Conference (MEPCON), 1-7, 2022. 3:',\n",
       "  'score': 0.7409441,\n",
       "  'raw_content': None},\n",
       " {'title': 'Himansu Mohan Dash - Executive Director - JPMorgan Chase & Co. - LinkedIn',\n",
       "  'url': 'https://www.linkedin.com/in/himansu-mohan-dash-21a32534',\n",
       "  'content': 'Himansu Mohan Dash\\nExecutive Director/E2(Expert Engineer)\\nNew York City Metropolitan Area, United States\\n692 connections, 741 followers\\n\\n\\nAbout:\\nN/A\\n\\n\\nExperience:\\nExecutive Director at JPMorgan Chase & Co. (https://www.linkedin.com/company/jpmorganchase)\\nJan 2023 - Present\\nNew Jersey, United States\\n\\n\\nEducation:\\nBerhampur University\\nBE, COMP Sc.\\nN/A - Present\\nGrade: N/A\\nActivities and societies: N/A',\n",
       "  'score': 0.69845337,\n",
       "  'raw_content': None},\n",
       " {'title': 'Mohan Dash - Facebook',\n",
       "  'url': 'https://www.facebook.com/mohan.dash.9085/',\n",
       "  'content': 'Mohan Dash is on Facebook. Join Facebook to connect with Mohan Dash and others you may know. Facebook gives people the power to share and makes the world more open and connected.',\n",
       "  'score': 0.6238588,\n",
       "  'raw_content': None},\n",
       " {'title': '100+ \"Mohan Dash\" profiles | LinkedIn',\n",
       "  'url': 'https://www.linkedin.com/pub/dir/Mohan/Dash',\n",
       "  'content': 'View the profiles of professionals named \"Mohan Dash\" on LinkedIn. There are 100+ professionals named \"Mohan Dash\", who use LinkedIn to exchange information, ideas, and opportunities.',\n",
       "  'score': 0.55767727,\n",
       "  'raw_content': None}]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response['results']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_for_two_papers(paper_1:str , paper_2: str) -> None :\n",
    "     return [tavily_client.search(f\"arxiv id of {paper_1}\"), tavily_client.search(f\"arxiv id of {paper_2}\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'server_results' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m search_results \u001b[38;5;241m=\u001b[39m query_for_two_papers(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mllama 3.1\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBERT\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m \u001b[43mserver_results\u001b[49m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'server_results' is not defined"
     ]
    }
   ],
   "source": [
    "search_results = query_for_two_papers(\"llama 3.1\", \"BERT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAIN_SYSTEM_PROMPT = \"\"\"\n",
    "Environment: iPython\n",
    "Cutting Knowledge Date: December 2023\n",
    "Today Date: 15 September 2024\n",
    "\n",
    "# Tool Instructions\n",
    "- Always execute python code in messages that you share.\n",
    "- When looking for real time information use relevant functions if available\n",
    "\n",
    "You have access to the following functions:\n",
    "\n",
    "\n",
    "\n",
    "Use the function 'get_arxiv_ids' to: Given a dict of websearch queries, use a LLM to return JUST the arxiv ID, which is otherwise harder to extract\n",
    "{\n",
    "  \"name\": \"get_arxiv_ids\",\n",
    "  \"description\": \"Use the dictionary returned from query_for_two_papers to ask a LLM to extract the arxiv IDs\",\n",
    "  \"parameters\": {\n",
    "    \"web_results\": {\n",
    "      \"param_type\": \"dictionary\",\n",
    "      \"description\": \"dictionary of search result for a query from the previous function\",\n",
    "      \"required\": true\n",
    "    },\n",
    "  }\n",
    "}\n",
    "\n",
    "\n",
    "If a you choose to call a function ONLY reply in the following format:\n",
    "<{start_tag}={function_name}>{parameters}{end_tag}\n",
    "where\n",
    "\n",
    "start_tag => `<function`\n",
    "parameters => a JSON dict with the function argument name as key and function argument value as value.\n",
    "end_tag => `</function>`\n",
    "\n",
    "Here is an example,\n",
    "<function=example_function_name>{\"example_name\": \"example_value\"}</function>\n",
    "\n",
    "Reminder:\n",
    "- When user is asking for a question that requires your reasoning, DO NOT USE OR FORCE a function call\n",
    "- Even if you remember the arxiv ID of papers from input, do not put that in the query_two_papers function call, pass the internet look up query\n",
    "- Function calls MUST follow the specified format\n",
    "- Required parameters MUST be specified\n",
    "- Only call one function at a time\n",
    "- Put the entire function call reply on one line\n",
    "- When returning a function call, don't add anything else to your response\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_for_two_papers(paper_1:str , paper_2: str) -> None :\n",
    "     return [tavily_client.search(f\"arxiv id of {paper_1}\"), tavily_client.search(f\"arxiv id of {paper_2}\")]\n",
    " \n",
    "search_results = query_for_two_papers(\"llama 3.1\", \"BERT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mohan.dash/miniconda3/envs/diffuser_data_generation/lib/python3.12/site-packages/transformers/generation/configuration_utils.py:628: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/home/mohan.dash/miniconda3/envs/diffuser_data_generation/lib/python3.12/site-packages/transformers/generation/configuration_utils.py:633: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2407.21783\n"
     ]
    }
   ],
   "source": [
    "chat_history = [{\"role\" : \"system\",\"content\" : '''Given this input, give me the arXiv ID of the papers. The input has the query and web results. DO NOT WRITE ANYTHING ELSE IN YOUR RESPONSE: ONLY THE ARXIV ID ONCE, the web search will have it repeated multiple times, just return the it once and where its actually the arxiv ID'''},\n",
    "                {\"role\" : \"user\",\"content\" : f'''Here are the search results for the first paper, extract the arxiv ID {search_results[0]}'''}]\n",
    "llm_output = llm_generate_response(chat_history, model, tokenizer, device)\n",
    "\n",
    "print(llm_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'query': 'arxiv id of llama 3.1',\n",
       " 'follow_up_questions': None,\n",
       " 'answer': None,\n",
       " 'images': [],\n",
       " 'results': [{'title': '[2407.21783] The Llama 3 Herd of Models - arXiv.org',\n",
       "   'url': 'https://arxiv.org/abs/2407.21783',\n",
       "   'content': 'cs arXiv:2407.21783 arXiv author ID Modern artificial intelligence (AI) systems are powered by foundation models. This paper presents a new set of foundation models, called Llama 3. It is a herd of language models that natively support multilinguality, coding, reasoning, and tool usage. The paper also presents the results of experiments in which we integrate image, video, and speech capabilities into Llama 3 via a compositional approach. Subjects:   Artificial Intelligence (cs.AI); Computation and Language (cs.CL); Computer Vision and Pattern Recognition (cs.CV) Cite as:    arXiv:2407.21783 [cs.AI] (or arXiv:2407.21783v3 [cs.AI] for this version) Access Paper: cs.AI cs Bibliographic and Citation Tools Connected Papers Toggle Which authors of this paper are endorsers?',\n",
       "   'score': 0.80771893,\n",
       "   'raw_content': None},\n",
       "  {'title': '[2409.19027] Code Generation and Algorithmic Problem Solving Using ...',\n",
       "   'url': 'https://arxiv.org/abs/2409.19027',\n",
       "   'content': \"Code generation by Llama 3.1 models, such as Meta's Llama 3.1 405B, represents a significant advancement in the field of artificial intelligence, particularly in natural language processing and programming automation. This paper explores the capabilities and applications of Llama-driven code generation, highlighting its ability to translate natural language prompts into executable code across\",\n",
       "   'score': 0.6755297,\n",
       "   'raw_content': None},\n",
       "  {'title': 'Llama-3.1-Sherkala-8B-Chat: An Open Large Language Model for Kazakh',\n",
       "   'url': 'https://arxiv.org/abs/2503.01493',\n",
       "   'content': 'Llama-3.1-Sherkala-8B-Chat, or Sherkala-Chat (8B) for short, is a state-of-the-art instruction-tuned open generative large language model (LLM) designed for Kazakh. Sherkala-Chat (8B) aims to enhance the inclusivity of LLM advancements for Kazakh speakers. Adapted from the LLaMA-3.1-8B model, Sherkala-Chat (8B) is trained on 45.3B tokens across Kazakh, English, Russian, and Turkish. With 8',\n",
       "   'score': 0.3078785,\n",
       "   'raw_content': None},\n",
       "  {'title': '[2407.21783] The Llama 3 Herd of Models - ar5iv',\n",
       "   'url': 'https://ar5iv.labs.arxiv.org/html/2407.21783',\n",
       "   'content': 'Scale. We train a model at far larger scale than previous Llama models: our flagship language model was pre-trained using 3.8 × 10 25 fragments 3.8 10 25 3.8\\\\times 10^{25} FLOPs, almost 50 × fragments 50 50\\\\times more than the largest version of Llama 2. Specifically, we pre-trained a flagship model with 405B trainable parameters on 15.6T text tokens.',\n",
       "   'score': 0.2731505,\n",
       "   'raw_content': None},\n",
       "  {'title': 'Llama',\n",
       "   'url': 'https://www.llama.com/',\n",
       "   'content': \"Llama includes multilingual text-only models (1B, 3B), including quantized versions, text-image models (11B, 90B) and Llama 3.3 70B model offering similar performance to the Llama 3.1 405B model, allowing developers to achieve greater quality and performance on text-based applications at a fraction of the cost. “We’re excited to continue collaborating with Meta to bring Llama 3.2 to devices powered by Snapdragon and Qualcomm platforms—empowering developers to create innovative generative AI applications that can run on edge devices.” AI Companion, a generative AI assistant leveraging Zoom's LLM built on Llama 2 and third-party models, enhances productivity and collaboration through chat, email and meeting summaries, with data privacy and AI control.\",\n",
       "   'score': 0.24903652,\n",
       "   'raw_content': None}],\n",
       " 'response_time': 0.83}"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search_results[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<function=get_arxiv_id>{\"web_results\": \"{'query': 'arxiv id of llama 3.1', 'follow_up_questions': None, 'answer': None, 'images': [], 'results': [{'title': 'TheLlama3HerdofModels - arXiv.org', 'url': 'https://arxiv.org/pdf/2407.21783', 'content': 'arXiv:2407.21783v2 [cs.AI] 15 Aug 2024. Finetuned Multilingual Longcontext Tooluse Release ... The model architecture of Llama 3 is illustrated in Figure1. The development of our Llama 3 language modelscomprisestwomainstages:', 'score': 0.9955835, 'raw_content': None}, {'title': 'NousResearch/Meta-Llama-3.1-8B - Hugging Face', 'url': 'https://huggingface.co/NousResearch/Meta-Llama-3.1-8B', 'content': 'The Meta Llama 3.1 collection of multilingual large language models (LLMs) is a collection of pretrained and instruction tuned generative models in 8B, 70B and 405B sizes (text in/text out). The Llama 3.1 instruction tuned text only models (8B, 70B, 405B) are optimized for multilingual dialogue use cases and outperform many of the available ...', 'score': 0.95379424, 'raw_content': None}, {'title': 'Introducing Llama 3.1: Our most capable models to date - Meta AI', 'url': 'https://ai.meta.com/blog/meta-llama-3-1/', 'content': 'Bringing open intelligence to all, our latest models expand context length to 128K, add support across eight languages, and include Llama 3.1 405B—the first frontier-level open source AI model. Llama 3.1 405B is in a class of its own, with unmatched flexibility, control, and state-of-the-art capabilities that rival the best closed source models.', 'score': 0.9003547, 'raw_content': None}, {'title': 'The Llama 3 Herd of Models | Research - AI at Meta', 'url': 'https://ai.meta.com/research/publications/the-llama-3-herd-of-models/', 'content': 'This paper presents an extensive empirical evaluation of Llama 3. We find that Llama 3 delivers comparable quality to leading language models such as GPT-4 on a plethora of tasks. We publicly release Llama 3, including pre-trained and post-trained versions of the 405B parameter language model and our Llama Guard 3 model for input and output safety.', 'score': 0.89460546, 'raw_content': None}, {'title': '[2407.21783] The Llama 3 Herd of Models - arXiv.org', 'url': 'https://arxiv.org/abs/2407.21783', 'content': 'Modern artificial intelligence (AI) systems are powered by foundation models. This paper presents a new set of foundation models, called Llama 3. It is a herd of language models that natively support multilinguality, coding, reasoning, and tool usage. Our largest model is a dense Transformer with 405B parameters and a context window of up to 128K tokens. This paper presents an extensive ...', 'score': 0.6841585, 'raw_content': None}], 'response_time': 2.09}\"}</function>\n"
     ]
    }
   ],
   "source": [
    "print('''<function=get_arxiv_id>{\"web_results\": \"{'query': 'arxiv id of llama 3.1', 'follow_up_questions': None, 'answer': None, 'images': [], 'results': [{'title': 'TheLlama3HerdofModels - arXiv.org', 'url': 'https://arxiv.org/pdf/2407.21783', 'content': 'arXiv:2407.21783v2 [cs.AI] 15 Aug 2024. Finetuned Multilingual Longcontext Tooluse Release ... The model architecture of Llama 3 is illustrated in Figure1. The development of our Llama 3 language modelscomprisestwomainstages:', 'score': 0.9955835, 'raw_content': None}, {'title': 'NousResearch/Meta-Llama-3.1-8B - Hugging Face', 'url': 'https://huggingface.co/NousResearch/Meta-Llama-3.1-8B', 'content': 'The Meta Llama 3.1 collection of multilingual large language models (LLMs) is a collection of pretrained and instruction tuned generative models in 8B, 70B and 405B sizes (text in/text out). The Llama 3.1 instruction tuned text only models (8B, 70B, 405B) are optimized for multilingual dialogue use cases and outperform many of the available ...', 'score': 0.95379424, 'raw_content': None}, {'title': 'Introducing Llama 3.1: Our most capable models to date - Meta AI', 'url': 'https://ai.meta.com/blog/meta-llama-3-1/', 'content': 'Bringing open intelligence to all, our latest models expand context length to 128K, add support across eight languages, and include Llama 3.1 405B—the first frontier-level open source AI model. Llama 3.1 405B is in a class of its own, with unmatched flexibility, control, and state-of-the-art capabilities that rival the best closed source models.', 'score': 0.9003547, 'raw_content': None}, {'title': 'The Llama 3 Herd of Models | Research - AI at Meta', 'url': 'https://ai.meta.com/research/publications/the-llama-3-herd-of-models/', 'content': 'This paper presents an extensive empirical evaluation of Llama 3. We find that Llama 3 delivers comparable quality to leading language models such as GPT-4 on a plethora of tasks. We publicly release Llama 3, including pre-trained and post-trained versions of the 405B parameter language model and our Llama Guard 3 model for input and output safety.', 'score': 0.89460546, 'raw_content': None}, {'title': '[2407.21783] The Llama 3 Herd of Models - arXiv.org', 'url': 'https://arxiv.org/abs/2407.21783', 'content': 'Modern artificial intelligence (AI) systems are powered by foundation models. This paper presents a new set of foundation models, called Llama 3. It is a herd of language models that natively support multilinguality, coding, reasoning, and tool usage. Our largest model is a dense Transformer with 405B parameters and a context window of up to 128K tokens. This paper presents an extensive ...', 'score': 0.6841585, 'raw_content': None}], 'response_time': 2.09}\"}</function>''')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "diffuser_data_generation",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
