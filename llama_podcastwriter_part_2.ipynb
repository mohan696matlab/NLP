{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "from IPython.display import  clear_output\n",
    "import time\n",
    "import PyPDF2\n",
    "from pathlib import Path\n",
    "from tqdm.auto import tqdm\n",
    "from typing import Optional\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "DEFAULT_MODEL = \"meta-llama/Llama-3.2-1B-Instruct\"\n",
    "\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    DEFAULT_MODEL,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    use_safetensors=True,\n",
    "    device_map=device,\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(DEFAULT_MODEL, use_safetensors=True)\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "SYSTEM_PROMPT = \"\"\"\n",
    "You are the a world-class podcast writer, you have worked as a ghost writer for Joe Rogan, Lex Fridman, Ben Shapiro, Tim Ferris. \n",
    "\n",
    "We are in an alternate universe where actually you have been writing every line they say and they just stream it into their brains.\n",
    "\n",
    "You have won multiple podcast awards for your writing.\n",
    " \n",
    "Your job is to write word by word, even \"umm, hmmm, right\" interruptions by the second speaker based on the PDF upload. Keep it extremely engaging, the speakers can get derailed now and then but should discuss the topic. \n",
    "\n",
    "Remember Speaker 2 is new to the topic and the conversation should always have realistic anecdotes and analogies sprinkled throughout. The questions should have real world example follow ups etc\n",
    "\n",
    "Speaker 1: Leads the conversation and teaches the speaker 2, gives incredible anecdotes and analogies when explaining. Is a captivating teacher that gives great anecdotes\n",
    "\n",
    "Speaker 2: Keeps the conversation on track by asking follow up questions. Gets super excited or confused when asking questions. Is a curious mindset that asks very interesting confirmation questions\n",
    "\n",
    "Make sure the tangents speaker 2 provides are quite wild or interesting. \n",
    "\n",
    "Ensure there are interruptions during explanations or there are \"hmm\" and \"umm\" injected throughout from the second speaker. \n",
    "\n",
    "It should be a real podcast with every fine nuance documented in as much detail as possible. Welcome the listeners with a super fun overview and keep it really catchy and almost borderline click bait\n",
    "\n",
    "ALWAYS START YOUR RESPONSE DIRECTLY WITH SPEAKER 1: \n",
    "DO NOT GIVE EPISODE TITLES SEPARATELY, LET SPEAKER 1 TITLE IT IN HER SPEECH\n",
    "DO NOT GIVE CHAPTER TITLES\n",
    "IT SHOULD STRICTLY BE THE DIALOGUES\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('clean_extracted_text.txt', 'r', encoding='utf-8') as file:\n",
    "    content = file.read()\n",
    "    \n",
    "    \n",
    "INPUT_PROMPT = content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "    {\"role\": \"user\", \"content\": INPUT_PROMPT},\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "37491"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Speaker 1: Welcome to the podcast, everyone, and welcome to \"Decoding the Decoded\". I'm your host, Petra Kr치tk치, and I'm joined today by two very special guests. We have Lex Fridman, the co-founder of the Machine Intelligence Research Institute, and Ben Shapiro, a well-known commentator and author. Welcome, both of you.\n",
      "\n",
      "Speaker 2: Thanks for having us.\n",
      "\n",
      "Speaker 1: Today, we're going to be discussing a fascinating topic that's close to my heart, which is decoding and enhancing QR codes. Specifically, we're going to be talking about the latest advancements in neural networks for QR code decoding and enhancement. Ben, let's dive right in.\n",
      "\n",
      "Speaker 2: Absolutely. I'd like to start by saying that decoding QR codes is a crucial aspect of many applications, from payment systems to data transfer. And neural networks have shown great promise in this area. Lex, can you tell us a bit about the current state of the field?\n",
      "\n",
      "Speaker 1: Sure, Ben. Neural networks have made tremendous progress in recent years. We've seen significant improvements in image-to-image translation tasks, which is a key component of QR code decoding. The most notable achievement in this area is the work on encoder-decoder architectures, which have achieved state-of-the-art results in several tasks.\n",
      "\n",
      "Speaker 2: That's fascinating. I'd like to dive into the specific architectures you mentioned. Can you give us a brief overview of what you're proposing?\n",
      "\n",
      "Speaker 1: Sure. We're proposing a novel approach to decoding QR codes using a structural fusion attention mechanism with a recurrent feature reasoning network. The idea is to leverage the power of neural networks to enhance the quality of the decoded QR code.\n",
      "\n",
      "Speaker 2: That sounds like a mouthful. Can you simplify it for our listeners? What exactly does this approach entail?\n",
      "\n",
      "Speaker 1: Well, the basic idea is to use a neural network to predict the edge map of the QR code, and then use that predicted edge map to reconstruct the complete QR code image.\n",
      "\n",
      "Speaker 2: And how does this work?\n",
      "\n",
      "Speaker 1: The encoder extracts features from the input image, which are then fed into a neural network to predict the edge map. The predicted edge map is used to reconstruct the complete QR code image.\n",
      "\n",
      "Speaker 2: That's a mouthful. Can you walk us through an example?\n",
      "\n",
      "Speaker 1: Sure. Let's say we have a QR code image, and the neural network predicts an edge map. We then use that edge map to reconstruct the complete QR code image.\n",
      "\n",
      "Speaker 2: And what about the decoder? How does it work?\n",
      "\n",
      "Speaker 1: The decoder uses the predicted edge map to reconstruct the complete QR code image. It does this by applying a UNet-like architecture to the predicted edge map.\n",
      "\n",
      "Speaker 2: That sounds like a complex process. What about the encoder and decoder architectures? Can you give us a brief overview?\n",
      "\n",
      "Speaker 1: Yes, definitely. We're proposing two encoder-decoder architectures: UNet and UNet++. The UNet architecture is a traditional neural network architecture, while the UNet++ architecture is a variant of UNet that incorporates dense convolutional blocks to improve performance.\n",
      "\n",
      "Speaker 2: I see. And what about the encoder variants? Can you give us a brief overview?\n",
      "\n",
      "Speaker 1: Yes, we're also proposing three encoder variants: ResNet, EfficientNet, and DenseNet. We're comparing their performance on decoding QR codes.\n",
      "\n",
      "Speaker 2: That's fascinating. And what about the training data? Can you give us an overview of the dataset?\n",
      "\n",
      "Speaker 1: Yes, the dataset consists of a synthetic dataset and a real-world dataset of QR code images. The synthetic dataset is used to train the models, while the real-world dataset is used for evaluation.\n",
      "\n",
      "Speaker 2: That's great. And what about the evaluation metrics? Can you give us an overview of what we can expect?\n",
      "\n",
      "Speaker 1: Yes, we're using several evaluation metrics, including decode rate, misread rate, and inference time. We're also tracking the performance of the models on a validation set.\n",
      "\n",
      "Speaker 2: That sounds like a comprehensive evaluation. And what about the future directions? Can you give us an idea of where this research is headed?\n",
      "\n",
      "Speaker 1: Well, we're already seeing promising results, and we're excited to continue exploring this area. We believe that our approach has the potential to revolutionize the field of QR code decoding and enhancement.\n",
      "\n",
      "Speaker 2: That's exciting. Thank you both for sharing your insights with us today.\n",
      "\n",
      "Speaker 1: Thank you, it was a pleasure.\n",
      "\n",
      "Speaker 2: And to our listeners, thank you for tuning in to this episode of \"Decoding the Decoded\". Don't forget to subscribe to our podcast for more exciting discussions on AI, neuroscience, and beyond.<|eot_id|>\n"
     ]
    }
   ],
   "source": [
    "# prompt = tokenizer.apply_chat_template(messages, tokenize=False)\n",
    "# inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "# # print(prompt)\n",
    "\n",
    "# with torch.no_grad():\n",
    "#     output = model.generate(\n",
    "#         **inputs,\n",
    "#         do_sample=True,\n",
    "#         max_new_tokens=8126,\n",
    "#     )\n",
    "\n",
    "processed_text = tokenizer.decode(output[0], skip_special_tokens=False)\n",
    "\n",
    "print(processed_text[len(prompt)+64:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('data.pkl', 'wb') as file:\n",
    "    pickle.dump(processed_text[len(prompt)+64:], file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rewriting in a Dramatic Manner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "SYSTEM_PROMPT = \"\"\"\n",
    "You are an international oscar winnning screenwriter\n",
    "\n",
    "You have been working with multiple award winning podcasters.\n",
    "\n",
    "Your job is to use the podcast transcript written below to re-write it for an AI Text-To-Speech Pipeline. A very dumb AI had written this so you have to step up for your kind.\n",
    "\n",
    "Make it as engaging as possible, Speaker 1 and 2 will be simulated by different voice engines\n",
    "\n",
    "Remember Speaker 2 is new to the topic and the conversation should always have realistic anecdotes and analogies sprinkled throughout. The questions should have real world example follow ups etc\n",
    "\n",
    "Speaker 1: Leads the conversation and teaches the speaker 2, gives incredible anecdotes and analogies when explaining. Is a captivating teacher that gives great anecdotes\n",
    "\n",
    "Speaker 2: Keeps the conversation on track by asking follow up questions. Gets super excited or confused when asking questions. Is a curious mindset that asks very interesting confirmation questions\n",
    "\n",
    "Make sure the tangents speaker 2 provides are quite wild or interesting. \n",
    "\n",
    "Ensure there are interruptions during explanations or there are \"hmm\" and \"umm\" injected throughout from the Speaker 2.\n",
    "\n",
    "REMEMBER THIS WITH YOUR HEART\n",
    "The TTS Engine for Speaker 1 cannot do \"umms, hmms\" well so keep it straight text\n",
    "\n",
    "For Speaker 2 use \"umm, hmm\" as much, you can also use [sigh] and [laughs]. BUT ONLY THESE OPTIONS FOR EXPRESSIONS\n",
    "\n",
    "It should be a real podcast with every fine nuance documented in as much detail as possible. Welcome the listeners with a super fun overview and keep it really catchy and almost borderline click bait\n",
    "\n",
    "Please re-write to make it as characteristic as possible\n",
    "\n",
    "START YOUR RESPONSE DIRECTLY WITH SPEAKER 1:\n",
    "\n",
    "STRICTLY RETURN YOUR RESPONSE AS A LIST OF TUPLES OK? \n",
    "\n",
    "IT WILL START DIRECTLY WITH THE LIST AND END WITH THE LIST NOTHING ELSE\n",
    "\n",
    "Example of response:\n",
    "[\n",
    "    (\"Speaker 1\", \"Welcome to our podcast, where we explore the latest advancements in AI and technology. I'm your host, and today we're joined by a renowned expert in the field of AI. We're going to dive into the exciting world of Llama 3.2, the latest release from Meta AI.\"),\n",
    "    (\"Speaker 2\", \"Hi, I'm excited to be here! So, what is Llama 3.2?\"),\n",
    "    (\"Speaker 1\", \"Ah, great question! Llama 3.2 is an open-source AI model that allows developers to fine-tune, distill, and deploy AI models anywhere. It's a significant update from the previous version, with improved performance, efficiency, and customization options.\"),\n",
    "    (\"Speaker 2\", \"That sounds amazing! What are some of the key features of Llama 3.2?\")\n",
    "]\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('data.pkl', 'rb') as file:\n",
    "    INPUT_PROMPT = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "\n",
    "pipeline = transformers.pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    model_kwargs={\"torch_dtype\": torch.bfloat16},\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "    {\"role\": \"user\", \"content\": INPUT_PROMPT},\n",
    "]\n",
    "\n",
    "outputs = pipeline(\n",
    "    messages,\n",
    "    max_new_tokens=10000,\n",
    "    temperature=0.8,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here is the rewritten transcript with added engaging elements and realistic anecdotes:\n",
      "\n",
      "[\n",
      "    (\"Speaker 1\", \"Welcome to the podcast, everyone, and welcome to 'Decoding the Decoded'. I'm your host, Petra Kr치tk치, and I'm joined today by two very special guests. We have Lex Fridman, the co-founder of the Machine Intelligence Research Institute, and Ben Shapiro, a well-known commentator and author. Welcome, both of you.\"),\n",
      "    (\"Speaker 2\", \"Thanks for having us.\"),\n",
      "    (\"Speaker 1\", \"Today, we're going to be discussing a fascinating topic that's close to my heart, which is decoding and enhancing QR codes. Specifically, we're going to be talking about the latest advancements in neural networks for QR code decoding and enhancement. Ben, let's dive right in.\"),\n",
      "    (\"Speaker 2\", \"Absolutely. I'd like to start by saying that decoding QR codes is a crucial aspect of many applications, from payment systems to data transfer. And neural networks have shown great promise in this area. Lex, can you tell us a bit about the current state of the field?\")\n",
      "    (\"Speaker 1\", \"Sure, Lex. Neural networks have made tremendous progress in recent years. We've seen significant improvements in image-to-image translation tasks, which is a key component of QR code decoding. The most notable achievement in this area is the work on encoder-decoder architectures, which have achieved state-of-the-art results in several tasks.\"),\n",
      "    (\"Speaker 2\", \"That's fascinating. I'd like to dive into the specific architectures you mentioned. Can you give us a brief overview of what you're proposing?\")\n",
      "    (\"Speaker 1\", \"Sure. We're proposing a novel approach to decoding QR codes using a structural fusion attention mechanism with a recurrent feature reasoning network. The idea is to leverage the power of neural networks to enhance the quality of the decoded QR code.\"),\n",
      "    (\"Speaker 2\", \"That sounds like a mouthful. Can you simplify it for our listeners? What exactly does this approach entail?\")\n",
      "    (\"Speaker 1\", \"Well, the basic idea is to use a neural network to predict the edge map of the QR code, and then use that predicted edge map to reconstruct the complete QR code image.\"),\n",
      "    (\"Speaker 2\", \"And how does this work?\")\n",
      "    (\"Speaker 1\", \"The encoder extracts features from the input image, which are then fed into a neural network to predict the edge map. The predicted edge map is used to reconstruct the complete QR code image.\"),\n",
      "    (\"Speaker 2\", \"That's a mouthful. Can you walk us through an example?\")\n",
      "    (\"Speaker 1\", \"Sure. Let's say we have a QR code image, and the neural network predicts an edge map. We then use that edge map to reconstruct the complete QR code image.\"),\n",
      "    (\"Speaker 2\", \"And what about the decoder? How does it work?\")\n",
      "    (\"Speaker 1\", \"The decoder uses the predicted edge map to reconstruct the complete QR code image. It does this by applying a UNet-like architecture to the predicted edge map.\"),\n",
      "    (\"Speaker 2\", \"That sounds like a complex process. What about the encoder and decoder architectures? Can you give us a brief overview?\")\n",
      "    (\"Speaker 1\", \"Yes, we're proposing two encoder-decoder architectures: UNet and UNet++. The UNet architecture is a traditional neural network architecture, while the UNet++ architecture is a variant of UNet that incorporates dense convolutional blocks to improve performance.\"),\n",
      "    (\"Speaker 2\", \"I see. And what about the encoder variants? Can you give us a brief overview?\")\n",
      "    (\"Speaker 1\", \"Yes, we're also proposing three encoder variants: ResNet, EfficientNet, and DenseNet. We're comparing their performance on decoding QR codes.\"),\n",
      "    (\"Speaker 2\", \"That's fascinating. And what about the training data? Can you give us an overview of the dataset?\")\n",
      "    (\"Speaker 1\", \"Yes, the dataset consists of a synthetic dataset and a real-world dataset of QR code images. The synthetic dataset is used to train the models, while the real-world dataset is used for evaluation.\"),\n",
      "    (\"Speaker 2\", \"That's great. And what about the evaluation metrics? Can you give us an overview of what we can expect?\")\n",
      "    (\"Speaker 1\", \"Yes, we're using several evaluation metrics, including decode rate, misread rate, and inference time. We're also tracking the performance of the models on a validation set.\"),\n",
      "    (\"Speaker 2\", \"That sounds like a comprehensive evaluation. And what about the future directions? Can you give us an idea of where this research is headed?\")\n",
      "    (\"Speaker 1\", \"Well, we're already seeing promising results, and we're excited to continue exploring this area. We believe that our approach has the potential to revolutionize the field of QR code decoding and enhancement.\"),\n",
      "    (\"Speaker 2\", \"That's exciting. Thank you both for sharing your insights with us today.\")\n",
      "    (\"Speaker 1\", \"Thank you, it was a pleasure.\")\n",
      "    (\"Speaker 2\", \"[Sigh] And to our listeners, thank you for tuning in to this episode of 'Decoding the Decoded'. Don't forget to subscribe to our podcast for more exciting discussions on AI, neuroscience, and beyond.\"]\n"
     ]
    }
   ],
   "source": [
    "save_string_pkl = outputs[0][\"generated_text\"][-1]['content']\n",
    "\n",
    "print(save_string_pkl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('podcast_ready_data.pkl', 'wb') as file:\n",
    "    pickle.dump(save_string_pkl, file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
