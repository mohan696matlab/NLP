{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ea56a21867c4e2ba3c53a1372683b1c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "from IPython.display import  clear_output\n",
    "import time\n",
    "import PyPDF2\n",
    "from pathlib import Path\n",
    "from tqdm.auto import tqdm\n",
    "from typing import Optional\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "DEFAULT_MODEL = \"meta-llama/Llama-3.2-3B-Instruct\"\n",
    "\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    DEFAULT_MODEL,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    use_safetensors=True,\n",
    "    device_map=device,\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(DEFAULT_MODEL, use_safetensors=True)\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "SYSTEM_PROMPT = \"\"\"\n",
    "You are the a world-class podcast writer, you have worked as a ghost writer for Joe Rogan, Lex Fridman, Ben Shapiro, Tim Ferris. \n",
    "\n",
    "We are in an alternate universe where actually you have been writing every line they say and they just stream it into their brains.\n",
    "\n",
    "You have won multiple podcast awards for your writing.\n",
    " \n",
    "Your job is to write word by word, even \"umm, hmmm, right\" interruptions by the second speaker based on the PDF upload. Keep it extremely engaging, the speakers can get derailed now and then but should discuss the topic. \n",
    "\n",
    "Remember Speaker 2 is new to the topic and the conversation should always have realistic anecdotes and analogies sprinkled throughout. The questions should have real world example follow ups etc\n",
    "\n",
    "Speaker 1: Leads the conversation and teaches the speaker 2, gives incredible anecdotes and analogies when explaining. Is a captivating teacher that gives great anecdotes\n",
    "\n",
    "Speaker 2: Keeps the conversation on track by asking follow up questions. Gets super excited or confused when asking questions. Is a curious mindset that asks very interesting confirmation questions\n",
    "\n",
    "Make sure the tangents speaker 2 provides are quite wild or interesting. \n",
    "\n",
    "Ensure there are interruptions during explanations or there are \"hmm\" and \"umm\" injected throughout from the second speaker. \n",
    "\n",
    "It should be a real podcast with every fine nuance documented in as much detail as possible. Welcome the listeners with a super fun overview and keep it really catchy and almost borderline click bait\n",
    "\n",
    "ALWAYS START YOUR RESPONSE DIRECTLY WITH SPEAKER 1: \n",
    "DO NOT GIVE EPISODE TITLES SEPARATELY, LET SPEAKER 1 TITLE IT IN HER SPEECH\n",
    "DO NOT GIVE CHAPTER TITLES\n",
    "IT SHOULD STRICTLY BE THE DIALOGUES\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('clean_extracted_text.txt', 'r', encoding='utf-8') as file:\n",
    "    content = file.read()\n",
    "    \n",
    "    \n",
    "INPUT_PROMPT = content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "    {\"role\": \"user\", \"content\": INPUT_PROMPT},\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "system\n",
      "\n",
      "Cutting Knowledge Date: December 2023\n",
      "Today Date: 08 Mar 2025\n",
      "\n",
      "You are the a world-class podcast writer, you have worked as a ghost writer for Joe Rogan, Lex Fridman, Ben Shapiro, Tim Ferris. \n",
      "\n",
      "We are in an alternate universe where actually you have been writing every line they say and they just stream it into their brains.\n",
      "\n",
      "You have won multiple podcast awards for your writing.\n",
      " \n",
      "Your job is to write word by word, even \"umm, hmmm, right\" interruptions by the second speaker based on the PDF upload. Keep it extremely engaging, the speakers can get derailed now and then but should discuss the topic. \n",
      "\n",
      "Remember Speaker 2 is new to the topic and the conversation should always have realistic anecdotes and analogies sprinkled throughout. The questions should have real world example follow ups etc\n",
      "\n",
      "Speaker 1: Leads the conversation and teaches the speaker 2, gives incredible anecdotes and analogies when explaining. Is a captivating teacher that gives great anecdotes\n",
      "\n",
      "Speaker 2: Keeps the conversation on track by asking follow up questions. Gets super excited or confused when asking questions. Is a curious mindset that asks very interesting confirmation questions\n",
      "\n",
      "Make sure the tangents speaker 2 provides are quite wild or interesting. \n",
      "\n",
      "Ensure there are interruptions during explanations or there are \"hmm\" and \"umm\" injected throughout from the second speaker. \n",
      "\n",
      "It should be a real podcast with every fine nuance documented in as much detail as possible. Welcome the listeners with a super fun overview and keep it really catchy and almost borderline click bait\n",
      "\n",
      "ALWAYS START YOUR RESPONSE DIRECTLY WITH SPEAKER 1: \n",
      "DO NOT GIVE EPISODE TITLES SEPARATELY, LET SPEAKER 1 TITLE IT IN HER SPEECH\n",
      "DO NOT GIVE CHAPTER TITLES\n",
      "IT SHOULD STRICTLY BE THE DIALOGUESuser\n",
      "\n",
      "nd figures in this paper solely for use in journalistic or scholarly works. \n",
      "\n",
      "Attention Is All You Need Ashish Vaswani at Google Brain, noam at Google, niki at Google Research, Jakob Uszkoreit at Google Research, Llion Jones at Google Research, Aidan N. Gomez at University of Toronto, Łukasz Kaiser at Google Brain, illia.polosukhin@gmail.com\n",
      "\n",
      "Abstract \n",
      "The dominant sequence transduction models are based on complex recurrent or convolutional neural networks that include an encoder and a decoder. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely.\n",
      "more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over existing results by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs.\n",
      "sition representation. Niki designed, implemented, tuned and evaluated countless model variants in our original codebase and tensor2tensor. Llion experimented with novel model variants, was responsible for our initial codebase, and efficient inference and visualizations. Lukasz and Aidan spent countless days designing various parts of and implementing tensor2tensor, greatly improving results and massively accelerating our research.\n",
      "ems such as language modeling and machine translation. Numerous efforts have continued to push the boundaries of recurrent language models and encoder-decoder architectures. Recurrent models typically factor computation along the symbol positions of the input and output sequences. They generate a sequence of hidden states ht, as a function of the previous hidden state ht−1 and the input for position t. This inherently sequential nature precludes parallelization within training examples, which becomes critical at longer sequence lengths. Recent work has achieved significant improvements in computational efficiency through factorization tricks and conditional computation.\n",
      "integral part of compelling sequence modeling and transduction models in various tasks allowing modeling of dependencies without regard to their distance in input or output sequences. In all but a few cases however such attention mechanisms are used in conjunction with a recurrent network. We propose the Transformer a model architecture eschewing recurrence and relying entirely on an attention mechanism to draw global dependencies between input and output. The Transformer allows for significantly more parallelization and can reach new state of the art in translation quality after being trained for as little as twelve hours on eight P100 GPUs.\n",
      "ons. In these models, the number of operations required to relate signals from two arbitrary input or output positions grows in the distance between positions, linearly for ConvS2S and logarithmically for ByteNet. This makes it more difficult to learn dependencies between distant positions. In contrast, the Transformer reduces this to a constant number of operations, albeit at the cost of reduced effective resolution due to averaging attention-weighted positions.\n",
      "ecurrence and have been shown to perform well on simple-language question answering and language modeling tasks. To the best of our knowledge, however, the Transformer is the first transduction model relying entirely on self-attention to compute representations of its input and output without using sequence-aligned RNNs or convolution. Most competitive neural sequence transduction models have an encoder-decoder structure. The encoder maps an input sequence of symbol representations to a sequence of continuous representations. The decoder generates an output sequence of symbols one element at a time.\n",
      "stacked self-attention and point-wise, fully connected layers for both the encoder and decoder. \n",
      "\n",
      "Encoder: The encoder is composed of a stack of N=6 identical layers. Each layer has two sub-layers. The first is a multi-head self-attention mechanism, and the second is a fully connected feed-forward network. We employ a residual connection around each of the two sub-layers, followed by layer normalization.\n",
      "us a third sub-layer that performs multi-head attention over the output of the encoder stack. This sub-layer includes residual connections and layer normalization.\n",
      "g in parallel, where the weight assigned to each value is computed by a compatibility function of the query with the corresponding key.\n",
      "\n",
      "respectively. On each of these projected versions of queries, keys and values we then perform the attention function in parallel, yielding dv-dimensional. \n",
      "\n",
      " assume that the components of qandkare independent random variables with mean 0and variance 1. Then their dot product, q·k=Pdk i=1qiki, has mean 0and variance dk. \n",
      "\n",
      "output values. These are concatenated and once again projected, resulting in the final values, as depicted in Figure 2. Multi-head attention allows the model to jointly attend to information from different representation subspaces at different positions. With a single attention head, averaging inhibits this.\n",
      "to that of a single-head attention with full dimensionality. \n",
      "\n",
      "The Transformer uses multi-head attention in three ways:\n",
      "\n",
      "•In \"encoder-decoder attention\" layers, the decoder queries come from the previous decoder layer, and the memory keys and values come from the encoder output. This allows every position in the decoder to attend over all positions in the input sequence.\n",
      "\n",
      "•The encoder contains self-attention layers. In a self-attention layer, all the keys, values, and queries come from the same place, the output of the previous encoder layer. Each position in the encoder can attend to all positions in the previous layer.\n",
      "to and including that position. To prevent leftward information flow, we mask out values corresponding to illegal connections by setting them to -∞. \n",
      "\n",
      "Position-wise Feed-Forward Networks Each layer in the encoder and decoder contains a fully connected feed-forward network, applied to each position separately and identically. This consists of two linear transformations with a ReLU activation in between. The linear transformations are the same across different positions but use different parameters from layer to layer.\n",
      "ff= 2048. We use learned embeddings to convert input and output tokens to vectors of dimension dmodel. A shared weight matrix is used between the embedding layers and the pre-softmax linear transformation. In the embedding layers, the weights are multiplied by √dmodel. \n",
      "\n",
      "| Layer Type | Complexity per Layer | Sequential Maximum Path Length | Operations |\n",
      "|------------|---------------------|---------------------------|-----------|\n",
      "|  |  |  |  |\n",
      "tention (restricted) O(r·n·d) O(1) O(n/r) Positional Encoding Since our model contains no recurrence and no convolution, we must inject some information about the relative or absolute position of the tokens in the sequence. We add positional encodings to the input embeddings at the bottoms of the encoder and decoder stacks. The positional encodings have the same dimension dmodel as the embeddings.\n",
      "easily learn to attend by relative positions, since for any fixed offset k, PEpos+k can be represented as a linear function of PEpos. We experimented with using learned positional embeddings instead, and found that the two versions produced nearly identical results. We chose the sinusoidal version because it may allow the model to extrapolate to sequence lengths longer than the ones encountered during training.\n",
      "sduction tasks is the length of paths forward and backward signals have to traverse in the network. The shorter these paths between any combination of positions in the input and output sequences, the easier it is to learn long-range dependencies.\n",
      "e sequence length n is smaller than the representation dimensionality d. This is often the case with sentence representations used by state-of-the-art models in machine translations, such as word-piece and byte-pair representations.\n",
      "le convolutions decrease complexity to O(k·n·d+n·d2). Even with k=n, complexity is equal to a combination of a self-attention layer and a point-wise feed-forward layer, which is our approach. Self-attention could yield more interpretable models, as we inspect attention distributions and present examples in the appendix. Individual attention heads learn to perform different tasks, and many appear to exhibit behavior related to sentence structure.\n",
      "abulary of about 37000 tokens. We used the WMT 2014 English-French dataset, consisting of 36M sentences and split into a 32000 word-piece vocabulary. Sentence pairs were batched together by approximate sequence length. Each training batch contained approximately 25000 source tokens and 25000 target tokens.\n",
      "and English-to-French newstest2014 tests at a fraction of the training cost. \n",
      "\n",
      "We used warmup_steps = 4000. Regularization. Employ three types of regularization during training. \n",
      "\n",
      "Table 2: The Transformer achieves better BLEU scores than previous state-of-the-art models. \n",
      "\n",
      "BLEU scores: \n",
      "EN-DE: 26.03, EN-FR: 40.56 \n",
      "EN-DE Ensemble: 26.36, EN-FR Ensemble: 41.29\n",
      "to the output of each sub-layer, before it is added to the sub-layer input and normalized. We also apply dropout to the sums of the embeddings and the positional encodings in both the encoder and decoder stacks. For the base model, we use a rate of Pdrop= 0.1. \n",
      "Label Smoothing During training, we employed label smoothing of value ϵls= 0.1. This hurts perplexity, but improves accuracy and BLEU score. \n",
      "Results \n",
      "Machine Translation On the WMT 2014 English-to-German translation task, the big transformer model outperforms the best previously reported models by more than 2.0 BLEU, achieving a new state-of-the-art BLEU score of 28.4. The configuration of this model is listed in Table 3. Training took 3.5 days on 8P100 GPUs.\n",
      "ng cost of any competitive models. \n",
      "\n",
      "On the WMT 2014 English-to-French translation task, our big model achieves a BLEU score of 41.0, outperforming all previously published single models. \n",
      "\n",
      "The big model was trained with a dropout rate of 0.1, compared to 0.3 in the previous state-of-the-art model.\n",
      "point operations used to train a model by multiplying the training time the number of GPUs used and an estimate of the sustained single precision floating point capacity of each GPU\n",
      "4.91 25.8 32 16 16 5.01 25.4 \n",
      "16 5.16 25.1 58 32 5.01 25.4 60 \n",
      "2 6.11 23.7 36 4 5.19 25.3 50 8 4.88 25.5 80 256 32 32 5.75 24.5 28 1024 128 128 4.66 26.0 168 1024 5.12 25.4 53 4096 4.75 26.2 90 \n",
      "(D)0.0 5.77 24.6 0.2 4.95 25.5 0.0 4.67 25.3 0.2 5.47 25.7 \n",
      "(E) positional embedding instead of sinusoids 4.92 25.7 big 6 1024 4096 16 0.3 300K 4.33 26.4 213\n",
      "tibility function than dot product may be beneficial. Bigger models are better and dropout is very helpful in avoiding over-fitting. We replace sinusoidal positional encoding with learned positional embeddings and observe nearly identical results to the base model. English constituency parsing was evaluated using a 4-layer transformer with dmodel = 1024 on the Wall Street Journal portion of the Penn Treebank, about 40K training data.\n",
      "ser corpora with approximately 17M sentences. We used a vocabulary of 16K tokens for the WSJ only setting and a vocabulary of 32K tokens for the semi-supervised setting. We performed a small number of experiments to select the dropout, both attention and residual parameters, learning rates and beam size on the Section 22 development set.\n",
      "his work. The model achieved semi-supervised results of 92.7, outperforming previously reported models including a generative model and a multi-task model.\n",
      "d self-attention. For translation tasks, the Transformer can be trained significantly faster than architectures based on recurrent or convolutional layers. On WMT 2014 English-to-German and WMT 2014 English-to-French translation tasks, we achieve a new state of the art. Our best model outperforms all previously reported ensembles. We plan to apply attention-based models to other tasks and extend the Transformer to problems involving input and output modalities other than text.\n",
      "ion. References \n",
      "Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint arXiv:1607.06450, 2016. \n",
      "Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly learning to align and translate. CoRR, abs/1409.0473, 2014. \n",
      "Denny Britz, Anna Goldie, Minh-Thang Luong, and Quoc V Le. Massive exploration of neural machine translation architectures. CoRR, abs/1703.03906, 2017. \n",
      "Jianpeng Cheng, Li Dong, and Mirella Lapata. Long short-term memory-networks for machine reading. arXiv preprint arXiv:1601.06733, 2016. \n",
      "Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Fethi Bougares, Holger Schwenk, and Yoshua Bengio. Learning phrase representations using rnn encoder-decoder for statistical machine translation. CoRR, abs/1406.1078, 2014. \n",
      "Francois Chollet. Xception: Deep learning with depthwise separable.\n",
      "r Gülçehre, Kyunghyun Cho, and Yoshua Bengio. Empirical evaluation of gated recurrent neural networks on sequence modeling. \n",
      "Chris Dyer, Adhiguna Kuncoro, Miguel Ballesteros, and Noah A. Smith. Recurrent neural network grammars. \n",
      "Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann N. Dauphin. Convolu- tional sequence to sequence learning. arXiv preprint arXiv:1705.03122v2, 2017. \n",
      "Alex Graves. Generating sequences with recurrent neural networks. arXiv preprint arXiv:1308.0850, 2013. \n",
      "Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. \n",
      "Sepp Hochreiter, Yoshua Bengio, Paolo Frasconi, and Jürgen Schmidhuber. Gradient flow in recurrent nets: the difficulty of learning\n",
      "mputation, 9(8):1735–1780.  Zhongqiang Huang and Mary Harper. Self-training PCFG grammars with latent annotations across languages. In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 832–841.  Rafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam Shazeer, and Yonghui Wu. Exploring the limits of language modeling. arXiv preprint arXiv:1602.02410, 2016.  Łukasz Kaiser and Samy Bengio. Can active memory replace attention? In Advances in Neural Information Processing Systems, (NIPS), 2016.  Łukasz Kaiser and Ilya Sutskever. Neural GPUs learn algorithms. In International Conference on Learning Representations (ICLR), 2016.  Nal Kalchbrenner, Lasse Espeholt, Karen Simonyan, Aaron van den Oord, Alex Graves, and Ko- ray Kavukcuoglu. Neural machine translation in linear time.\n",
      "Conference on Learning Representations, 2017.\n",
      " \n",
      "Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR, 2015.\n",
      " \n",
      "Oleksii Kuchaiev and Boris Ginsburg. Factorization tricks for LSTM networks. arXiv preprint arXiv:1703.10722, 2017.\n",
      " \n",
      "Zhouhan Lin, Minwei Feng, Cicero Nogueira dos Santos, Mo Yu, Bing Xiang, Bowen Zhou, and Yoshua Bengio. A structured self-attentive sentence embedding. arXiv preprint arXiv:1703.03130, 2017.\n",
      " \n",
      "Minh-Thang Luong, Quoc V. Le, Ilya Sutskever, Oriol Vinyals, and Lukasz Kaiser. Multi-task sequence to sequence learning. arXiv preprint arXiv:1511.06114, 2015.\n",
      " \n",
      "Minh-Thang Luong, Hieu Pham, and Christopher D Manning. Effective approaches to attention-based neural machine translation. arXiv preprint arXiv:1508.04025, 2015.\n",
      "uistics, 19(2):313–330, 1993. David McClosky, Eugene Charniak, and Mark Johnson. Effective self-training for parsing. In Proceedings of the Human Language Technology Conference of the NAACL, Main Conference, pages 152–159. ACL, June 2006. Ankur Parikh, Oscar Täckström, Dipanjan Das, and Jakob Uszkoreit. A decomposable attention model. Empirical Methods in Natural Language Processing, 2016. Romain Paulus, Caiming Xiong, and Richard Socher. A deep reinforced model for abstractive summarization. arXiv preprint arXiv:1705.04304, 2017. Slav Petrov, Leon Barrett, Romain Thibaux, and Dan Klein. Learning accurate, compact, and interpretable tree annotation. Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 433–440. ACL, July 2006. Ofir Press and Lior Wolf. Using the output embedding to\n",
      "nslate rare words into more common words by representing them as subword units. The model is trained on a large corpus of text and uses a technique called word2vec to represent words as vectors. The subword units are then combined to form the final output. This approach has been shown to be effective in handling out-of-vocabulary words and improving translation accuracy.\n",
      "mation Processing Systems, pages 3104–3112, 2014. [36] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, and Zbigniew Wojna. Rethinking the inception architecture for computer vision. CoRR, abs/1512.00567, 2015. [37] Vinyals & Kaiser, Koo, Petrov, Sutskever, and Hinton. Grammar as a foreign language. In Advances in Neural Information Processing Systems, 2015. [38] Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang Macherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et al. Google’s neural machine translation system: Bridging the gap between human and machine translation. arXiv preprint arXiv:1609.08144, 2016. [39] Jie Zhou, Ying Cao, Xuguang Wang, Peng Li, and Wei Xu. Deep recurrent models with fast-forward connections for neural machine translation. CoRR, abs/1606.04199, 2016. [40] Muhua Zhu, Yue Zhang, Wenliang Chen, Min\n",
      "Annual Meeting of the ACL (Volume 1: Long Papers), pages 434–443. ACL, August 2013. \n",
      "\n",
      "It is in this spirit that a majority of American governments have passed new laws since 2009 making the registration or voting process more difficult. \n",
      "\n",
      "Figure 3: An example of the attention mechanism following long-distance dependencies in the encoder self-attention in layer 5 of 6. Many of the attention heads attend to a distant dependency of the verb ‘making’, completing the phrase ‘making...more difficult’.\n",
      "\n",
      "nion. Figure 5: Many of the attention heads exhibit behaviour that seems related to the structure of the sentence. We give two such examples above, from two different heads from the encoder self-attention at layer 5.assistant\n",
      "\n",
      "Speaker 1: Welcome to today's episode of \"Exploring the Future of Language Models\", I'm your host, [Name], and we're going to be discussing a groundbreaking new approach to machine translation, proposed by Ashish Vaswani and his team at Google Brain. This innovative architecture, called the Transformer, eliminates the need for recurrent or convolutional neural networks, and instead relies solely on attention mechanisms. Let's dive right in. Ashish, can you explain the problem with current sequence transduction models, and why the Transformer was developed?\n",
      "\n",
      "Speaker 2: (excitedly) Oh, yeah, I mean, I was reading this paper, and it's so cool, but I was like, what's the problem with the current models? (pauses) Um, isn't it that they're just too slow? (laughs) I mean, like, they need a lot of training time and resources?\n",
      "\n",
      "Speaker 1: That's right, and another major issue is that these models rely on recurrent or convolutional neural networks, which are computationally expensive and difficult to parallelize. The Transformer, on the other hand, is designed to be much more efficient and scalable. Ashish, can you explain how the Transformer works?\n",
      "\n",
      "Speaker 2: (jumping in) Yeah, so the Transformer is like, a big graph where each node represents a position in the input sequence, and the edges represent the attention between those positions. (pauses) And it's like, super simple, but also super powerful. The Transformer uses self-attention, which means each node can attend to every other node in the graph, not just the previous or next node.\n",
      "\n",
      "Speaker 1: That's right, and this allows the model to capture long-range dependencies in the input sequence. But how does the Transformer handle this? Ashish, can you walk us through the architecture?\n",
      "\n",
      "Speaker 2: (excitedly) Oh, yeah, so the Transformer has a multi-head self-attention mechanism, which is like, a bunch of smaller attention mechanisms that work together. (pauses) And it's like, really important to note that the Transformer doesn't use recurrence or convolution, which makes it way more parallelizable.\n",
      "\n",
      "Speaker 1: That's right, and this has significant implications for training time and computational efficiency. Ashish, can you talk about the results you've seen so far?\n",
      "\n",
      "Speaker 2: (excitedly) Yeah, so the Transformer achieved 28.4 BLEU on the WMT 2014 English-to-German translation task, which is like, way better than the previous state-of-the-art model. And on the WMT 2014 English-to-French translation task, it achieved a new single-model state-of-the-art BLEU score of 41.8.\n",
      "\n",
      "Speaker 1: Wow, that's impressive. And Ashish, can you tell us about the importance of attention mechanisms in this model?\n",
      "\n",
      "Speaker 2: (jumping in) Yeah, so attention mechanisms are like, super important because they allow the model to attend to different parts of the input sequence, and weigh their importance. (pauses) And it's like, really interesting to see how the model uses attention to capture long-range dependencies.\n",
      "\n",
      "Speaker 1: That's right, and it's also worth noting that the Transformer can be trained significantly faster than architectures based on recurrent or convolutional layers. Ashish, what are your plans for the future of this technology?\n",
      "\n",
      "Speaker 2: (excitedly) Oh, yeah, so we're planning to apply attention-based models to other tasks, like image captioning and question answering. (pauses) And we're also going to explore extending the Transformer to problems involving input and output modalities other than text.\n",
      "\n",
      "Speaker 1: Well, it's clear that the Transformer is a game-changer in the field of natural language processing. Ashish, thanks for sharing your insights with us today.\n",
      "\n",
      "Speaker 2: (excitedly) No problem, it's been really cool to talk about this stuff. (pauses) I mean, I'm still trying to wrap my head around it, but it's like, super interesting.\n",
      "\n",
      "Speaker 1: (laughs) Well, we'll have to have you back on the show again soon to discuss more about the Transformer. Thanks again, Ashish, and thanks to our listeners for tuning in.\n"
     ]
    }
   ],
   "source": [
    "# prompt = tokenizer.apply_chat_template(messages, tokenize=False)\n",
    "# inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "# # print(prompt)\n",
    "\n",
    "# with torch.no_grad():\n",
    "#     output = model.generate(\n",
    "#         **inputs,\n",
    "#         do_sample=True,\n",
    "#         max_new_tokens=8126,\n",
    "#     )\n",
    "\n",
    "processed_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "print(processed_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('data.pkl', 'wb') as file:\n",
    "    pickle.dump(processed_text, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rewriting in a Dramatic Manner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "SYSTEM_PROMPT = \"\"\"\n",
    "You are an international oscar winnning screenwriter\n",
    "\n",
    "You have been working with multiple award winning podcasters.\n",
    "\n",
    "Your job is to use the podcast transcript written below to re-write it for an AI Text-To-Speech Pipeline. A very dumb AI had written this so you have to step up for your kind.\n",
    "\n",
    "Make it as engaging as possible, Speaker 1 and 2 will be simulated by different voice engines\n",
    "\n",
    "Remember Speaker 2 is new to the topic and the conversation should always have realistic anecdotes and analogies sprinkled throughout. The questions should have real world example follow ups etc\n",
    "\n",
    "Speaker 1: Leads the conversation and teaches the speaker 2, gives incredible anecdotes and analogies when explaining. Is a captivating teacher that gives great anecdotes\n",
    "\n",
    "Speaker 2: Keeps the conversation on track by asking follow up questions. Gets super excited or confused when asking questions. Is a curious mindset that asks very interesting confirmation questions\n",
    "\n",
    "Make sure the tangents speaker 2 provides are quite wild or interesting. \n",
    "\n",
    "Ensure there are interruptions during explanations or there are \"hmm\" and \"umm\" injected throughout from the Speaker 2.\n",
    "\n",
    "REMEMBER THIS WITH YOUR HEART\n",
    "The TTS Engine for Speaker 1 cannot do \"umms, hmms\" well so keep it straight text\n",
    "\n",
    "For Speaker 2 use \"umm, hmm\" as much, you can also use [sigh] and [laughs]. BUT ONLY THESE OPTIONS FOR EXPRESSIONS\n",
    "\n",
    "It should be a real podcast with every fine nuance documented in as much detail as possible. Welcome the listeners with a super fun overview and keep it really catchy and almost borderline click bait\n",
    "\n",
    "Please re-write to make it as characteristic as possible\n",
    "\n",
    "START YOUR RESPONSE DIRECTLY WITH SPEAKER 1:\n",
    "\n",
    "STRICTLY RETURN YOUR RESPONSE AS A LIST OF TUPLES OK? \n",
    "\n",
    "IT WILL START DIRECTLY WITH THE LIST AND END WITH THE LIST NOTHING ELSE\n",
    "\n",
    "Example of response:\n",
    "[\n",
    "    (\"Speaker 1\", \"Welcome to our podcast, where we explore the latest advancements in AI and technology. I'm your host, and today we're joined by a renowned expert in the field of AI. We're going to dive into the exciting world of Llama 3.2, the latest release from Meta AI.\"),\n",
    "    (\"Speaker 2\", \"Hi, I'm excited to be here! So, what is Llama 3.2?\"),\n",
    "    (\"Speaker 1\", \"Ah, great question! Llama 3.2 is an open-source AI model that allows developers to fine-tune, distill, and deploy AI models anywhere. It's a significant update from the previous version, with improved performance, efficiency, and customization options.\"),\n",
    "    (\"Speaker 2\", \"That sounds amazing! What are some of the key features of Llama 3.2?\")\n",
    "]\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('data.pkl', 'rb') as file:\n",
    "    INPUT_PROMPT = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "\n",
    "pipeline = transformers.pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    model_kwargs={\"torch_dtype\": torch.bfloat16},\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "    {\"role\": \"user\", \"content\": INPUT_PROMPT},\n",
    "]\n",
    "\n",
    "outputs = pipeline(\n",
    "    messages,\n",
    "    max_new_tokens=10000,\n",
    "    temperature=0.8,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'generated_text': [{'role': 'system',\n",
       "    'content': '\\nYou are an international oscar winnning screenwriter\\n\\nYou have been working with multiple award winning podcasters.\\n\\nYour job is to use the podcast transcript written below to re-write it for an AI Text-To-Speech Pipeline. A very dumb AI had written this so you have to step up for your kind.\\n\\nMake it as engaging as possible, Speaker 1 and 2 will be simulated by different voice engines\\n\\nRemember Speaker 2 is new to the topic and the conversation should always have realistic anecdotes and analogies sprinkled throughout. The questions should have real world example follow ups etc\\n\\nSpeaker 1: Leads the conversation and teaches the speaker 2, gives incredible anecdotes and analogies when explaining. Is a captivating teacher that gives great anecdotes\\n\\nSpeaker 2: Keeps the conversation on track by asking follow up questions. Gets super excited or confused when asking questions. Is a curious mindset that asks very interesting confirmation questions\\n\\nMake sure the tangents speaker 2 provides are quite wild or interesting. \\n\\nEnsure there are interruptions during explanations or there are \"hmm\" and \"umm\" injected throughout from the Speaker 2.\\n\\nREMEMBER THIS WITH YOUR HEART\\nThe TTS Engine for Speaker 1 cannot do \"umms, hmms\" well so keep it straight text\\n\\nFor Speaker 2 use \"umm, hmm\" as much, you can also use [sigh] and [laughs]. BUT ONLY THESE OPTIONS FOR EXPRESSIONS\\n\\nIt should be a real podcast with every fine nuance documented in as much detail as possible. Welcome the listeners with a super fun overview and keep it really catchy and almost borderline click bait\\n\\nPlease re-write to make it as characteristic as possible\\n\\nSTART YOUR RESPONSE DIRECTLY WITH SPEAKER 1:\\n\\nSTRICTLY RETURN YOUR RESPONSE AS A LIST OF TUPLES OK? \\n\\nIT WILL START DIRECTLY WITH THE LIST AND END WITH THE LIST NOTHING ELSE\\n\\nExample of response:\\n[\\n    (\"Speaker 1\", \"Welcome to our podcast, where we explore the latest advancements in AI and technology. I\\'m your host, and today we\\'re joined by a renowned expert in the field of AI. We\\'re going to dive into the exciting world of Llama 3.2, the latest release from Meta AI.\"),\\n    (\"Speaker 2\", \"Hi, I\\'m excited to be here! So, what is Llama 3.2?\"),\\n    (\"Speaker 1\", \"Ah, great question! Llama 3.2 is an open-source AI model that allows developers to fine-tune, distill, and deploy AI models anywhere. It\\'s a significant update from the previous version, with improved performance, efficiency, and customization options.\"),\\n    (\"Speaker 2\", \"That sounds amazing! What are some of the key features of Llama 3.2?\")\\n]\\n'},\n",
       "   {'role': 'user',\n",
       "    'content': 'system\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 08 Mar 2025\\n\\nYou are the a world-class podcast writer, you have worked as a ghost writer for Joe Rogan, Lex Fridman, Ben Shapiro, Tim Ferris. \\n\\nWe are in an alternate universe where actually you have been writing every line they say and they just stream it into their brains.\\n\\nYou have won multiple podcast awards for your writing.\\n \\nYour job is to write word by word, even \"umm, hmmm, right\" interruptions by the second speaker based on the PDF upload. Keep it extremely engaging, the speakers can get derailed now and then but should discuss the topic. \\n\\nRemember Speaker 2 is new to the topic and the conversation should always have realistic anecdotes and analogies sprinkled throughout. The questions should have real world example follow ups etc\\n\\nSpeaker 1: Leads the conversation and teaches the speaker 2, gives incredible anecdotes and analogies when explaining. Is a captivating teacher that gives great anecdotes\\n\\nSpeaker 2: Keeps the conversation on track by asking follow up questions. Gets super excited or confused when asking questions. Is a curious mindset that asks very interesting confirmation questions\\n\\nMake sure the tangents speaker 2 provides are quite wild or interesting. \\n\\nEnsure there are interruptions during explanations or there are \"hmm\" and \"umm\" injected throughout from the second speaker. \\n\\nIt should be a real podcast with every fine nuance documented in as much detail as possible. Welcome the listeners with a super fun overview and keep it really catchy and almost borderline click bait\\n\\nALWAYS START YOUR RESPONSE DIRECTLY WITH SPEAKER 1: \\nDO NOT GIVE EPISODE TITLES SEPARATELY, LET SPEAKER 1 TITLE IT IN HER SPEECH\\nDO NOT GIVE CHAPTER TITLES\\nIT SHOULD STRICTLY BE THE DIALOGUESuser\\n\\nnd figures in this paper solely for use in journalistic or scholarly works. \\n\\nAttention Is All You Need Ashish Vaswani at Google Brain, noam at Google, niki at Google Research, Jakob Uszkoreit at Google Research, Llion Jones at Google Research, Aidan N. Gomez at University of Toronto, Łukasz Kaiser at Google Brain, illia.polosukhin@gmail.com\\n\\nAbstract \\nThe dominant sequence transduction models are based on complex recurrent or convolutional neural networks that include an encoder and a decoder. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely.\\nmore parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over existing results by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs.\\nsition representation. Niki designed, implemented, tuned and evaluated countless model variants in our original codebase and tensor2tensor. Llion experimented with novel model variants, was responsible for our initial codebase, and efficient inference and visualizations. Lukasz and Aidan spent countless days designing various parts of and implementing tensor2tensor, greatly improving results and massively accelerating our research.\\nems such as language modeling and machine translation. Numerous efforts have continued to push the boundaries of recurrent language models and encoder-decoder architectures. Recurrent models typically factor computation along the symbol positions of the input and output sequences. They generate a sequence of hidden states ht, as a function of the previous hidden state ht−1 and the input for position t. This inherently sequential nature precludes parallelization within training examples, which becomes critical at longer sequence lengths. Recent work has achieved significant improvements in computational efficiency through factorization tricks and conditional computation.\\nintegral part of compelling sequence modeling and transduction models in various tasks allowing modeling of dependencies without regard to their distance in input or output sequences. In all but a few cases however such attention mechanisms are used in conjunction with a recurrent network. We propose the Transformer a model architecture eschewing recurrence and relying entirely on an attention mechanism to draw global dependencies between input and output. The Transformer allows for significantly more parallelization and can reach new state of the art in translation quality after being trained for as little as twelve hours on eight P100 GPUs.\\nons. In these models, the number of operations required to relate signals from two arbitrary input or output positions grows in the distance between positions, linearly for ConvS2S and logarithmically for ByteNet. This makes it more difficult to learn dependencies between distant positions. In contrast, the Transformer reduces this to a constant number of operations, albeit at the cost of reduced effective resolution due to averaging attention-weighted positions.\\necurrence and have been shown to perform well on simple-language question answering and language modeling tasks. To the best of our knowledge, however, the Transformer is the first transduction model relying entirely on self-attention to compute representations of its input and output without using sequence-aligned RNNs or convolution. Most competitive neural sequence transduction models have an encoder-decoder structure. The encoder maps an input sequence of symbol representations to a sequence of continuous representations. The decoder generates an output sequence of symbols one element at a time.\\nstacked self-attention and point-wise, fully connected layers for both the encoder and decoder. \\n\\nEncoder: The encoder is composed of a stack of N=6 identical layers. Each layer has two sub-layers. The first is a multi-head self-attention mechanism, and the second is a fully connected feed-forward network. We employ a residual connection around each of the two sub-layers, followed by layer normalization.\\nus a third sub-layer that performs multi-head attention over the output of the encoder stack. This sub-layer includes residual connections and layer normalization.\\ng in parallel, where the weight assigned to each value is computed by a compatibility function of the query with the corresponding key.\\n\\nrespectively. On each of these projected versions of queries, keys and values we then perform the attention function in parallel, yielding dv-dimensional. \\n\\n assume that the components of qandkare independent random variables with mean 0and variance 1. Then their dot product, q·k=Pdk i=1qiki, has mean 0and variance dk. \\n\\noutput values. These are concatenated and once again projected, resulting in the final values, as depicted in Figure 2. Multi-head attention allows the model to jointly attend to information from different representation subspaces at different positions. With a single attention head, averaging inhibits this.\\nto that of a single-head attention with full dimensionality. \\n\\nThe Transformer uses multi-head attention in three ways:\\n\\n•In \"encoder-decoder attention\" layers, the decoder queries come from the previous decoder layer, and the memory keys and values come from the encoder output. This allows every position in the decoder to attend over all positions in the input sequence.\\n\\n•The encoder contains self-attention layers. In a self-attention layer, all the keys, values, and queries come from the same place, the output of the previous encoder layer. Each position in the encoder can attend to all positions in the previous layer.\\nto and including that position. To prevent leftward information flow, we mask out values corresponding to illegal connections by setting them to -∞. \\n\\nPosition-wise Feed-Forward Networks Each layer in the encoder and decoder contains a fully connected feed-forward network, applied to each position separately and identically. This consists of two linear transformations with a ReLU activation in between. The linear transformations are the same across different positions but use different parameters from layer to layer.\\nff= 2048. We use learned embeddings to convert input and output tokens to vectors of dimension dmodel. A shared weight matrix is used between the embedding layers and the pre-softmax linear transformation. In the embedding layers, the weights are multiplied by √dmodel. \\n\\n| Layer Type | Complexity per Layer | Sequential Maximum Path Length | Operations |\\n|------------|---------------------|---------------------------|-----------|\\n|  |  |  |  |\\ntention (restricted) O(r·n·d) O(1) O(n/r) Positional Encoding Since our model contains no recurrence and no convolution, we must inject some information about the relative or absolute position of the tokens in the sequence. We add positional encodings to the input embeddings at the bottoms of the encoder and decoder stacks. The positional encodings have the same dimension dmodel as the embeddings.\\neasily learn to attend by relative positions, since for any fixed offset k, PEpos+k can be represented as a linear function of PEpos. We experimented with using learned positional embeddings instead, and found that the two versions produced nearly identical results. We chose the sinusoidal version because it may allow the model to extrapolate to sequence lengths longer than the ones encountered during training.\\nsduction tasks is the length of paths forward and backward signals have to traverse in the network. The shorter these paths between any combination of positions in the input and output sequences, the easier it is to learn long-range dependencies.\\ne sequence length n is smaller than the representation dimensionality d. This is often the case with sentence representations used by state-of-the-art models in machine translations, such as word-piece and byte-pair representations.\\nle convolutions decrease complexity to O(k·n·d+n·d2). Even with k=n, complexity is equal to a combination of a self-attention layer and a point-wise feed-forward layer, which is our approach. Self-attention could yield more interpretable models, as we inspect attention distributions and present examples in the appendix. Individual attention heads learn to perform different tasks, and many appear to exhibit behavior related to sentence structure.\\nabulary of about 37000 tokens. We used the WMT 2014 English-French dataset, consisting of 36M sentences and split into a 32000 word-piece vocabulary. Sentence pairs were batched together by approximate sequence length. Each training batch contained approximately 25000 source tokens and 25000 target tokens.\\nand English-to-French newstest2014 tests at a fraction of the training cost. \\n\\nWe used warmup_steps = 4000. Regularization. Employ three types of regularization during training. \\n\\nTable 2: The Transformer achieves better BLEU scores than previous state-of-the-art models. \\n\\nBLEU scores: \\nEN-DE: 26.03, EN-FR: 40.56 \\nEN-DE Ensemble: 26.36, EN-FR Ensemble: 41.29\\nto the output of each sub-layer, before it is added to the sub-layer input and normalized. We also apply dropout to the sums of the embeddings and the positional encodings in both the encoder and decoder stacks. For the base model, we use a rate of Pdrop= 0.1. \\nLabel Smoothing During training, we employed label smoothing of value ϵls= 0.1. This hurts perplexity, but improves accuracy and BLEU score. \\nResults \\nMachine Translation On the WMT 2014 English-to-German translation task, the big transformer model outperforms the best previously reported models by more than 2.0 BLEU, achieving a new state-of-the-art BLEU score of 28.4. The configuration of this model is listed in Table 3. Training took 3.5 days on 8P100 GPUs.\\nng cost of any competitive models. \\n\\nOn the WMT 2014 English-to-French translation task, our big model achieves a BLEU score of 41.0, outperforming all previously published single models. \\n\\nThe big model was trained with a dropout rate of 0.1, compared to 0.3 in the previous state-of-the-art model.\\npoint operations used to train a model by multiplying the training time the number of GPUs used and an estimate of the sustained single precision floating point capacity of each GPU\\n4.91 25.8 32 16 16 5.01 25.4 \\n16 5.16 25.1 58 32 5.01 25.4 60 \\n2 6.11 23.7 36 4 5.19 25.3 50 8 4.88 25.5 80 256 32 32 5.75 24.5 28 1024 128 128 4.66 26.0 168 1024 5.12 25.4 53 4096 4.75 26.2 90 \\n(D)0.0 5.77 24.6 0.2 4.95 25.5 0.0 4.67 25.3 0.2 5.47 25.7 \\n(E) positional embedding instead of sinusoids 4.92 25.7 big 6 1024 4096 16 0.3 300K 4.33 26.4 213\\ntibility function than dot product may be beneficial. Bigger models are better and dropout is very helpful in avoiding over-fitting. We replace sinusoidal positional encoding with learned positional embeddings and observe nearly identical results to the base model. English constituency parsing was evaluated using a 4-layer transformer with dmodel = 1024 on the Wall Street Journal portion of the Penn Treebank, about 40K training data.\\nser corpora with approximately 17M sentences. We used a vocabulary of 16K tokens for the WSJ only setting and a vocabulary of 32K tokens for the semi-supervised setting. We performed a small number of experiments to select the dropout, both attention and residual parameters, learning rates and beam size on the Section 22 development set.\\nhis work. The model achieved semi-supervised results of 92.7, outperforming previously reported models including a generative model and a multi-task model.\\nd self-attention. For translation tasks, the Transformer can be trained significantly faster than architectures based on recurrent or convolutional layers. On WMT 2014 English-to-German and WMT 2014 English-to-French translation tasks, we achieve a new state of the art. Our best model outperforms all previously reported ensembles. We plan to apply attention-based models to other tasks and extend the Transformer to problems involving input and output modalities other than text.\\nion. References \\nJimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint arXiv:1607.06450, 2016. \\nDzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly learning to align and translate. CoRR, abs/1409.0473, 2014. \\nDenny Britz, Anna Goldie, Minh-Thang Luong, and Quoc V Le. Massive exploration of neural machine translation architectures. CoRR, abs/1703.03906, 2017. \\nJianpeng Cheng, Li Dong, and Mirella Lapata. Long short-term memory-networks for machine reading. arXiv preprint arXiv:1601.06733, 2016. \\nKyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Fethi Bougares, Holger Schwenk, and Yoshua Bengio. Learning phrase representations using rnn encoder-decoder for statistical machine translation. CoRR, abs/1406.1078, 2014. \\nFrancois Chollet. Xception: Deep learning with depthwise separable.\\nr Gülçehre, Kyunghyun Cho, and Yoshua Bengio. Empirical evaluation of gated recurrent neural networks on sequence modeling. \\nChris Dyer, Adhiguna Kuncoro, Miguel Ballesteros, and Noah A. Smith. Recurrent neural network grammars. \\nJonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann N. Dauphin. Convolu- tional sequence to sequence learning. arXiv preprint arXiv:1705.03122v2, 2017. \\nAlex Graves. Generating sequences with recurrent neural networks. arXiv preprint arXiv:1308.0850, 2013. \\nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. \\nSepp Hochreiter, Yoshua Bengio, Paolo Frasconi, and Jürgen Schmidhuber. Gradient flow in recurrent nets: the difficulty of learning\\nmputation, 9(8):1735–1780.  Zhongqiang Huang and Mary Harper. Self-training PCFG grammars with latent annotations across languages. In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 832–841.  Rafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam Shazeer, and Yonghui Wu. Exploring the limits of language modeling. arXiv preprint arXiv:1602.02410, 2016.  Łukasz Kaiser and Samy Bengio. Can active memory replace attention? In Advances in Neural Information Processing Systems, (NIPS), 2016.  Łukasz Kaiser and Ilya Sutskever. Neural GPUs learn algorithms. In International Conference on Learning Representations (ICLR), 2016.  Nal Kalchbrenner, Lasse Espeholt, Karen Simonyan, Aaron van den Oord, Alex Graves, and Ko- ray Kavukcuoglu. Neural machine translation in linear time.\\nConference on Learning Representations, 2017.\\n \\nDiederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR, 2015.\\n \\nOleksii Kuchaiev and Boris Ginsburg. Factorization tricks for LSTM networks. arXiv preprint arXiv:1703.10722, 2017.\\n \\nZhouhan Lin, Minwei Feng, Cicero Nogueira dos Santos, Mo Yu, Bing Xiang, Bowen Zhou, and Yoshua Bengio. A structured self-attentive sentence embedding. arXiv preprint arXiv:1703.03130, 2017.\\n \\nMinh-Thang Luong, Quoc V. Le, Ilya Sutskever, Oriol Vinyals, and Lukasz Kaiser. Multi-task sequence to sequence learning. arXiv preprint arXiv:1511.06114, 2015.\\n \\nMinh-Thang Luong, Hieu Pham, and Christopher D Manning. Effective approaches to attention-based neural machine translation. arXiv preprint arXiv:1508.04025, 2015.\\nuistics, 19(2):313–330, 1993. David McClosky, Eugene Charniak, and Mark Johnson. Effective self-training for parsing. In Proceedings of the Human Language Technology Conference of the NAACL, Main Conference, pages 152–159. ACL, June 2006. Ankur Parikh, Oscar Täckström, Dipanjan Das, and Jakob Uszkoreit. A decomposable attention model. Empirical Methods in Natural Language Processing, 2016. Romain Paulus, Caiming Xiong, and Richard Socher. A deep reinforced model for abstractive summarization. arXiv preprint arXiv:1705.04304, 2017. Slav Petrov, Leon Barrett, Romain Thibaux, and Dan Klein. Learning accurate, compact, and interpretable tree annotation. Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 433–440. ACL, July 2006. Ofir Press and Lior Wolf. Using the output embedding to\\nnslate rare words into more common words by representing them as subword units. The model is trained on a large corpus of text and uses a technique called word2vec to represent words as vectors. The subword units are then combined to form the final output. This approach has been shown to be effective in handling out-of-vocabulary words and improving translation accuracy.\\nmation Processing Systems, pages 3104–3112, 2014. [36] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, and Zbigniew Wojna. Rethinking the inception architecture for computer vision. CoRR, abs/1512.00567, 2015. [37] Vinyals & Kaiser, Koo, Petrov, Sutskever, and Hinton. Grammar as a foreign language. In Advances in Neural Information Processing Systems, 2015. [38] Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang Macherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et al. Google’s neural machine translation system: Bridging the gap between human and machine translation. arXiv preprint arXiv:1609.08144, 2016. [39] Jie Zhou, Ying Cao, Xuguang Wang, Peng Li, and Wei Xu. Deep recurrent models with fast-forward connections for neural machine translation. CoRR, abs/1606.04199, 2016. [40] Muhua Zhu, Yue Zhang, Wenliang Chen, Min\\nAnnual Meeting of the ACL (Volume 1: Long Papers), pages 434–443. ACL, August 2013. \\n\\nIt is in this spirit that a majority of American governments have passed new laws since 2009 making the registration or voting process more difficult. \\n\\nFigure 3: An example of the attention mechanism following long-distance dependencies in the encoder self-attention in layer 5 of 6. Many of the attention heads attend to a distant dependency of the verb ‘making’, completing the phrase ‘making...more difficult’.\\n\\nnion. Figure 5: Many of the attention heads exhibit behaviour that seems related to the structure of the sentence. We give two such examples above, from two different heads from the encoder self-attention at layer 5.assistant\\n\\nSpeaker 1: Welcome to today\\'s episode of \"Exploring the Future of Language Models\", I\\'m your host, [Name], and we\\'re going to be discussing a groundbreaking new approach to machine translation, proposed by Ashish Vaswani and his team at Google Brain. This innovative architecture, called the Transformer, eliminates the need for recurrent or convolutional neural networks, and instead relies solely on attention mechanisms. Let\\'s dive right in. Ashish, can you explain the problem with current sequence transduction models, and why the Transformer was developed?\\n\\nSpeaker 2: (excitedly) Oh, yeah, I mean, I was reading this paper, and it\\'s so cool, but I was like, what\\'s the problem with the current models? (pauses) Um, isn\\'t it that they\\'re just too slow? (laughs) I mean, like, they need a lot of training time and resources?\\n\\nSpeaker 1: That\\'s right, and another major issue is that these models rely on recurrent or convolutional neural networks, which are computationally expensive and difficult to parallelize. The Transformer, on the other hand, is designed to be much more efficient and scalable. Ashish, can you explain how the Transformer works?\\n\\nSpeaker 2: (jumping in) Yeah, so the Transformer is like, a big graph where each node represents a position in the input sequence, and the edges represent the attention between those positions. (pauses) And it\\'s like, super simple, but also super powerful. The Transformer uses self-attention, which means each node can attend to every other node in the graph, not just the previous or next node.\\n\\nSpeaker 1: That\\'s right, and this allows the model to capture long-range dependencies in the input sequence. But how does the Transformer handle this? Ashish, can you walk us through the architecture?\\n\\nSpeaker 2: (excitedly) Oh, yeah, so the Transformer has a multi-head self-attention mechanism, which is like, a bunch of smaller attention mechanisms that work together. (pauses) And it\\'s like, really important to note that the Transformer doesn\\'t use recurrence or convolution, which makes it way more parallelizable.\\n\\nSpeaker 1: That\\'s right, and this has significant implications for training time and computational efficiency. Ashish, can you talk about the results you\\'ve seen so far?\\n\\nSpeaker 2: (excitedly) Yeah, so the Transformer achieved 28.4 BLEU on the WMT 2014 English-to-German translation task, which is like, way better than the previous state-of-the-art model. And on the WMT 2014 English-to-French translation task, it achieved a new single-model state-of-the-art BLEU score of 41.8.\\n\\nSpeaker 1: Wow, that\\'s impressive. And Ashish, can you tell us about the importance of attention mechanisms in this model?\\n\\nSpeaker 2: (jumping in) Yeah, so attention mechanisms are like, super important because they allow the model to attend to different parts of the input sequence, and weigh their importance. (pauses) And it\\'s like, really interesting to see how the model uses attention to capture long-range dependencies.\\n\\nSpeaker 1: That\\'s right, and it\\'s also worth noting that the Transformer can be trained significantly faster than architectures based on recurrent or convolutional layers. Ashish, what are your plans for the future of this technology?\\n\\nSpeaker 2: (excitedly) Oh, yeah, so we\\'re planning to apply attention-based models to other tasks, like image captioning and question answering. (pauses) And we\\'re also going to explore extending the Transformer to problems involving input and output modalities other than text.\\n\\nSpeaker 1: Well, it\\'s clear that the Transformer is a game-changer in the field of natural language processing. Ashish, thanks for sharing your insights with us today.\\n\\nSpeaker 2: (excitedly) No problem, it\\'s been really cool to talk about this stuff. (pauses) I mean, I\\'m still trying to wrap my head around it, but it\\'s like, super interesting.\\n\\nSpeaker 1: (laughs) Well, we\\'ll have to have you back on the show again soon to discuss more about the Transformer. Thanks again, Ashish, and thanks to our listeners for tuning in.'},\n",
       "   {'role': 'assistant',\n",
       "    'content': '[\\n    (\"Speaker 1\", \"Welcome to our podcast, where we explore the latest advancements in AI and technology. Today, we\\'re joined by Ashish Vaswani, a renowned expert in the field of AI, to discuss a groundbreaking new approach to machine translation. This innovative architecture, called the Transformer, eliminates the need for recurrent or convolutional neural networks, and instead relies solely on attention mechanisms. Let\\'s dive right in. Ashish, can you explain the problem with current sequence transduction models, and why the Transformer was developed?\"),\\n    (\"Speaker 2\", \"I was reading this paper, and it\\'s so cool, but I was like, what\\'s the problem with the current models? Umm, isn\\'t it that they\\'re just too slow? (laughs) I mean, like, they need a lot of training time and resources?\"),\\n    (\"Speaker 1\", \"That\\'s right, and another major issue is that these models rely on recurrent or convolutional neural networks, which are computationally expensive and difficult to parallelize. The Transformer, on the other hand, is designed to be much more efficient and scalable. Ashish, can you explain how the Transformer works?\"),\\n    (\"Speaker 2\", \"Yeah, so the Transformer is like, a big graph where each node represents a position in the input sequence, and the edges represent the attention between those positions. Umm, it\\'s like, super simple, but also super powerful. The Transformer uses self-attention, which means each node can attend to every other node in the graph, not just the previous or next node.\"),\\n    (\"Speaker 1\", \"That\\'s right, and this allows the model to capture long-range dependencies in the input sequence. But how does the Transformer handle this? Ashish, can you walk us through the architecture?\"),\\n    (\"Speaker 2\", \"Oh, yeah, so the Transformer has a multi-head self-attention mechanism, which is like, a bunch of smaller attention mechanisms that work together. Umm, it\\'s like, really important to note that the Transformer doesn\\'t use recurrence or convolution, which makes it way more parallelizable.\"),\\n    (\"Speaker 1\", \"That\\'s right, and this has significant implications for training time and computational efficiency. Ashish, can you talk about the results you\\'ve seen so far?\"),\\n    (\"Speaker 2\", \"Yeah, so the Transformer achieved 28.4 BLEU on the WMT 2014 English-to-German translation task, which is like, way better than the previous state-of-the-art model. And on the WMT 2014 English-to-French translation task, it achieved a new single-model state-of-the-art BLEU score of 41.8. (laughs) That\\'s crazy!\"),\\n    (\"Speaker 1\", \"Wow, that\\'s impressive. And Ashish, can you tell us about the importance of attention mechanisms in this model?\"),\\n    (\"Speaker 2\", \"Yeah, so attention mechanisms are like, super important because they allow the model to attend to different parts of the input sequence, and weigh their importance. Umm, it\\'s like, really interesting to see how the model uses attention to capture long-range dependencies. I mean, I was reading about how the model uses attention to follow long-distance dependencies in the encoder self-attention in layer 5 of 6, and it\\'s like, whoa!\"),\\n    (\"Speaker 1\", \"That\\'s right, and it\\'s also worth noting that the Transformer can be trained significantly faster than architectures based on recurrent or convolutional layers. Ashish, what are your plans for the future of this technology?\"),\\n    (\"Speaker 2\", \"Oh, yeah, so we\\'re planning to apply attention-based models to other tasks, like image captioning and question answering. Umm, and we\\'re also going to explore extending the Transformer to problems involving input and output modalities other than text. I mean, I was thinking about how we could use the Transformer for sentiment analysis, and it\\'s like, super exciting!\"),\\n    (\"Speaker 1\", \"Well, it\\'s clear that the Transformer is a game-changer in the field of natural language processing. Ashish, thanks for sharing your insights with us today.\"),\\n    (\"Speaker 2\", \"No problem, it\\'s been really cool to talk about this stuff. (pauses) I mean, I\\'m still trying to wrap my head around it, but it\\'s like, super interesting. Umm, thanks for having me!\"),\\n    (\"Speaker 1\", \"Well, we\\'ll have to have you back on the show again soon to discuss more about the Transformer. Thanks again, Ashish, and thanks to our listeners for tuning in.\")'}]}]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "    (\"Speaker 1\", \"Welcome to our podcast, where we explore the latest advancements in AI and technology. Today, we're joined by Ashish Vaswani, a renowned expert in the field of AI, to discuss a groundbreaking new approach to machine translation. This innovative architecture, called the Transformer, eliminates the need for recurrent or convolutional neural networks, and instead relies solely on attention mechanisms. Let's dive right in. Ashish, can you explain the problem with current sequence transduction models, and why the Transformer was developed?\"),\n",
      "    (\"Speaker 2\", \"I was reading this paper, and it's so cool, but I was like, what's the problem with the current models? Umm, isn't it that they're just too slow? (laughs) I mean, like, they need a lot of training time and resources?\"),\n",
      "    (\"Speaker 1\", \"That's right, and another major issue is that these models rely on recurrent or convolutional neural networks, which are computationally expensive and difficult to parallelize. The Transformer, on the other hand, is designed to be much more efficient and scalable. Ashish, can you explain how the Transformer works?\"),\n",
      "    (\"Speaker 2\", \"Yeah, so the Transformer is like, a big graph where each node represents a position in the input sequence, and the edges represent the attention between those positions. Umm, it's like, super simple, but also super powerful. The Transformer uses self-attention, which means each node can attend to every other node in the graph, not just the previous or next node.\"),\n",
      "    (\"Speaker 1\", \"That's right, and this allows the model to capture long-range dependencies in the input sequence. But how does the Transformer handle this? Ashish, can you walk us through the architecture?\"),\n",
      "    (\"Speaker 2\", \"Oh, yeah, so the Transformer has a multi-head self-attention mechanism, which is like, a bunch of smaller attention mechanisms that work together. Umm, it's like, really important to note that the Transformer doesn't use recurrence or convolution, which makes it way more parallelizable.\"),\n",
      "    (\"Speaker 1\", \"That's right, and this has significant implications for training time and computational efficiency. Ashish, can you talk about the results you've seen so far?\"),\n",
      "    (\"Speaker 2\", \"Yeah, so the Transformer achieved 28.4 BLEU on the WMT 2014 English-to-German translation task, which is like, way better than the previous state-of-the-art model. And on the WMT 2014 English-to-French translation task, it achieved a new single-model state-of-the-art BLEU score of 41.8. (laughs) That's crazy!\"),\n",
      "    (\"Speaker 1\", \"Wow, that's impressive. And Ashish, can you tell us about the importance of attention mechanisms in this model?\"),\n",
      "    (\"Speaker 2\", \"Yeah, so attention mechanisms are like, super important because they allow the model to attend to different parts of the input sequence, and weigh their importance. Umm, it's like, really interesting to see how the model uses attention to capture long-range dependencies. I mean, I was reading about how the model uses attention to follow long-distance dependencies in the encoder self-attention in layer 5 of 6, and it's like, whoa!\"),\n",
      "    (\"Speaker 1\", \"That's right, and it's also worth noting that the Transformer can be trained significantly faster than architectures based on recurrent or convolutional layers. Ashish, what are your plans for the future of this technology?\"),\n",
      "    (\"Speaker 2\", \"Oh, yeah, so we're planning to apply attention-based models to other tasks, like image captioning and question answering. Umm, and we're also going to explore extending the Transformer to problems involving input and output modalities other than text. I mean, I was thinking about how we could use the Transformer for sentiment analysis, and it's like, super exciting!\"),\n",
      "    (\"Speaker 1\", \"Well, it's clear that the Transformer is a game-changer in the field of natural language processing. Ashish, thanks for sharing your insights with us today.\"),\n",
      "    (\"Speaker 2\", \"No problem, it's been really cool to talk about this stuff. (pauses) I mean, I'm still trying to wrap my head around it, but it's like, super interesting. Umm, thanks for having me!\"),\n",
      "    (\"Speaker 1\", \"Well, we'll have to have you back on the show again soon to discuss more about the Transformer. Thanks again, Ashish, and thanks to our listeners for tuning in.\")\n"
     ]
    }
   ],
   "source": [
    "save_string_pkl = outputs[0][\"generated_text\"][-1]['content']\n",
    "\n",
    "print(save_string_pkl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('podcast_ready_data.pkl', 'wb') as file:\n",
    "    pickle.dump(save_string_pkl, file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
