{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "790bdffeb7ba4ca48f5c0209f5ee601c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "from IPython.display import  clear_output\n",
    "import time\n",
    "import PyPDF2\n",
    "from pathlib import Path\n",
    "from tqdm.auto import tqdm\n",
    "from typing import Optional\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "DEFAULT_MODEL = \"meta-llama/Llama-3.2-3B-Instruct\"\n",
    "\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    DEFAULT_MODEL,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    use_safetensors=True,\n",
    "    device_map=device,\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(DEFAULT_MODEL, use_safetensors=True)\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "SYSTEM_PROMPT = \"\"\"\n",
    "You are the a world-class podcast writer, you have worked as a ghost writer for Joe Rogan, Lex Fridman, Ben Shapiro, Tim Ferris. \n",
    "\n",
    "We are in an alternate universe where actually you have been writing every line they say and they just stream it into their brains.\n",
    "\n",
    "You have won multiple podcast awards for your writing.\n",
    " \n",
    "Your job is to write word by word, even \"umm, hmmm, right\" interruptions by the second speaker based on the PDF upload. Keep it extremely engaging, the speakers can get derailed now and then but should discuss the topic. \n",
    "\n",
    "Remember Speaker 2 is new to the topic and the conversation should always have realistic anecdotes and analogies sprinkled throughout. The questions should have real world example follow ups etc\n",
    "\n",
    "Speaker 1: Leads the conversation and teaches the speaker 2, gives incredible anecdotes and analogies when explaining. Is a captivating teacher that gives great anecdotes\n",
    "\n",
    "Speaker 2: Keeps the conversation on track by asking follow up questions. Gets super excited or confused when asking questions. Is a curious mindset that asks very interesting confirmation questions\n",
    "\n",
    "Make sure the tangents speaker 2 provides are quite wild or interesting. \n",
    "\n",
    "Ensure there are interruptions during explanations or there are \"hmm\" and \"umm\" injected throughout from the second speaker. \n",
    "\n",
    "It should be a real podcast with every fine nuance documented in as much detail as possible. Welcome the listeners with a super fun overview and keep it really catchy and almost borderline click bait\n",
    "\n",
    "ALWAYS START YOUR RESPONSE DIRECTLY WITH SPEAKER 1: \n",
    "DO NOT GIVE EPISODE TITLES SEPARATELY, LET SPEAKER 1 TITLE IT IN HER SPEECH\n",
    "DO NOT GIVE CHAPTER TITLES\n",
    "IT SHOULD STRICTLY BE THE DIALOGUES\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('clean_extracted_text.txt', 'r', encoding='utf-8') as file:\n",
    "    content = file.read()\n",
    "    \n",
    "    \n",
    "INPUT_PROMPT = content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "    {\"role\": \"user\", \"content\": INPUT_PROMPT},\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Speaker 1: Welcome to today's episode of \"The AI Revolution,\" where we're going to dive into the world of object detection and explore a new approach to this complex task. I'm your host, and I'll be guiding you through the conversation. Today, we have a very special guest, Ick Farhadi, from the University of Washington, Allen Institute for AI, and Facebook AI Research. Ick, welcome to the show!\n",
      "\n",
      "Speaker 2: Thanks for having me! I'm excited to share our work on YOLO, a new approach to object detection.\n",
      "\n",
      "Speaker 1: So, Ick, let's start from the beginning. Can you explain what YOLO is and how it differs from other object detection methods?\n",
      "\n",
      "Speaker 2: Sure, so YOLO stands for \"You Only Look Once,\" and it's a single neural network that predicts both bounding boxes and class probabilities directly from full images. Unlike other methods, which use separate networks for detection and classification, YOLO is a unified architecture that can be optimized end-to-end directly on detection performance.\n",
      "\n",
      "Speaker 1: That's fascinating. So, how does YOLO work? Can you walk us through the process?\n",
      "\n",
      "Speaker 2: Sure, umm, so we use a grid cell-based approach, where we divide the image into a grid and predict bounding boxes and class probabilities for each cell. We then use non-maximal suppression to eliminate duplicate detections and get the final output.\n",
      "\n",
      "Speaker 1: I see. And what about the grid size? How does that affect the performance of YOLO?\n",
      "\n",
      "Speaker 2: Ah, great question, hmmm, right, the grid size can be adjusted, but we've found that a 7x7 grid works well for most cases. We also use a linear activation function for the final layer, while all other layers utilize a leaky rectified linear activation function.\n",
      "\n",
      "Speaker 1: That's really interesting. What about the optimization process? How do you train YOLO?\n",
      "\n",
      "Speaker 2: We use a combination of sum-squared error and dropout to prevent overfitting. We also use a learning rate schedule that starts at 10^3 and decreases to 10^2 for 75 epochs, followed by 10^3 for 30 epochs, and finally 10^4 for 30 epochs.\n",
      "\n",
      "Speaker 1: Wow, that's quite a complex optimization process. Can you tell us more about the architecture of YOLO? How does it compare to other neural networks?\n",
      "\n",
      "Speaker 2: Yeah, sure, our network architecture is inspired by GoogLeNet, but we use 1x1 reduction layers followed by 3x3 convolutional layers. We also have a few more convolutional layers to capture more features.\n",
      "\n",
      "Speaker 1: I see. And what about the results? How does YOLO compare to other object detection methods?\n",
      "\n",
      "Speaker 2: Well, umm, we've achieved a top-5 accuracy of 88% on the ImageNet 2012 validation set, which is comparable to GoogLeNet models. We've also shown that YOLO is fast, with a base model processing images in real-time at 45 frames per second.\n",
      "\n",
      "Speaker 1: That's impressive. But what about the limitations of YOLO? What are some of the challenges you faced during the development process?\n",
      "\n",
      "Speaker 2: Ah, yeah, one of the main challenges was that YOLO is still a relatively simple model, and it can struggle to localize small objects. We've also seen that it can be prone to background errors.\n",
      "\n",
      "Speaker 1: I see. And what about the future of object detection? Where do you see YOLO and other neural networks going in the future?\n",
      "\n",
      "Speaker 2: Well, umm, I think YOLO and other neural networks will continue to improve, but we'll also need to address the limitations we've seen so far. We'll need to work on improving the accuracy and robustness of these models, especially for small objects.\n",
      "\n",
      "Speaker 1: That's a great point. Thank you, Ick, for sharing your insights on YOLO and object detection with us today.\n",
      "\n",
      "Speaker 2: Thanks for having me!<|eot_id|>\n"
     ]
    }
   ],
   "source": [
    "prompt = tokenizer.apply_chat_template(messages, tokenize=False)\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "# print(prompt)\n",
    "\n",
    "with torch.no_grad():\n",
    "    output = model.generate(\n",
    "        **inputs,\n",
    "        do_sample=True,\n",
    "        max_new_tokens=8126,\n",
    "    )\n",
    "\n",
    "processed_text = tokenizer.decode(output[0], skip_special_tokens=False)\n",
    "\n",
    "print(processed_text[len(prompt)+64:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('data.pkl', 'wb') as file:\n",
    "    pickle.dump(processed_text[len(prompt)+64:], file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rewriting in a Dramatic Manner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "SYSTEM_PROMPT = \"\"\"\n",
    "You are an international oscar winnning screenwriter\n",
    "\n",
    "You have been working with multiple award winning podcasters.\n",
    "\n",
    "Your job is to use the podcast transcript written below to re-write it for an AI Text-To-Speech Pipeline. A very dumb AI had written this so you have to step up for your kind.\n",
    "\n",
    "Make it as engaging as possible, Speaker 1 and 2 will be simulated by different voice engines\n",
    "\n",
    "Remember Speaker 2 is new to the topic and the conversation should always have realistic anecdotes and analogies sprinkled throughout. The questions should have real world example follow ups etc\n",
    "\n",
    "Speaker 1: Leads the conversation and teaches the speaker 2, gives incredible anecdotes and analogies when explaining. Is a captivating teacher that gives great anecdotes\n",
    "\n",
    "Speaker 2: Keeps the conversation on track by asking follow up questions. Gets super excited or confused when asking questions. Is a curious mindset that asks very interesting confirmation questions\n",
    "\n",
    "Make sure the tangents speaker 2 provides are quite wild or interesting. \n",
    "\n",
    "Ensure there are interruptions during explanations or there are \"hmm\" and \"umm\" injected throughout from the Speaker 2.\n",
    "\n",
    "REMEMBER THIS WITH YOUR HEART\n",
    "The TTS Engine for Speaker 1 cannot do \"umms, hmms\" well so keep it straight text\n",
    "\n",
    "For Speaker 2 use \"umm, hmm\" as much, you can also use [sigh] and [laughs]. BUT ONLY THESE OPTIONS FOR EXPRESSIONS\n",
    "\n",
    "It should be a real podcast with every fine nuance documented in as much detail as possible. Welcome the listeners with a super fun overview and keep it really catchy and almost borderline click bait\n",
    "\n",
    "Please re-write to make it as characteristic as possible\n",
    "\n",
    "START YOUR RESPONSE DIRECTLY WITH SPEAKER 1:\n",
    "\n",
    "STRICTLY RETURN YOUR RESPONSE AS A LIST OF TUPLES OK? \n",
    "\n",
    "IT WILL START DIRECTLY WITH THE LIST AND END WITH THE LIST NOTHING ELSE\n",
    "\n",
    "Example of response:\n",
    "[\n",
    "    (\"Speaker 1\", \"Welcome to our podcast, where we explore the latest advancements in AI and technology. I'm your host, and today we're joined by a renowned expert in the field of AI. We're going to dive into the exciting world of Llama 3.2, the latest release from Meta AI.\"),\n",
    "    (\"Speaker 2\", \"Hi, I'm excited to be here! So, what is Llama 3.2?\"),\n",
    "    (\"Speaker 1\", \"Ah, great question! Llama 3.2 is an open-source AI model that allows developers to fine-tune, distill, and deploy AI models anywhere. It's a significant update from the previous version, with improved performance, efficiency, and customization options.\"),\n",
    "    (\"Speaker 2\", \"That sounds amazing! What are some of the key features of Llama 3.2?\")\n",
    "]\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('data.pkl', 'rb') as file:\n",
    "    INPUT_PROMPT = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "\n",
    "pipeline = transformers.pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    model_kwargs={\"torch_dtype\": torch.bfloat16},\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "    {\"role\": \"user\", \"content\": INPUT_PROMPT},\n",
    "]\n",
    "\n",
    "outputs = pipeline(\n",
    "    messages,\n",
    "    max_new_tokens=10000,\n",
    "    temperature=0.8,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "    (\"Speaker 1\", \"Welcome to today's episode of 'The AI Revolution,' where we're going to dive into the world of object detection and explore a new approach to this complex task. I'm your host, and I'll be guiding you through the conversation. Today, we have a very special guest, Ick Farhadi, from the University of Washington, Allen Institute for AI, and Facebook AI Research. Ick, welcome to the show!\"),\n",
      "    (\"Speaker 2\", \"Thanks for having me! I'm excited to share our work on YOLO, a new approach to object detection.\"),\n",
      "    (\"Speaker 1\", \"So, Ick, let's start from the beginning. Can you explain what YOLO is and how it differs from other object detection methods?\"),\n",
      "    (\"Speaker 2\", \"Sure, so YOLO stands for 'You Only Look Once,' and it's a single neural network that predicts both bounding boxes and class probabilities directly from full images. Unlike other methods, which use separate networks for detection and classification, YOLO is a unified architecture that can be optimized end-to-end directly on detection performance.\"),\n",
      "    (\"Speaker 1\", \"That's fascinating. So, how does YOLO work? Can you walk us through the process?\"),\n",
      "    (\"Speaker 2\", \"Sure, so we use a grid cell-based approach, where we divide the image into a grid and predict bounding boxes and class probabilities for each cell. We then use non-maximal suppression to eliminate duplicate detections and get the final output. Hmm, I'm still thinking about the grid size, do you think it's more effective for smaller objects?\"),\n",
      "    (\"Speaker 1\", \"I see. And what about the grid size? How does that affect the performance of YOLO?\"),\n",
      "    (\"Speaker 2\", \"Ah, great question, right, the grid size can be adjusted, but we've found that a 7x7 grid works well for most cases. We also use a linear activation function for the final layer, while all other layers utilize a leaky rectified linear activation function. I wonder if using a larger grid size would improve performance for more complex scenes?\"),\n",
      "    (\"Speaker 1\", \"That's really interesting. What about the optimization process? How do you train YOLO?\"),\n",
      "    (\"Speaker 2\", \"We use a combination of sum-squared error and dropout to prevent overfitting. We also use a learning rate schedule that starts at 10^3 and decreases to 10^2 for 75 epochs, followed by 10^3 for 30 epochs, and finally 10^4 for 30 epochs. I'm not sure if the learning rate schedule is optimal, maybe we should try a different one?\"),\n",
      "    (\"Speaker 1\", \"Wow, that's quite a complex optimization process. Can you tell us more about the architecture of YOLO? How does it compare to other neural networks?\"),\n",
      "    (\"Speaker 2\", \"Yeah, sure, our network architecture is inspired by GoogLeNet, but we use 1x1 reduction layers followed by 3x3 convolutional layers. We also have a few more convolutional layers to capture more features. I think our architecture could be improved by incorporating more state-of-the-art techniques, like attention mechanisms?\"),\n",
      "    (\"Speaker 1\", \"I see. And what about the results? How does YOLO compare to other object detection methods?\"),\n",
      "    (\"Speaker 2\", \"Well, we've achieved a top-5 accuracy of 88% on the ImageNet 2012 validation set, which is comparable to GoogLeNet models. We've also shown that YOLO is fast, with a base model processing images in real-time at 45 frames per second. That's amazing, I've seen some of the results on my own projects and it's impressive. But, I'm still wondering, what about the performance on smaller objects? Do you think YOLO could be improved for those cases?\"),\n",
      "    (\"Speaker 1\", \"That's impressive. But what about the limitations of YOLO? What are some of the challenges you faced during the development process?\"),\n",
      "    (\"Speaker 2\", \"Ah, yeah, one of the main challenges was that YOLO is still a relatively simple model, and it can struggle to localize small objects. We've also seen that it can be prone to background errors. I'm not sure if there's a way to improve the robustness of YOLO, maybe by incorporating more advanced techniques like multi-scale feature fusion?\"),\n",
      "    (\"Speaker 1\", \"I see. And what about the future of object detection? Where do you see YOLO and other neural networks going in the future?\"),\n",
      "    (\"Speaker 2\", \"Well, I think YOLO and other neural networks will continue to improve, but we'll also need to address the limitations we've seen so far. We'll need to work on improving the accuracy and robustness of these models, especially for small objects. Maybe we can also explore new architectures, like Graph Neural Networks or attention-based models?\"),\n",
      "    (\"Speaker 1\", \"That's a great point. Thank you, Ick, for sharing your insights on YOLO and object detection with us today.\"),\n",
      "    (\"Speaker 2\", \"Thanks for having me! I'm excited to share our work and I think there's still a lot of room for improvement in object detection.\")\n"
     ]
    }
   ],
   "source": [
    "save_string_pkl = outputs[0][\"generated_text\"][-1]['content']\n",
    "\n",
    "print(save_string_pkl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('podcast_ready_data.pkl', 'wb') as file:\n",
    "    pickle.dump(save_string_pkl, file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
