{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install -U sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f10a039768e4636a2e98daf63f7375f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "from IPython.display import  clear_output\n",
    "import time\n",
    "import PyPDF2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "DEFAULT_MODEL = \"meta-llama/Llama-3.2-3B-Instruct\"\n",
    "\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    DEFAULT_MODEL,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    use_safetensors=True,\n",
    "    device_map=device,\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(DEFAULT_MODEL, use_safetensors=True)\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "model.generation_config.pad_token_id = 128001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_transcription = '''0:00:00.000,0:00:05.000\n",
    "Have you wished you could turn long messy meeting recordings into concise structured summaries\n",
    "\n",
    "0:00:05.000,0:00:10.000\n",
    " automatically. Today, I'm going to show you exactly how to do that using AI. The best part,\n",
    "\n",
    "0:00:10.000,0:00:15.000\n",
    " it's fully automated. Whether your meeting is in English, Hindi, French or any other language,\n",
    "\n",
    "0:00:15.000,0:00:21.000\n",
    " this method can transcribe and summarize effortlessly. We'll be leveraging two powerful\n",
    "\n",
    "0:00:21.000,0:00:25.000\n",
    " models for this task, whisper and automatic speech recognition model that can\n",
    "\n",
    "0:00:25.000,0:00:31.000\n",
    " transcribe meetings in any language. And another is Lama, a large language modeled by Meta, which is\n",
    "\n",
    "0:00:31.000,0:00:37.000\n",
    " some, which will summarize the transcript into clear and actionable meetings minutes. If you're\n",
    "\n",
    "0:00:37.000,0:00:42.000\n",
    " new to whisper or Lama, no worries, I've already made detailed videos explaining both. So check them\n",
    "\n",
    "0:00:42.000,0:00:48.000\n",
    " out in my playlist or in the I button above and the description below. With that said let's jump right into the video and build the application.\n",
    "\n",
    "0:00:48.000,0:00:52.000\n",
    " Before we dive into the code I want to make sure you have everything set up correctly.\n",
    "\n",
    "0:00:53.000,0:00:58.000\n",
    " To follow along smoothly install the necessary packages and set up your environment first.\n",
    "\n",
    "0:00:58.000,0:01:03.000\n",
    " You can either use Python's virtual environment or Konda. Once you've installed all the required\n",
    "\n",
    "0:01:03.000,0:01:05.000\n",
    " libraries you're good to go.\n",
    "\n",
    "0:01:05.000,0:01:08.000\n",
    " All the required details are present in my repository,\n",
    "\n",
    "0:01:08.000,0:01:09.000\n",
    " which I'll link in the description below.\n",
    "\n",
    "0:01:09.000,0:01:12.000\n",
    " Now let's talk about the meeting summarizer\n",
    "\n",
    "0:01:12.000,0:01:14.000\n",
    " using the whisper model.\n",
    "\n",
    "0:01:14.000,0:01:16.000\n",
    " Now let's talk about the meeting summarizer\n",
    "\n",
    "0:01:16.000,0:01:17.000\n",
    " using the whisper model.\n",
    "\n",
    "0:01:17.000,0:01:18.000\n",
    " Before running the code,\n",
    "\n",
    "0:01:18.000,0:01:21.000\n",
    " make sure you have the FFMPEG installed.\n",
    "\n",
    "0:01:21.000,0:01:23.000\n",
    " If you haven't installed it, don't worry.\n",
    "\n",
    "0:01:23.000,0:01:25.000\n",
    " I provided a link here that will\n",
    "\n",
    "0:01:25.000,0:01:31.000\n",
    " work you through the installation process. It's a little tedious if you are using Windows, but for\n",
    "\n",
    "0:01:32.000,0:01:38.000\n",
    " Mac or Linux, it's pretty easy. It basically helps us to handle media files like audio, video.\n",
    "\n",
    "0:01:39.000,0:01:51.000\n",
    " Once that setup, we are ready to do. We'll be working with the Hugging Phase Transformer library. In particular, we'll use the Pipeline method with an audio model for causal language modeling.\n",
    "\n",
    "0:01:51.000,0:01:57.000\n",
    " Essentially a large language model will also use a tokenizer to process our text and specify\n",
    "\n",
    "0:01:57.000,0:01:58.000\n",
    " our device.\n",
    "\n",
    "0:01:58.000,0:02:01.000\n",
    " Here, we specify our device.\n",
    "\n",
    "0:02:01.000,0:02:04.000\n",
    " Now we'll come to converting speech to text with the whisper model.\n",
    "\n",
    "0:02:04.000,0:02:09.000\n",
    " Whether it input is audio or video, the first step is to transcribe. For this, we'll use OpenAI's\n",
    "\n",
    "0:02:09.000,0:02:16.000\n",
    " whisper model. The more if you have a video file, let's say MP4 or AVI, you can run this\n",
    "\n",
    "0:02:16.000,0:02:22.000\n",
    " particular lineup code to convert it into an audio file from a video file. This is the\n",
    "\n",
    "0:02:22.000,0:02:25.000\n",
    " function convert MP4 to MP3. Now let's break down the\n",
    "\n",
    "0:02:25.000,0:02:30.000\n",
    " key parameters of the wishbox. First we initialize the pipeline with automatic speech recognition\n",
    "\n",
    "0:02:30.000,0:02:38.000\n",
    " as task type. Then we select the model variant we want to use there are whisper large whisper\n",
    "\n",
    "0:02:38.000,0:02:45.000\n",
    " medium whisper small I'm using the small medium small version. Since audio files can be pretty large, we're going\n",
    "\n",
    "0:02:45.000,0:02:50.000\n",
    " to divide it into 30 second chunks and process them chunk by chunk. By setting return time\n",
    "\n",
    "0:02:50.000,0:02:57.000\n",
    " stamp equal to 2. We ensure that each transcribed segment includes a timestamp making it easy\n",
    "\n",
    "0:02:57.000,0:03:01.000\n",
    " to follow along. And this final part is kind of important, whereas we do the language selection.\n",
    "\n",
    "0:03:02.000,0:03:05.000\n",
    " We spoke either transcribe the original language or translate\n",
    "\n",
    "0:03:05.000,0:03:13.000\n",
    " it into English. Here I am choosing the translation and I am choosing the language of my source audio\n",
    "\n",
    "0:03:13.000,0:03:20.000\n",
    " as Hindi. But the source audio has both Hindi and English present in it. And the audio, the Usper\n",
    "\n",
    "0:03:20.000,0:03:25.000\n",
    " model is smart enough on itself to transcribe, to translate when there is Hindi\n",
    "\n",
    "0:03:25.000,0:03:30.000\n",
    " and just keep it keep it in English if the audio is in English. Here you can use French,\n",
    "\n",
    "0:03:30.000,0:03:36.000\n",
    " Spanish, or any other language among like 99 different languages you can use in this\n",
    "\n",
    "0:03:36.000,0:03:42.000\n",
    " case. Once the pipeline is initialized, all we have to do is pass in the path to our MP3\n",
    "\n",
    "0:03:42.000,0:03:45.000\n",
    " file and the model takes care of the risks. For example,\n",
    "\n",
    "0:03:45.000,0:03:49.000\n",
    " I choose this video to transcribe. I don't think China grew at the speed that it grew.\n",
    "\n",
    "0:03:49.000,0:03:56.000\n",
    " I think a lot of data is printed on the page. So I'm pretty sure they didn't grow at the rate that.\n",
    "\n",
    "0:03:56.000,0:04:02.000\n",
    " So you see this particular video is spoken both in Hindi as well as English. But the\n",
    "\n",
    "0:04:02.000,0:04:06.000\n",
    " whisper model is smart enough to transcribe Hindi to English as well\n",
    "\n",
    "0:04:06.000,0:04:11.000\n",
    " and just let it be English when they are speaking English and you see here is our transcription\n",
    "\n",
    "0:04:11.000,0:04:18.000\n",
    " see do you think India will grow at the speed China grow and so now as all our audio is in one\n",
    "\n",
    "0:04:18.000,0:04:26.000\n",
    " particular language it becomes very easy to create create a summarization of the meeting or minutes of the meeting,\n",
    "\n",
    "0:04:26.000,0:04:30.000\n",
    " whatever you like. And for that, we're going to use a large language model. For this, I'm\n",
    "\n",
    "0:04:30.000,0:04:36.000\n",
    " going to use the Lama 3.2, a three billion parameter model. If you have a lower end GPU,\n",
    "\n",
    "0:04:36.000,0:04:43.000\n",
    " you can opt for one billion parameter variant as well. And even you can make it more optimized\n",
    "\n",
    "0:04:43.000,0:04:47.000\n",
    " by using quantization techniques. And here how I created\n",
    "\n",
    "0:04:47.000,0:04:52.000\n",
    " the conversation setup. In the prompt, I ask write the minutes of this meeting transcript in simple\n",
    "\n",
    "0:04:52.000,0:04:59.000\n",
    " and precise English. And in the, this is my system prompt. And in the user query, I gave all the\n",
    "\n",
    "0:04:59.000,0:05:11.000\n",
    " transcription that I just created from the whisper model. Then I use my tokenizer to convert this dictionary of conversation into actual prompt with different\n",
    "\n",
    "0:05:11.000,0:05:12.000\n",
    " tokens.\n",
    "\n",
    "0:05:12.000,0:05:17.000\n",
    " If you are not sure how, what do I mean by that?\n",
    "\n",
    "0:05:17.000,0:05:22.000\n",
    " Please check out my previous video where I went deep into how the Lama model works and\n",
    "\n",
    "0:05:22.000,0:05:24.000\n",
    " different tokens of the Lama model.\n",
    "\n",
    "0:05:24.000,0:05:29.000\n",
    " After that I use the tokenizer to convert these words or texts into numbers that I can\n",
    "\n",
    "0:05:29.000,0:05:32.000\n",
    " easily fit into my LLM.\n",
    "\n",
    "0:05:32.000,0:05:36.000\n",
    " After that, I use the generate method to create the output.\n",
    "\n",
    "0:05:36.000,0:05:42.000\n",
    " I have set maximum tokens to 1000 to keep the summary a little bit concise.\n",
    "\n",
    "0:05:42.000,0:05:44.000\n",
    " Here do sample equal to true.\n",
    "\n",
    "0:05:44.000,0:05:46.000\n",
    " It introduces stochasticity or a\n",
    "\n",
    "0:05:46.000,0:05:50.000\n",
    " probabilistic nature, meaning each run might produce slightly different results by sampling\n",
    "\n",
    "0:05:50.000,0:05:58.000\n",
    " the most verbal rewards. Finally, I decode the output back into readable text, skipping\n",
    "\n",
    "0:05:58.000,0:06:04.000\n",
    " any special token such as like a startup sentence token or end of sentence token. These things\n",
    "\n",
    "0:06:04.000,0:06:05.000\n",
    " are not necessary.\n",
    "\n",
    "0:06:05.000,0:06:07.000\n",
    " And here you see the final output.\n",
    "\n",
    "0:06:07.000,0:06:09.000\n",
    " See, I put them in the markdown file\n",
    "\n",
    "0:06:09.000,0:06:12.000\n",
    " and you see they look pretty well.\n",
    "\n",
    "0:06:12.000,0:06:13.000\n",
    " The speaker discusses the economic growth of India\n",
    "\n",
    "0:06:13.000,0:06:17.000\n",
    " and China starting stating that India's growth\n",
    "\n",
    "0:06:17.000,0:06:23.000\n",
    "And it pretty well summarizes the actual meeting,\n",
    "has been remarkable.\n",
    "\n",
    "0:06:23.000,0:06:25.000\n",
    " even though it's in Hindi and English.\n",
    "\n",
    "0:06:25.000,0:06:30.000\n",
    " The same approach can be used to summarize any YouTube video as well and with this you\n",
    "\n",
    "0:06:30.000,0:06:33.000\n",
    " can automatically generate summaries in a few clicks.\n",
    "\n",
    "0:06:33.000,0:06:35.000\n",
    " And yeah, that's a wrap for today's video.\n",
    "\n",
    "0:06:35.000,0:06:39.000\n",
    " If you found this tutorial helpful, give it a thumbs up and subscribe to my channel for\n",
    "\n",
    "0:06:39.000,0:06:44.000\n",
    " more deep dives into large language models and computer vision projects.\n",
    "\n",
    "0:06:44.000,0:06:46.000\n",
    " Till then, stay curious and keep explaining.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3014])\n",
      "\n",
      "\n",
      "Here is the list of timestamps with corresponding topics:\n",
      "\n",
      "00:00:00.000 - Introduction to the Video\n",
      "00:00:05.000 - Introduction to the Video (continued)\n",
      "00:01:12.000 - Meeting Summarizer using Whisper Model\n",
      "00:04:30.000 - Meeting Summarizer using Lama Model\n",
      "00:05:42.000 - Summarization of Meeting using Large Language Model\n",
      "00:06:07.000 - Final Output of Summarization\n"
     ]
    }
   ],
   "source": [
    "\n",
    "conversation = [\n",
    "    {\"role\": \"system\", \"content\": f'''**Prompt:**  \n",
    "\n",
    "\"Given the subtitles of a YouTube video, generate a structured list of timestamps in the format MM:SS along with their corresponding topics. Identify key moments in the video where the subject matter changes or an important point is introduced. Ensure that each timestamp represents a meaningful transition or highlight. Format the output as follows:  \n",
    "\n",
    "MM:SS - Topic  \n",
    "\n",
    "Example:  \n",
    "00:30 - Introduction to the Video  \n",
    "02:15 - Explanation of Key Concept  \n",
    "05:45 - Demonstration of the Method  \n",
    "\n",
    "Ensure that the topics are concise and accurately reflect the content of each section. There should be at max 5 topics from start coverining the entire duration. It should strictly follow MM:SS format. DO NOT USE MARKDOWN FORMAT'''},\n",
    "    {\"role\": \"user\", \"content\": f'''{video_transcription}'''},\n",
    "]\n",
    "# \n",
    "prompt = tokenizer.apply_chat_template(conversation, tokenize=False)\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "print(inputs.input_ids.shape)\n",
    "\n",
    "with torch.no_grad():\n",
    "    output = model.generate(\n",
    "        **inputs,\n",
    "        do_sample=True,\n",
    "        max_new_tokens=256\n",
    "    )\n",
    "\n",
    "\n",
    "processed_text = tokenizer.decode(output[0][len(inputs.input_ids[0])+3:], skip_special_tokens=True)\n",
    "\n",
    "print(processed_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Automating Meeting Summarization with AI: A Step-by-Step Guide**\n",
    "\n",
    "Are you tired of listening to long, messy meeting recordings and wishing you could condense them into concise, structured summaries? With the help of AI, you can now do just that. In this tutorial, we'll show you how to use two powerful models, Whisper and LLaMA, to transcribe and summarize meetings in any language.\n",
    "\n",
    "**Getting Started**\n",
    "\n",
    "Before we dive into the code, make sure you have the necessary packages and environment set up. You can either use Python's virtual environment or Conda. Once you've installed all the required libraries, you're good to go. All the required details are present in our repository, which you can link in the description below.\n",
    "\n",
    "**Using Whisper for Automatic Speech Recognition**\n",
    "\n",
    "We'll start with the Whisper model, which can transcribe meetings in any language. Before running the code, make sure you have FFMPEG installed. If you haven't installed it, don't worry â€“ we'll walk you through the installation process. With Whisper, we'll use the Pipeline method with an audio model for causal language modeling. This large language model will also use a tokenizer to process our text and specify our device.\n",
    "\n",
    "To convert speech to text, we'll use OpenAI's Whisper model. If you have a video file, you can run the `convert_mp4_to_mp3` function to convert it into an audio file. For example, let's say you have a video file that's spoken in both Hindi and English. The Whisper model will transcribe it into English, ignoring the Hindi parts.\n",
    "\n",
    "**Using LLaMA for Summarization**\n",
    "\n",
    "For summarization, we'll use the LLaMA model, a 3.2 billion parameter model. You can opt for the 1 billion parameter variant if you have a lower-end GPU. To use LLaMA, we'll create a conversation setup with a prompt and a user query. The prompt will ask the model to write the minutes of the meeting transcript in simple and precise English, while the user query will provide the transcription from the Whisper model.\n",
    "\n",
    "Here's a sample conversation setup:\n",
    "\n",
    "```\n",
    "prompt: Write the minutes of this meeting transcript in simple and precise English.\n",
    "user query: The speaker discusses the economic growth of India and China, stating that India's growth has been remarkable.\n",
    "```\n",
    "\n",
    "We'll then use the tokenizer to convert the conversation into actual tokens and fit them into the LLaMA model. After that, we'll use the generate method to create the output, setting maximum tokens to 1000 to keep the summary concise. Finally, we'll decode the output back into readable text, skipping special tokens.\n",
    "\n",
    "**Putting it All Together**\n",
    "\n",
    "Here's the complete code:\n",
    "\n",
    "```python\n",
    "import whisper\n",
    "import torch\n",
    "from transformers import LLaMAForConditionalGeneration, LLaMATokenizer\n",
    "\n",
    "# Initialize Whisper model\n",
    "model = whisper.load(\"whisper-small\")\n",
    "tokenizer = whisper.load_translator(\"whisper-small\")\n",
    "\n",
    "# Initialize LLaMA model\n",
    "llama_model = LLaMAForConditionalGeneration.from_pretrained(\"llama-base\")\n",
    "llama_tokenizer = LLaMATokenizer.from_pretrained(\"llama-base\")\n",
    "\n",
    "# Convert speech to text using Whisper\n",
    "audio_file = \"path_to_audio_file.mp3\"\n",
    "transcription = model.transcribe(audio_file)\n",
    "\n",
    "# Summarize the transcription using LLaMA\n",
    "prompt = \"Write the minutes of this meeting transcript in simple and precise English.\"\n",
    "user_query = transcription\n",
    "output = llama_model.generate(user_query, max_length=1000)\n",
    "\n",
    "# Decode the output back into readable text\n",
    "output_text = llama_tokenizer.decode(output)\n",
    "print(output_text)\n",
    "```\n",
    "\n",
    "**Conclusion**\n",
    "\n",
    "With this tutorial, you can now automatically generate summaries of meetings in just a few clicks. The same approach can be used to summarize any YouTube video, and we hope you found this tutorial helpful. Give it a thumbs up and subscribe to our channel for more deep dives into large language models and computer vision projects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
