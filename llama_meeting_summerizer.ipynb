{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we need to install the Hugging Face Transformers library, which gives us access to state-of-the-art speech and language models. If you haven’t installed it yet, run:\n",
    "\n",
    "```\n",
    "pip install transformers torch\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let’s import the necessary libraries and set up our device for computation. If you have a GPU, the code will automatically use it for better performance!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline, AutoModelForSpeechSeq2Seq, AutoProcessor\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "from IPython.display import  clear_output\n",
    "import time\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SPEECH-TO-TEXT WITH WHISPER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we’ll use OpenAI’s Whisper model to transcribe an audio file. We’re using the whisper-small model for efficiency, but you can switch to whisper-large for better accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda\n"
     ]
    }
   ],
   "source": [
    "pipe  = pipeline(\"automatic-speech-recognition\",\n",
    "                    \"openai/whisper-small\", \n",
    "                    chunk_length_s=30,\n",
    "                    stride_length_s=5,\n",
    "                    return_timestamps=True,\n",
    "                    device=device, \n",
    "                    generate_kwargs = {\"language\": 'French', \"task\": \"translate\"}) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let’s process an audio file. The model will transcribe and translate it into English.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mohan.dash/miniconda3/envs/diffuser_data_generation/lib/python3.12/site-packages/transformers/models/whisper/generation_whisper.py:573: FutureWarning: The input name `inputs` is deprecated. Please make sure to use `input_features` instead.\n",
      "  warnings.warn(\n",
      "You have passed task=translate, but also have set `forced_decoder_ids` to [[1, None], [2, 50359]] which creates a conflict. `forced_decoder_ids` will be ignored in favor of task=translate.\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Whisper did not predict an ending timestamp, which can happen if audio is cut off in the middle of a word. Also make sure WhisperTimeStampLogitsProcessor was used during generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.0, 2.0)--> Yes, yes.\n",
      "(87.0, 90.0)--> Ok. How do you analyze with the instantanet data? Well, today, what we're going to do is that the Valiz-Pokéoki we will define for the first stage of use with installed cameras. And the second stage of use, it will be a control point where we just downloaded or posted images on a server. The analysis is not instant, it is a part of the organization that we do not use the same time. Because if we use the analysis to do the control of the four, we start by calling the control point. And there is only this environment with the cameras that are used. In fact, to use the same analysis for your applications, you have to stop the first take-off and put the images. But yes, the analysis is not going to be... Yes, the image analysis is instantaneous, we know. And then we can also do this programming, program the analysis of the light. So that, you are talking about stopping the first and start the second. It also depends on the level of familiarization with the valise. In my opinion, at the beginning, we can say that we can program the analysis once we receive the images. You can consult the images that you want. This is also the technique that we can suggest. Validation will be used in the morning for the capture control.\n",
      "(90.0, 94.0)--> If you never send images of the microscope, the Validation will not be received.\n",
      "(94.0, 97.0)--> Validation will be used at the moment when the first control is stopped.\n",
      "(97.0, 101.0)--> This will allow you to use the two control points,\n",
      "(101.0, 105.0)--> and above all, the second one will synchronization in the mastite time.\n",
      "(105.0, 147.0)--> The two? Yes. Ok, that's it, it's up to us to organize. And in any case, I will try to look at the time of the mokar, how it works. Well, in any case, we will do a training on both. We have products in the analysis of the planters that can execute and analyze in control points. So we can say that if the sensor is controlled from 8 to 10 o'clock, we can program all the micro-stores from 8 to 10 o'clock. As soon as we arrive at a mover, we have all the results. And it prevents me from telling you that if I need an emergency analysis, I stop the first one, I start the second one, I start again.\n",
      "(147.0, 289.88)--> It's really just the understanding of the situation. I think it's a good idea to have a good understanding of the situation. It's a good idea to have a good understanding of the situation. There are a few images to look at. Yes, yes. The time we have in the time, we will say, the time of the extension is 1.25 minutes per scope, 1.25 minutes per minute. In the instance, it's an instantane case, which we will say 1.25 the results of the next few minutes, and in the mean time, the use of the new mirror on the screen. Oh really? Yes. In that case, you will start to see how much you want to see the results. If it's a face. Do you want to keep the images in the mirror? Yes, I do. The quality of the doll. I think it's a good idea. I've already told you about it. The report, actually, has been... The report and the image number with its position. Yes, but in fact, the one I was looking at, the one you said was the one I was looking at, the one I was looking at, is the list of warnings, chaos and per image, or I give the table of chaos and warnings with the name of image. What do you want to have at the end? Do you want to have the position of the types of errors by diversity, or for each image, to see what we have as a result? Yes, what is interesting is that you hold the body and the body together. So if we have the position of the image, the same image for this one, at least the operator of the suite, you see, there are sometimes, I hope, 11, but we will say that we have sometimes, for example, for a man, he has a weapon the Tarpish, we would have a gun, but if we had a gun, it would be a bullet.\n",
      "(289.88, 290.88)--> Ok.\n",
      "(290.88, 292.88)--> You can imagine that.\n",
      "(292.88, 280.0)-->\n",
      "(289.0, 293.0)--> No, it's clear for us Can we ask questions?\n",
      "(293.0, 327.0)--> Can we open the cases for the warning? For the cases of Pui plus Pui plus Pui. It's not very clear to me. What are the reasons between the two?\n",
      "(327.0, 320.0)-->\n",
      "(324.0, 328.0)--> I didn't understand the question, sorry. You wanted to hear the difference between the two?\n",
      "(328.0, 329.0)--> Yes, yes, yes.\n",
      "(329.0, 335.0)--> He is a real guy, a donor, a donor, a donor.\n",
      "(335.0, 339.0)--> Yes, he is called Pui plus Pui.\n",
      "(339.0, 342.0)--> He explained it, he said it before.\n",
      "(342.0, 346.0)--> No, it's just a case. It's called Pui plus Pui.\n",
      "(346.0, 407.0)--> He explained it in the video. No, I think he learned it in the book. Here, on the book, we have this genealogy, which is exactly what it is, if it is for my own benefit, your image? Sure. Can I share my image? Yes, yes. I think it's ask for images with different contacts. Since they talked about doing that, for example, or doing a presentation with the minimum humidity can see them and test if they are impacted. One image is a contrast, the other one is a magnet.\n",
      "(407.0, 410.0)--> Is it because of the question we have, that you answered?\n",
      "(410.0, 467.0)--> No, it's just because you mentioned that potentially there is a vibration in the contrast. So it's to know if we would have two images and that they are both different. But just to see what this contrast is about the detector. Just to visualize it on our side side. Because if we are not sensitive to contrast, but it is also in the limit, we want to see what the two extreme cases are to make the two of us public. I would not have the images in a very new way. Ok can control the conditions of the luminosity.\n",
      "(467.0, 464.0)--> Yes, it's like the magnetism after, today it's a counteractivity of the No, but it's very good if you can control the conditions of luminosity, the reaction is better.\n",
      "(465.0, 505.0)--> Yes, it's like the magnetism after the magnetic field, it's a counter- a magnetism, after all, it's a sort of a scientific counter-attack on the inside, and I don't know if it's possible to respect the other conditions. Yes, yes. I would say it's a sort of a sort of a counter-attack on the inside, etc. All the... Yes, if you want to... Yes, I will imagine, it's a warning category, but I don't know exactly where the two photos are. It's a very good idea to have a photo.\n",
      "(505.0, 507.0)--> It's a good idea to have a photo.\n",
      "(507.0, 510.0)--> The photo is a little bit like a bazooka.\n",
      "(510.0, 806.0)--> It's not very complicated. I think it's a little bit different. It's not very complicated. I don't know. It's not very detailed. In fact, it remains a bit of a screen. It looks very good on the post-op screens, but when the screen is very small or different, it's less visible. Ok, I see. It's very light. Ok. It's here. Yes, there must be a rectangle, that's it. Yes, it's much more marked. The marked. The red light is on the left. And on the other side, it's... Well, my friends, we'll see each other soon. Ok, ok. And for this time, is this here, or is it the exact time that it right time? Is it a defect? I think the support is not perfect in this area. I think he will confirm that he can already have the test. we can see the frame. I have well understood that it was... I black exterior is a black leather jacket, where did it come from? And the piece of metal, a little carol that comes a bit... I think it's a bit... I think it's a bit... I think it's a bit... I think it's a bit... I think it's a bit... I think it's a bit... I? No, it's clear for me. I don't know if you have any other questions. No, it's great for us. Moane is doing tests on the images you sent. He will share a return to communicate this notion of separability and to make a return on each diversity. The percentage of the election of the Orlin, the one, the top one, ok, maybe. We're going to integrate that in a proposal. Will you have another point or will you see better? I don't think so, there are other questions that are finally coming up, but I think we have all the elements to advance. If I have any other questions, I can send you emails, and advance like that. Yes, go ahead. I wanted to tell you that I have more hands on the image capture, I was going to send you a third file, and I had that and maybe you can also contact the person who has not been in the band. He is in the band. Very well, thank you. Thank you. Thank you. Hello welcome You're welcome\n",
      "(806.0, 808.0)--> How are you?\n",
      "(808.0, 810.0)--> I'm fine\n",
      "(810.0, 812.0)--> I'm fine\n",
      "(812.0, 814.0)--> You can find the details\n",
      "(814.0, 816.0)--> about the points and the skills\n",
      "(816.0, 818.0)--> What are they?\n",
      "(818.0, 820.0)--> They're all in the description\n",
      "(822.0, 824.0)--> The common message\n",
      "(824.0, 873.64)--> You're welcome I'm sorry, I'm sorry. I'm sorry. I'm sorry. I'm sorry. I'm sorry. I'm sorry. I'm sorry. I'm sorry. I'm sorry. I'm sorry. I'm sorry. I'm sorry. I'm sorry. I'm sorry. I'm going to going to be a good one. I think it's going to be a good one. I think it's going to be a good one. I think it's going to be a good one. I think it's going to be a good one. I think it's going to be a good one. I think it's going to be a good one. I think it's going to be a good one. I think it's going to be a good one. I think it's going to be a good one. I think it's going to be a good one. I think it's going to be a good one. I think it's going to be a good one. I think it's going to be a good one. Sofia, did you understand exactly, I did not understand fully like here, what is\n",
      "(873.64, 860.0)-->\n",
      "(862.0, 864.0)--> Are you ready to go? Yes\n",
      "(864.0, 866.0)--> I'm ready\n",
      "(866.0, 868.0)--> I'm going to work\n",
      "(868.0, 870.0)--> I'm going to work\n",
      "(870.0, 872.0)--> I'm going to work\n",
      "(872.0, 874.0)--> I'm going to work\n",
      "(874.0, 945.0)--> I'm trying to do it Can I turn it in? Maybe you can turn it in? Yeah The answer is that it's not a rectangle Okay, here you have... Okay, it's curved Yeah, it's. Maybe you can show it, look in another image and you will see it. Ok, I see. I'm not sure the cases. One case I did on whole image and then according to the anomaly map I choose the area. And another case I use three zone auto and then choose specific specific area And which one works better?\n",
      "(947.0, 953.0)--> According to accuracy, the image based accuracy, whether the image is ok or not ok, the global image works better\n",
      "(953.0, 958.0)--> But according to anomaly map, the individual regions look better\n",
      "(958.0, 960.0)--> The heat map is...\n",
      "(960.0, 986.2)--> Because what happens is, when this... in some images this one is missing, that becomes logical and only. And patch code cannot detect that. Because it basically takes the minimum distance. So even if it's not here, the white pixel have minimum distance compared to the memory bank pixels. So it just doesn't show any so that's why like\n",
      "(986.2, 990.88)--> efficient eddy we have efficient eddy which is good at everything this problem is it takes time\n",
      "(990.88, 996.2)--> yeah but it takes time for training\n",
      "(996.2, 1000.2)--> inference is like way faster compared to patch code\n",
      "(1000.2, 1047.0)--> so maybe I try mine and I see. Yeah, you should. I think we have really better performances if we put on the translation. Okay. Because you know, this is also the one of, in the patch code also they say that this is translation invariant Because it takes pixel-wise distance so it will be translating also shouldn't be a problem because it does like region-based It doesn't do image like for an auto encoder it will be a problem if it translates ok, in this particular region it doesn't find anything and it gives.\n",
      "(1047.0, 1167.0)--> But for patch code where it does like each patches takes the difference. So even for the translation it's not a problem. So on my algorithm it will be difficult because it's just a difference. Yes. So that is the advantage of patch code that is translation invariant, but that also becomes a weakness that it cannot detect logical anomaly. Yeah, but that may be we should use both. Yeah, that is what like the efficient AD is the combination. So it uses both autoencoder and the fake patch for the base delivery. So, I don't know if you are on exam is this, we have to do this, we have to do this, we have to do this, we have to do this, we have to do this, we have to do this, we have to do this, we have to do this, we have to do this, we have to do this, we have to do this, we have to do this, we have to do this, we have to do this, we have to do make a pattern, it's not different. So did you try to efficient other ones? Yes. And it works better? Yes. Has it worked for this case? Actually I tried on global, you can find in the very traditional, there is to do this. So, we can see that for the logical memory it detects exactly.\n",
      "(1167.0, None)--> But it takes around I think 1 hour to train. because they want to just look at the outer part right for the here, because they only want to look at outer part, so I mask the inner part. And I found that if I do black, then it confuses other part of the window. So, I just did look at random noise, fill it with random noise and it works. and only in the center, in your space in the center, like a shocker, but have to write the first one, I have to write the second one, I have to write the Thank you.\n"
     ]
    }
   ],
   "source": [
    "transcription = pipe(\"meeting.flac\" )\n",
    "#Once the transcription is complete, we’ll format the text with timestamps for better readability.\n",
    "formatted_lyrics = \"\"\n",
    "for line in transcription['chunks']:\n",
    "    text = line[\"text\"]\n",
    "    ts = line[\"timestamp\"]\n",
    "    formatted_lyrics += f\"{ts}-->{text}\\n\"\n",
    "\n",
    "print(formatted_lyrics.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcription saved to transcription.txt\n"
     ]
    }
   ],
   "source": [
    "# Let’s also save the transcription to a text file so we can use it later!\"\n",
    "with open(\"transcription.txt\", \"w\", encoding=\"utf-8\") as file:\n",
    "    file.write(formatted_lyrics.strip())\n",
    "\n",
    "print(\"Transcription saved to transcription.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SUMMARIZING WITH LLAMA\n",
    "Now, let’s take it a step further! We’ll use Meta’s LLaMA model to summarize the transcript. For this, we need the Llama-3.2-3B-Instruct model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "690c7f11b6fb426b80aac6baf6ffb47f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "DEFAULT_MODEL = \"meta-llama/Llama-3.2-3B-Instruct\"\n",
    "\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    DEFAULT_MODEL,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    use_safetensors=True,\n",
    "    device_map=device,\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(DEFAULT_MODEL, use_safetensors=True)\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We’ll define a conversation prompt instructing LLaMA to summarize the meeting transcript in simple English."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "system\n",
      "\n",
      "Cutting Knowledge Date: December 2023\n",
      "Today Date: 13 Mar 2025\n",
      "\n",
      "Summarize the following office meeting transcript in simple and precise English.\n",
      "     DO NOT USE MARKDOWN FORMATuser\n",
      "\n",
      "(0.0, 2.0)--> Yes, yes.\n",
      "(87.0, 90.0)--> Ok. How do you analyze with the instantanet data? Well, today, what we're going to do is that the Valiz-Pokéoki we will define for the first stage of use with installed cameras. And the second stage of use, it will be a control point where we just downloaded or posted images on a server. The analysis is not instant, it is a part of the organization that we do not use the same time. Because if we use the analysis to do the control of the four, we start by calling the control point. And there is only this environment with the cameras that are used. In fact, to use the same analysis for your applications, you have to stop the first take-off and put the images. But yes, the analysis is not going to be... Yes, the image analysis is instantaneous, we know. And then we can also do this programming, program the analysis of the light. So that, you are talking about stopping the first and start the second. It also depends on the level of familiarization with the valise. In my opinion, at the beginning, we can say that we can program the analysis once we receive the images. You can consult the images that you want. This is also the technique that we can suggest. Validation will be used in the morning for the capture control.\n",
      "(90.0, 94.0)--> If you never send images of the microscope, the Validation will not be received.\n",
      "(94.0, 97.0)--> Validation will be used at the moment when the first control is stopped.\n",
      "(97.0, 101.0)--> This will allow you to use the two control points,\n",
      "(101.0, 105.0)--> and above all, the second one will synchronization in the mastite time.\n",
      "(105.0, 147.0)--> The two? Yes. Ok, that's it, it's up to us to organize. And in any case, I will try to look at the time of the mokar, how it works. Well, in any case, we will do a training on both. We have products in the analysis of the planters that can execute and analyze in control points. So we can say that if the sensor is controlled from 8 to 10 o'clock, we can program all the micro-stores from 8 to 10 o'clock. As soon as we arrive at a mover, we have all the results. And it prevents me from telling you that if I need an emergency analysis, I stop the first one, I start the second one, I start again.\n",
      "(147.0, 289.88)--> It's really just the understanding of the situation. I think it's a good idea to have a good understanding of the situation. It's a good idea to have a good understanding of the situation. There are a few images to look at. Yes, yes. The time we have in the time, we will say, the time of the extension is 1.25 minutes per scope, 1.25 minutes per minute. In the instance, it's an instantane case, which we will say 1.25 the results of the next few minutes, and in the mean time, the use of the new mirror on the screen. Oh really? Yes. In that case, you will start to see how much you want to see the results. If it's a face. Do you want to keep the images in the mirror? Yes, I do. The quality of the doll. I think it's a good idea. I've already told you about it. The report, actually, has been... The report and the image number with its position. Yes, but in fact, the one I was looking at, the one you said was the one I was looking at, the one I was looking at, is the list of warnings, chaos and per image, or I give the table of chaos and warnings with the name of image. What do you want to have at the end? Do you want to have the position of the types of errors by diversity, or for each image, to see what we have as a result? Yes, what is interesting is that you hold the body and the body together. So if we have the position of the image, the same image for this one, at least the operator of the suite, you see, there are sometimes, I hope, 11, but we will say that we have sometimes, for example, for a man, he has a weapon the Tarpish, we would have a gun, but if we had a gun, it would be a bullet.\n",
      "(289.88, 290.88)--> Ok.\n",
      "(290.88, 292.88)--> You can imagine that.\n",
      "(292.88, 280.0)-->\n",
      "(289.0, 293.0)--> No, it's clear for us Can we ask questions?\n",
      "(293.0, 327.0)--> Can we open the cases for the warning? For the cases of Pui plus Pui plus Pui. It's not very clear to me. What are the reasons between the two?\n",
      "(327.0, 320.0)-->\n",
      "(324.0, 328.0)--> I didn't understand the question, sorry. You wanted to hear the difference between the two?\n",
      "(328.0, 329.0)--> Yes, yes, yes.\n",
      "(329.0, 335.0)--> He is a real guy, a donor, a donor, a donor.\n",
      "(335.0, 339.0)--> Yes, he is called Pui plus Pui.\n",
      "(339.0, 342.0)--> He explained it, he said it before.\n",
      "(342.0, 346.0)--> No, it's just a case. It's called Pui plus Pui.\n",
      "(346.0, 407.0)--> He explained it in the video. No, I think he learned it in the book. Here, on the book, we have this genealogy, which is exactly what it is, if it is for my own benefit, your image? Sure. Can I share my image? Yes, yes. I think it's ask for images with different contacts. Since they talked about doing that, for example, or doing a presentation with the minimum humidity can see them and test if they are impacted. One image is a contrast, the other one is a magnet.\n",
      "(407.0, 410.0)--> Is it because of the question we have, that you answered?\n",
      "(410.0, 467.0)--> No, it's just because you mentioned that potentially there is a vibration in the contrast. So it's to know if we would have two images and that they are both different. But just to see what this contrast is about the detector. Just to visualize it on our side side. Because if we are not sensitive to contrast, but it is also in the limit, we want to see what the two extreme cases are to make the two of us public. I would not have the images in a very new way. Ok can control the conditions of the luminosity.\n",
      "(467.0, 464.0)--> Yes, it's like the magnetism after, today it's a counteractivity of the No, but it's very good if you can control the conditions of luminosity, the reaction is better.\n",
      "(465.0, 505.0)--> Yes, it's like the magnetism after the magnetic field, it's a counter- a magnetism, after all, it's a sort of a scientific counter-attack on the inside, and I don't know if it's possible to respect the other conditions. Yes, yes. I would say it's a sort of a sort of a counter-attack on the inside, etc. All the... Yes, if you want to... Yes, I will imagine, it's a warning category, but I don't know exactly where the two photos are. It's a very good idea to have a photo.\n",
      "(505.0, 507.0)--> It's a good idea to have a photo.\n",
      "(507.0, 510.0)--> The photo is a little bit like a bazooka.\n",
      "(510.0, 806.0)--> It's not very complicated. I think it's a little bit different. It's not very complicated. I don't know. It's not very detailed. In fact, it remains a bit of a screen. It looks very good on the post-op screens, but when the screen is very small or different, it's less visible. Ok, I see. It's very light. Ok. It's here. Yes, there must be a rectangle, that's it. Yes, it's much more marked. The marked. The red light is on the left. And on the other side, it's... Well, my friends, we'll see each other soon. Ok, ok. And for this time, is this here, or is it the exact time that it right time? Is it a defect? I think the support is not perfect in this area. I think he will confirm that he can already have the test. we can see the frame. I have well understood that it was... I black exterior is a black leather jacket, where did it come from? And the piece of metal, a little carol that comes a bit... I think it's a bit... I think it's a bit... I think it's a bit... I think it's a bit... I think it's a bit... I think it's a bit... I? No, it's clear for me. I don't know if you have any other questions. No, it's great for us. Moane is doing tests on the images you sent. He will share a return to communicate this notion of separability and to make a return on each diversity. The percentage of the election of the Orlin, the one, the top one, ok, maybe. We're going to integrate that in a proposal. Will you have another point or will you see better? I don't think so, there are other questions that are finally coming up, but I think we have all the elements to advance. If I have any other questions, I can send you emails, and advance like that. Yes, go ahead. I wanted to tell you that I have more hands on the image capture, I was going to send you a third file, and I had that and maybe you can also contact the person who has not been in the band. He is in the band. Very well, thank you. Thank you. Thank you. Hello welcome You're welcome\n",
      "(806.0, 808.0)--> How are you?\n",
      "(808.0, 810.0)--> I'm fine\n",
      "(810.0, 812.0)--> I'm fine\n",
      "(812.0, 814.0)--> You can find the details\n",
      "(814.0, 816.0)--> about the points and the skills\n",
      "(816.0, 818.0)--> What are they?\n",
      "(818.0, 820.0)--> They're all in the description\n",
      "(822.0, 824.0)--> The common message\n",
      "(824.0, 873.64)--> You're welcome I'm sorry, I'm sorry. I'm sorry. I'm sorry. I'm sorry. I'm sorry. I'm sorry. I'm sorry. I'm sorry. I'm sorry. I'm sorry. I'm sorry. I'm sorry. I'm sorry. I'm sorry. I'm going to going to be a good one. I think it's going to be a good one. I think it's going to be a good one. I think it's going to be a good one. I think it's going to be a good one. I think it's going to be a good one. I think it's going to be a good one. I think it's going to be a good one. I think it's going to be a good one. I think it's going to be a good one. I think it's going to be a good one. I think it's going to be a good one. I think it's going to be a good one. I think it's going to be a good one. Sofia, did you understand exactly, I did not understand fully like here, what is\n",
      "(873.64, 860.0)-->\n",
      "(862.0, 864.0)--> Are you ready to go? Yes\n",
      "(864.0, 866.0)--> I'm ready\n",
      "(866.0, 868.0)--> I'm going to work\n",
      "(868.0, 870.0)--> I'm going to work\n",
      "(870.0, 872.0)--> I'm going to work\n",
      "(872.0, 874.0)--> I'm going to work\n",
      "(874.0, 945.0)--> I'm trying to do it Can I turn it in? Maybe you can turn it in? Yeah The answer is that it's not a rectangle Okay, here you have... Okay, it's curved Yeah, it's. Maybe you can show it, look in another image and you will see it. Ok, I see. I'm not sure the cases. One case I did on whole image and then according to the anomaly map I choose the area. And another case I use three zone auto and then choose specific specific area And which one works better?\n",
      "(947.0, 953.0)--> According to accuracy, the image based accuracy, whether the image is ok or not ok, the global image works better\n",
      "(953.0, 958.0)--> But according to anomaly map, the individual regions look better\n",
      "(958.0, 960.0)--> The heat map is...\n",
      "(960.0, 986.2)--> Because what happens is, when this... in some images this one is missing, that becomes logical and only. And patch code cannot detect that. Because it basically takes the minimum distance. So even if it's not here, the white pixel have minimum distance compared to the memory bank pixels. So it just doesn't show any so that's why like\n",
      "(986.2, 990.88)--> efficient eddy we have efficient eddy which is good at everything this problem is it takes time\n",
      "(990.88, 996.2)--> yeah but it takes time for training\n",
      "(996.2, 1000.2)--> inference is like way faster compared to patch code\n",
      "(1000.2, 1047.0)--> so maybe I try mine and I see. Yeah, you should. I think we have really better performances if we put on the translation. Okay. Because you know, this is also the one of, in the patch code also they say that this is translation invariant Because it takes pixel-wise distance so it will be translating also shouldn't be a problem because it does like region-based It doesn't do image like for an auto encoder it will be a problem if it translates ok, in this particular region it doesn't find anything and it gives.\n",
      "(1047.0, 1167.0)--> But for patch code where it does like each patches takes the difference. So even for the translation it's not a problem. So on my algorithm it will be difficult because it's just a difference. Yes. So that is the advantage of patch code that is translation invariant, but that also becomes a weakness that it cannot detect logical anomaly. Yeah, but that may be we should use both. Yeah, that is what like the efficient AD is the combination. So it uses both autoencoder and the fake patch for the base delivery. So, I don't know if you are on exam is this, we have to do this, we have to do this, we have to do this, we have to do this, we have to do this, we have to do this, we have to do this, we have to do this, we have to do this, we have to do this, we have to do this, we have to do this, we have to do this, we have to do this, we have to do make a pattern, it's not different. So did you try to efficient other ones? Yes. And it works better? Yes. Has it worked for this case? Actually I tried on global, you can find in the very traditional, there is to do this. So, we can see that for the logical memory it detects exactly.\n",
      "(1167.0, None)--> But it takes around I think 1 hour to train. because they want to just look at the outer part right for the here, because they only want to look at outer part, so I mask the inner part. And I found that if I do black, then it confuses other part of the window. So, I just did look at random noise, fill it with random noise and it works. and only in the center, in your space in the center, like a shocker, but have to write the first one, I have to write the second one, I have to write the Thank you.assistant\n",
      "\n",
      "Two people were discussing the analysis of images captured by cameras. They were setting up a system for the first stage of use, which involves defining a Valiz-Pokéoki for the first stage of use with installed cameras. The second stage will be a control point where images are downloaded or posted on a server. The analysis is not instant, but rather a part of the organization that doesn't use the same time. \n",
      "\n",
      "The discussion focused on the accuracy of the system and how it can be improved. They talked about using an autoencoder and a patch code for image analysis. The autoencoder is good for detecting anomalies, but it's not translation invariant. The patch code, on the other hand, is translation invariant but can't detect logical anomalies. The speaker suggested using both for better results.\n",
      "\n",
      "The conversation also discussed the training time for the system. The autoencoder takes around 1 hour to train, but the patch code can work with just a few minutes of training. The speaker tried using the patch code and it worked well for the logical memory but had issues with the outer part of the image. \n",
      "\n",
      "The conversation concluded with the speaker thanking the other person for the discussion and agreeing to work together to improve the system.\n"
     ]
    }
   ],
   "source": [
    "conversation = [\n",
    "    {\"role\": \"system\", \"content\": ''' Summarize the following office meeting transcript in simple and precise English.\n",
    "     DO NOT USE MARKDOWN FORMAT'''},\n",
    "    {\"role\": \"user\", \"content\": f'''{formatted_lyrics.strip()}'''},\n",
    "]\n",
    "\n",
    "prompt = tokenizer.apply_chat_template(conversation, tokenize=False)\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "# print(prompt)\n",
    "\n",
    "with torch.no_grad():\n",
    "    output = model.generate(\n",
    "        **inputs,\n",
    "        do_sample=True,\n",
    "        max_new_tokens=1000\n",
    "    )\n",
    "\n",
    "processed_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "print(processed_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "diffuser_data_generation",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
