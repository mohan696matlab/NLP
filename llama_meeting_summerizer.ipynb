{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we need to install the Hugging Face Transformers library, which gives us access to state-of-the-art speech and language models. If you haven’t installed it yet, run:\n",
    "\n",
    "```\n",
    "pip install transformers torch\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let’s import the necessary libraries and set up our device for computation. If you have a GPU, the code will automatically use it for better performance!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transcribing mp3 file to numpy requires the FFmpeg. Follow the steps in this link to have it installed. [(How to install FFmpeg in windows)](https://www.wikihow.com/Install-FFmpeg-on-Windows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline,AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "from IPython.display import  clear_output\n",
    "\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SPEECH-TO-TEXT WITH WHISPER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# If MP4 (or other video format) convert to MP3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ffmpeg version 4.4.2-0ubuntu0.22.04.1 Copyright (c) 2000-2021 the FFmpeg developers\n",
      "  built with gcc 11 (Ubuntu 11.2.0-19ubuntu1)\n",
      "  configuration: --prefix=/usr --extra-version=0ubuntu0.22.04.1 --toolchain=hardened --libdir=/usr/lib/x86_64-linux-gnu --incdir=/usr/include/x86_64-linux-gnu --arch=amd64 --enable-gpl --disable-stripping --enable-gnutls --enable-ladspa --enable-libaom --enable-libass --enable-libbluray --enable-libbs2b --enable-libcaca --enable-libcdio --enable-libcodec2 --enable-libdav1d --enable-libflite --enable-libfontconfig --enable-libfreetype --enable-libfribidi --enable-libgme --enable-libgsm --enable-libjack --enable-libmp3lame --enable-libmysofa --enable-libopenjpeg --enable-libopenmpt --enable-libopus --enable-libpulse --enable-librabbitmq --enable-librubberband --enable-libshine --enable-libsnappy --enable-libsoxr --enable-libspeex --enable-libsrt --enable-libssh --enable-libtheora --enable-libtwolame --enable-libvidstab --enable-libvorbis --enable-libvpx --enable-libwebp --enable-libx265 --enable-libxml2 --enable-libxvid --enable-libzimg --enable-libzmq --enable-libzvbi --enable-lv2 --enable-omx --enable-openal --enable-opencl --enable-opengl --enable-sdl2 --enable-pocketsphinx --enable-librsvg --enable-libmfx --enable-libdc1394 --enable-libdrm --enable-libiec61883 --enable-chromaprint --enable-frei0r --enable-libx264 --enable-shared\n",
      "  libavutil      56. 70.100 / 56. 70.100\n",
      "  libavcodec     58.134.100 / 58.134.100\n",
      "  libavformat    58. 76.100 / 58. 76.100\n",
      "  libavdevice    58. 13.100 / 58. 13.100\n",
      "  libavfilter     7.110.100 /  7.110.100\n",
      "  libswscale      5.  9.100 /  5.  9.100\n",
      "  libswresample   3.  9.100 /  3.  9.100\n",
      "  libpostproc    55.  9.100 / 55.  9.100\n",
      "Input #0, mov,mp4,m4a,3gp,3g2,mj2, from '2025-03-21 10-15-55.mp4':\n",
      "  Metadata:\n",
      "    major_brand     : isom\n",
      "    minor_version   : 512\n",
      "    compatible_brands: isomiso2avc1mp41\n",
      "    encoder         : Lavf58.76.100\n",
      "  Duration: 00:11:18.70, start: 0.000000, bitrate: 426 kb/s\n",
      "  Stream #0:0(und): Video: h264 (High) (avc1 / 0x31637661), yuv420p(tv, bt709), 1920x1080 [SAR 1:1 DAR 16:9], 237 kb/s, 60 fps, 60 tbr, 15360 tbn, 120 tbc (default)\n",
      "    Metadata:\n",
      "      handler_name    : VideoHandler\n",
      "      vendor_id       : [0][0][0][0]\n",
      "  Stream #0:1(und): Audio: aac (LC) (mp4a / 0x6134706D), 48000 Hz, mono, fltp, 175 kb/s (default)\n",
      "    Metadata:\n",
      "      handler_name    : SoundHandler\n",
      "      vendor_id       : [0][0][0][0]\n",
      "Stream mapping:\n",
      "  Stream #0:1 -> #0:0 (aac (native) -> mp3 (libmp3lame))\n",
      "Press [q] to stop, [?] for help\n",
      "Output #0, mp3, to 'meeting.mp3':\n",
      "  Metadata:\n",
      "    major_brand     : isom\n",
      "    minor_version   : 512\n",
      "    compatible_brands: isomiso2avc1mp41\n",
      "    TSSE            : Lavf58.76.100\n",
      "  Stream #0:0(und): Audio: mp3, 48000 Hz, mono, fltp, 192 kb/s (default)\n",
      "    Metadata:\n",
      "      handler_name    : SoundHandler\n",
      "      vendor_id       : [0][0][0][0]\n",
      "      encoder         : Lavc58.134.100 libmp3lame\n",
      "size=   14592kB time=00:10:32.92 bitrate= 188.9kbits/s speed= 141x    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conversion successful: meeting.mp3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "size=   15908kB time=00:11:18.67 bitrate= 192.0kbits/s speed= 141x    \n",
      "video:0kB audio:15908kB subtitle:0kB other streams:0kB global headers:0kB muxing overhead: 0.004445%\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "\n",
    "def convert_mp4_to_mp3(input_file, output_file):\n",
    "    \"\"\"Convert an MP4 file to MP3 using ffmpeg.\"\"\"\n",
    "    try:\n",
    "        command = [\n",
    "            \"ffmpeg\",\n",
    "            \"-i\", input_file,    # Input file\n",
    "            \"-vn\",               # No video\n",
    "            \"-acodec\", \"libmp3lame\",  # MP3 codec\n",
    "            \"-b:a\", \"192k\",      # Audio bitrate\n",
    "            output_file          # Output file\n",
    "        ]\n",
    "        subprocess.run(command, check=True)\n",
    "        print(f\"Conversion successful: {output_file}\")\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"Error during conversion: {e}\")\n",
    "\n",
    "# Example usage\n",
    "convert_mp4_to_mp3(\"2025-03-21 10-15-55.mp4\", \"meeting.mp3\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we’ll use OpenAI’s Whisper model to transcribe an audio file. We’re using the whisper-small model for efficiency, but you can switch to whisper-large for better accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda\n"
     ]
    }
   ],
   "source": [
    "pipe  = pipeline(\"automatic-speech-recognition\",\n",
    "                    \"openai/whisper-small\", \n",
    "                    chunk_length_s=30,\n",
    "                    stride_length_s=5,\n",
    "                    return_timestamps=True,\n",
    "                    device=device, \n",
    "                    generate_kwargs = {\"language\": 'english', \"task\": \"transcribe\"}) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let’s process an audio file. The model will transcribe and translate it into English.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mohan.dash/miniconda3/envs/diffuser_data_generation/lib/python3.12/site-packages/transformers/models/whisper/generation_whisper.py:573: FutureWarning: The input name `inputs` is deprecated. Please make sure to use `input_features` instead.\n",
      "  warnings.warn(\n",
      "You have passed task=transcribe, but also have set `forced_decoder_ids` to [[1, None], [2, 50359]] which creates a conflict. `forced_decoder_ids` will be ignored in favor of task=transcribe.\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.0, 107.0)--> Thank you. What if AI could help you find life's biggest question straight from Bhagavad Gita? In this video, I'll show you how to build an AI system that retrieves verses from the Gita and answers any live questions you ask. For example, here is a sample question, why does God allow suffering and evil in the world? This is a generated answer by AI in Bhagavad Gita, Lord Krishna explains, allows suffering and evil, but it's the natural order of the universe, sound and so forth. You can also get it in the... In the Bhagavad Gita, Lord Krishna explains that God does not allow suffering and evil.\n",
      "(107.0, 159.0)--> You can get that in the format of speaking as well. So, in this video, you will learn how to combine retrieval of Maintain Generation or Ragn shot with the timeless Ujjdam of Gita. So you can create an AI powered spiritual guide for you.\n",
      "(246.24, 408.64)--> So can AI really provide meaningful answers? Stay till the end to see it in action. Here is a sample question that I asked the Gita Rack, that why does God allow suffering and evil in the world? This is the answer. In the Bhagavad Gita, Lord Krishna explains that God does not allow suffering and evil, but it is the natural order of the universe. He says that the universe is governed by the law of karma, and every action has consequences. The suffering and evil in the world are a result of human actions, not God's will. God allows them to happen so that humans can learn and grow and to maintain the balance of the universe. It is like a teacher allowing a student to experience their mistakes so they can learn and become wiser. You see, this is from the rag that I developed. You can ask many different types of questions like, is there an afterlife? What happens when we die? I often feel lost in life. How do I find my true purpose? What is the role of prayer and does it really work? Many such questions. This AI app will look in the Gita and find the relevant verses and try to explain them to you in the best of its capacity. Also, it will cite it. So, why are we going for this app? Why we just not ask chat is pretty just like that? That just use the context of Kita to answer this following question. The main problem is hallucination with the large language mode. So what happens is when the AI generates you some like this for your particular question, you are never sure that is this correct or is just making things up because it can make things up very confidently as well. So what we would like to do is we would like to give it more context. Okay, from this particular context of Geeta, you find the relevant answer try try to explain it to, so that we are sure that it is not generating something out of its own mind, which is not true. So, how we can avoid this solution? Here comes the retrieval of vintage generation. What we can do is we ask the question and along with the context that these are some verses directly taken from Gita. Of course, they are translated in English because I am using a version of Gita which is already translated in English. So, the death end. So, for this particular question I got some relevant verses that could have this answer in them. So I got like here three, but usually I take 10 such chunk of text. So death and reincarnation of the soul just as a person dot dot dot like it. There is a long text. So I just show the initial part. The question is, is there an afterlife? What happens when it dies? So the relevant questions, the relevant part of Gita should also contain text about afterlife or death. Also you grieve for those who are not worried or grieved just as the living being acquires childhood body. There are some things that has been told in Gita about death and real life. Using this context, now the AI generated this answer. So in this answer, you see that it also quoted just as a living being acquires a childhood body, which is this particular text which is written in Gita. So you know it's not hallucinating, it's quoting something from the Gita. It told that when we die our soul doesn't really die, it just changes for it.\n",
      "(408.64, 413.6)--> It's like wearing a new cloth. We acquire new what is after discarding the old ones.\n",
      "(413.6, 417.92)--> This is what Bhagavad Gita says. It's exactly what it said.\n",
      "(417.92, 421.2)--> But sometimes what I feel is this 2.1 on there not very relevant.\n",
      "(421.2, 667.84)--> They're not very accurate at some time but sometime they are pretty good. So now the question comes how do you get the context? Usually this context comes from the Gita itself and our goal is to retrieve chunks which are relevant to the query. This is our query and we want to retrieve some chunks from Gita that are relevant to this query and give it to the model. Now the question comes how to chunk the text that we have. So we usually have a long, long corpus of text for Gita, especially there are 18 chapters and yeah, it's pretty long. So what we do is we randomly choose, choose a large portion of text and we would call it chunk one, then next one chunk two, chunk three and so on. We just split them into some chunks based on the size of the text that if let us say we want 100 words or 200 words that should be my chunk size. And after my chunks are created, I'm going to create a vector database out of that. For that I use a encoder model, a by encoder model, what it does is it you pass in that text that you your chunk in the text format and it gives you a vector of size 488. 384 and as we have total 431 chunk our vector database is consist of 431 rows each represent to each chunk and one chunk is of size 384. Now comes how do we retrieve from this vector database? It's pretty easy, but let's take an example that we have two chunk Python is a great language for data science and the weather today is sunny and warm. We pass it to the encoder model and we get two vectors. And this is our query, LLAP programming in Python. and if we pass it to the encoder model and we get two vectors and this is our query. I love programming in Python and if we pass it to the encoder model, we also get a vector. So we want to see which of these two is similar to this particular query. So for that, what we can do is because it's just vectors, we can plot them on a two dimensional graph graph I am using U map for dimensionality detection. So you see I love programming in Python Python is a great language for data sets, but the other part the weather today sunny and warm This is far from the query text the this is my query text I love programming and Python and This one is pretty far the closer one is Python is the great language for data. So I'll retrieve something related to Python. So putting it all together, we have a query. Is there after life after what happens? We don't pass just as it is to the LLM instead what we do, we pass it to the encoder model by encoder model we get the encoding of this query and choose the chunks those are closest to this particular query from our vector database so we got this particular like we got three chunks in death and re-incarnation of soul the nature of eternal being we are proud of our past thoughts and action and something. And we got a score of similarity that how similar they are to this particular text and in a decreasing order of similarity. And after that, we add that to our query called context. So now this becomes our modified query. And we pass this to the latch language model\n",
      "(667.84, 678.64)--> now so that it knows that what is the context from this context it will answer us.\n"
     ]
    }
   ],
   "source": [
    "transcription = pipe(\"meeting.mp3\" )\n",
    "#Once the transcription is complete, we’ll format the text with timestamps for better readability.\n",
    "formatted_lyrics = \"\"\n",
    "for line in transcription['chunks']:\n",
    "    text = line[\"text\"]\n",
    "    ts = line[\"timestamp\"]\n",
    "    formatted_lyrics += f\"{ts}-->{text}\\n\"\n",
    "\n",
    "print(formatted_lyrics.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcription saved to transcription.txt\n"
     ]
    }
   ],
   "source": [
    "# Let’s also save the transcription to a text file so we can use it later!\"\n",
    "with open(\"transcription.txt\", \"w\", encoding=\"utf-8\") as file:\n",
    "    file.write(formatted_lyrics.strip())\n",
    "\n",
    "print(\"Transcription saved to transcription.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SUMMARIZING WITH LLAMA\n",
    "Now, let’s take it a step further! We’ll use Meta’s LLaMA model to summarize the transcript. For this, we need the Llama-3.2-3B-Instruct model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88cbd44c09154ddda3f5b8e502a6f2c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "DEFAULT_MODEL = \"meta-llama/Llama-3.2-3B-Instruct\"\n",
    "\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    DEFAULT_MODEL,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    use_safetensors=True,\n",
    "    device_map=device,\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(DEFAULT_MODEL, use_safetensors=True)\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We’ll define a conversation prompt instructing LLaMA to summarize the meeting transcript in simple English."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "assistant\n",
      "\n",
      "Based on the provided script, I've created a skeleton structure for your YouTube video on \"How to build an AI system that retrieves verses from the Bhagavad Gita and answers live questions.\" Here's the skeleton structure with each point, what the viewer expects from each point, and how to present the information in a flow:\n",
      "\n",
      "**Introduction (0.0, 107.0)**\n",
      "\n",
      "* What to include: Introduce the concept of the video, explain the goal of building an AI system that retrieves verses from the Bhagavad Gita and answers live questions.\n",
      "* What the viewer expects: An overview of the topic, understanding of the purpose of the video.\n",
      "* How to present: Start with a hook to grab the viewer's attention, provide a brief introduction to the topic, and explain the goal of the video.\n",
      "\n",
      "Example:\n",
      "\"Welcome to our video on how to build an AI system that retrieves verses from the Bhagavad Gita and answers live questions. In this video, we'll show you how to combine retrieval of text with the timeless wisdom of the Gita. You can create an AI-powered spiritual guide for yourself. Stay tuned to the end to see it in action.\"\n",
      "\n",
      "**Section 1: Can AI really provide meaningful answers? (107.0, 159.0)**\n",
      "\n",
      "* What to include: Discuss the limitations of large language models (LLMs) and the need for context to provide accurate answers.\n",
      "* What the viewer expects: Understanding of the limitations of LLMs and the importance of context.\n",
      "* How to present: Use a conversational tone to explain the limitations of LLMs, and provide examples of how context can improve the accuracy of answers.\n",
      "\n",
      "Example:\n",
      "\"Can AI really provide meaningful answers? Well, the answer is yes, but with some limitations. LLMs can generate answers, but they can also hallucinate or provide irrelevant information. This is where context comes in. We need to provide the right context for the AI to understand the question and provide an accurate answer.\"\n",
      "\n",
      "**Section 2: How to retrieve context (159.0, 246.24)**\n",
      "\n",
      "* What to include: Explain the process of chunking text, creating a vector database, and retrieving relevant chunks.\n",
      "* What the viewer expects: Understanding of the process of retrieving context and how it improves the accuracy of answers.\n",
      "* How to present: Use analogies and examples to explain the process of chunking text, creating a vector database, and retrieving relevant chunks.\n",
      "\n",
      "Example:\n",
      "\"So, how do we retrieve context? We use a process called chunking, where we split the text into smaller chunks, like 100 words or 200 words. We then create a vector database using an encoder model, which converts each chunk into a vector. This vector database is used to retrieve relevant chunks for the query.\"\n",
      "\n",
      "**Section 3: Retrieving chunks and answering questions (246.24, 408.64)**\n",
      "\n",
      "* What to include: Explain how to retrieve relevant chunks from the vector database and use them to answer questions.\n",
      "* What the viewer expects: Understanding of how to retrieve relevant chunks and use them to answer questions.\n",
      "* How to present: Use examples to demonstrate how to retrieve relevant chunks and use them to answer questions.\n",
      "\n",
      "Example:\n",
      "\"So, how do we retrieve relevant chunks? We use the vector database to compare the query vector with the chunk vectors. The chunk with the highest similarity score is selected as the relevant chunk. We then use this chunk to answer the question. For example, if the query is 'Is there an afterlife?' and the relevant chunk is 'There is a long text about death and reincarnation of the soul...' we can use this chunk to answer the question.\"\n",
      "\n",
      "**Section 4: Example demonstration (408.64, 413.6)**\n",
      "\n",
      "* What to include: Demonstrate the process of retrieving chunks and answering questions using a sample query.\n",
      "* What the viewer expects: A clear understanding of the process and how it works.\n",
      "* How to present: Use a conversational tone to demonstrate the process and provide examples.\n",
      "\n",
      "Example:\n",
      "\"Let's try an example. We'll ask the AI 'Is there an afterlife?' and see how it answers. We pass the query to the encoder model, which retrieves the relevant chunk from the vector database. The relevant chunk is 'There is a long text about death and reincarnation of the soul...' and the AI answers 'Yes, there is an afterlife.'\"\n",
      "\n",
      "**Section 5: Conclusion (413.6, 421.2)**\n",
      "\n",
      "* What to include: Summarize the key points and emphasize the benefits of using an AI system that retrieves verses from the Bhagavad Gita.\n",
      "* What the viewer expects: A summary of the key points and an understanding of the benefits.\n",
      "* How to present: Use a conversational tone to summarize the key points and emphasize the benefits.\n",
      "\n",
      "Example:\n",
      "\"In conclusion, we've\n"
     ]
    }
   ],
   "source": [
    "conversation = [\n",
    "    {\"role\": \"system\", \"content\": '''You are a master in YouTube Script Writing and Information Delivering without making a viewer feel bored. I am working on a YouTube Script for a Video [Title]. I need a complete skeleton structure for it with all the points included, don’t miss any. In the skeleton structure, each point should include What should be Included in this point, What does the Viewer expect from this point (not in terms of feelings, in terms of information included and presentation) and How should this information be presented in a flow. Don’t forget to include examples of each point that give me an idea on how to write the script myself. I’m writing this script in a human conversational tone so keep that in mind while writing your examples. If there is any need of providing any reference, study results, mechanism, science backed techniques, facts or anything for any point in any part of the script to make it more informative, mention that in that particular point not at the end. Now using all your expertise write me a skeleton structure with every point included and some examples for each of them.'''},\n",
    "    {\"role\": \"user\", \"content\": f'''{formatted_lyrics.strip()}'''},\n",
    "]\n",
    "\n",
    "prompt = tokenizer.apply_chat_template(conversation, tokenize=False)\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "# print(prompt)\n",
    "\n",
    "with torch.no_grad():\n",
    "    output = model.generate(\n",
    "        **inputs,\n",
    "        do_sample=True,\n",
    "        max_new_tokens=1000\n",
    "    )\n",
    "\n",
    "processed_text = tokenizer.decode(output[0][len(inputs.input_ids[0]):], skip_special_tokens=True)\n",
    "\n",
    "print(processed_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The provided text appears to be a transcript of a meeting between multiple individuals, including Maximilian Moro, Carlos, and others. The meeting seems to be discussing various topics, including:\n",
    "\n",
    "1. **Control and inspection of components**: The group discusses the importance of controlling and inspecting components, particularly in the context of a map or a system. They talk about the need for a camera to capture images of the components and the importance of aligning them.\n",
    "2. **Camera setup and testing**: The group discusses the setup and testing of cameras, including the use of different types of cameras and the importance of testing for parallelism and alignment.\n",
    "3. **Image processing and analysis**: The group talks about the need for image processing and analysis, including the use of software to detect defects and anomalies in the images.\n",
    "4. **Feasibility studies**: The group discusses the importance of feasibility studies, including the creation of a set of images for each control and the testing of these images to determine the conditions of success.\n",
    "5. **Camera placement and ergonomics**: The group talks about the importance of camera placement and ergonomics, including the need to position cameras to capture the best images and to reduce the risk of errors.\n",
    "6. **Camera calibration and maintenance**: The group discusses the importance of camera calibration and maintenance, including the need to change batteries and perform regular checks on the cameras.\n",
    "7. **Image quality and resolution**: The group talks about the importance of image quality and resolution, including the need to capture high-quality images to ensure accurate inspection and analysis.\n",
    "8. **Labeling and annotation**: The group discusses the importance of labeling and annotating images, including the need to clearly label and identify components and defects.\n",
    "\n",
    "Some specific points mentioned in the transcript include:\n",
    "\n",
    "* The need to use a camera with a good resolution to capture high-quality images\n",
    "* The importance of aligning images to ensure accurate inspection and analysis\n",
    "* The need to test cameras for parallelism and alignment\n",
    "* The importance of feasibility studies to determine the conditions of success for each control\n",
    "* The need to position cameras to capture the best images and reduce the risk of errors\n",
    "* The importance of regular camera calibration and maintenance\n",
    "* The need to clearly label and annotate images to ensure accurate inspection and analysis\n",
    "\n",
    "Overall, the meeting appears to be focused on improving the inspection and analysis process using cameras and image processing techniques, and ensuring that the cameras are properly set up and maintained to capture high-quality images."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
