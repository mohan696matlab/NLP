{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we need to install the Hugging Face Transformers library, which gives us access to state-of-the-art speech and language models. If you haven’t installed it yet, run:\n",
    "\n",
    "```\n",
    "pip install transformers torch\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let’s import the necessary libraries and set up our device for computation. If you have a GPU, the code will automatically use it for better performance!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transcribing mp3 file to numpy requires the FFmpeg. Follow the steps in this link to have it installed. [(How to install FFmpeg in windows)](https://www.wikihow.com/Install-FFmpeg-on-Windows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline,AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "from IPython.display import  clear_output\n",
    "\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SPEECH-TO-TEXT WITH WHISPER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we’ll use OpenAI’s Whisper model to transcribe an audio file. We’re using the whisper-small model for efficiency, but you can switch to whisper-large for better accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda\n"
     ]
    }
   ],
   "source": [
    "pipe  = pipeline(\"automatic-speech-recognition\",\n",
    "                    \"openai/whisper-small\", \n",
    "                    chunk_length_s=30,\n",
    "                    stride_length_s=5,\n",
    "                    return_timestamps=True,\n",
    "                    device=device, \n",
    "                    generate_kwargs = {\"language\": 'Hindi', \"task\": \"translate\"}) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# If MP4 (or other video format) convert to MP3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ffmpeg version 4.4.2-0ubuntu0.22.04.1 Copyright (c) 2000-2021 the FFmpeg developers\n",
      "  built with gcc 11 (Ubuntu 11.2.0-19ubuntu1)\n",
      "  configuration: --prefix=/usr --extra-version=0ubuntu0.22.04.1 --toolchain=hardened --libdir=/usr/lib/x86_64-linux-gnu --incdir=/usr/include/x86_64-linux-gnu --arch=amd64 --enable-gpl --disable-stripping --enable-gnutls --enable-ladspa --enable-libaom --enable-libass --enable-libbluray --enable-libbs2b --enable-libcaca --enable-libcdio --enable-libcodec2 --enable-libdav1d --enable-libflite --enable-libfontconfig --enable-libfreetype --enable-libfribidi --enable-libgme --enable-libgsm --enable-libjack --enable-libmp3lame --enable-libmysofa --enable-libopenjpeg --enable-libopenmpt --enable-libopus --enable-libpulse --enable-librabbitmq --enable-librubberband --enable-libshine --enable-libsnappy --enable-libsoxr --enable-libspeex --enable-libsrt --enable-libssh --enable-libtheora --enable-libtwolame --enable-libvidstab --enable-libvorbis --enable-libvpx --enable-libwebp --enable-libx265 --enable-libxml2 --enable-libxvid --enable-libzimg --enable-libzmq --enable-libzvbi --enable-lv2 --enable-omx --enable-openal --enable-opencl --enable-opengl --enable-sdl2 --enable-pocketsphinx --enable-librsvg --enable-libmfx --enable-libdc1394 --enable-libdrm --enable-libiec61883 --enable-chromaprint --enable-frei0r --enable-libx264 --enable-shared\n",
      "  libavutil      56. 70.100 / 56. 70.100\n",
      "  libavcodec     58.134.100 / 58.134.100\n",
      "  libavformat    58. 76.100 / 58. 76.100\n",
      "  libavdevice    58. 13.100 / 58. 13.100\n",
      "  libavfilter     7.110.100 /  7.110.100\n",
      "  libswscale      5.  9.100 /  5.  9.100\n",
      "  libswresample   3.  9.100 /  3.  9.100\n",
      "  libpostproc    55.  9.100 / 55.  9.100\n",
      "Input #0, mov,mp4,m4a,3gp,3g2,mj2, from '2025-03-15 10-36-27.mp4':\n",
      "  Metadata:\n",
      "    major_brand     : isom\n",
      "    minor_version   : 512\n",
      "    compatible_brands: isomiso2avc1mp41\n",
      "    encoder         : Lavf58.76.100\n",
      "  Duration: 00:12:34.45, start: 0.000000, bitrate: 595 kb/s\n",
      "  Stream #0:0(und): Video: h264 (High) (avc1 / 0x31637661), yuv420p(tv, bt709), 1920x1080 [SAR 1:1 DAR 16:9], 390 kb/s, 60 fps, 60 tbr, 15360 tbn, 120 tbc (default)\n",
      "    Metadata:\n",
      "      handler_name    : VideoHandler\n",
      "      vendor_id       : [0][0][0][0]\n",
      "  Stream #0:1(und): Audio: aac (LC) (mp4a / 0x6134706D), 48000 Hz, mono, fltp, 192 kb/s (default)\n",
      "    Metadata:\n",
      "      handler_name    : SoundHandler\n",
      "      vendor_id       : [0][0][0][0]\n",
      "Stream mapping:\n",
      "  Stream #0:1 -> #0:0 (aac (native) -> mp3 (libmp3lame))\n",
      "Press [q] to stop, [?] for help\n",
      "Output #0, mp3, to 'output.mp3':\n",
      "  Metadata:\n",
      "    major_brand     : isom\n",
      "    minor_version   : 512\n",
      "    compatible_brands: isomiso2avc1mp41\n",
      "    TSSE            : Lavf58.76.100\n",
      "  Stream #0:0(und): Audio: mp3, 48000 Hz, mono, fltp, 192 kb/s (default)\n",
      "    Metadata:\n",
      "      handler_name    : SoundHandler\n",
      "      vendor_id       : [0][0][0][0]\n",
      "      encoder         : Lavc58.134.100 libmp3lame\n",
      "size=   15104kB time=00:10:53.85 bitrate= 189.2kbits/s speed= 145x    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conversion successful: output.mp3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "size=   17682kB time=00:12:34.34 bitrate= 192.0kbits/s speed= 145x    \n",
      "video:0kB audio:17681kB subtitle:0kB other streams:0kB global headers:0kB muxing overhead: 0.003999%\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "\n",
    "def convert_mp4_to_mp3(input_file, output_file):\n",
    "    \"\"\"Convert an MP4 file to MP3 using ffmpeg.\"\"\"\n",
    "    try:\n",
    "        command = [\n",
    "            \"ffmpeg\",\n",
    "            \"-i\", input_file,    # Input file\n",
    "            \"-vn\",               # No video\n",
    "            \"-acodec\", \"libmp3lame\",  # MP3 codec\n",
    "            \"-b:a\", \"192k\",      # Audio bitrate\n",
    "            output_file          # Output file\n",
    "        ]\n",
    "        subprocess.run(command, check=True)\n",
    "        print(f\"Conversion successful: {output_file}\")\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"Error during conversion: {e}\")\n",
    "\n",
    "# Example usage\n",
    "convert_mp4_to_mp3(\"2025-03-15 10-36-27.mp4\", \"output.mp3\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let’s process an audio file. The model will transcribe and translate it into English.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mohan.dash/miniconda3/envs/diffuser_data_generation/lib/python3.12/site-packages/transformers/models/whisper/generation_whisper.py:573: FutureWarning: The input name `inputs` is deprecated. Please make sure to use `input_features` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.0, 311.0)--> Before using the code, I would like you to come to this video and install the necessary packages and set up your environment. You can either use python's virtual environment or condi environment. You have to install all this library and after that we should be good to go. You can use all this Jupyter notebook freely. Coming to the code of meeting summarizer. for the whisper model. Now coming to the code you will need to install ffmpg library. If you do not have it already, it is used to operate on media files, video, audio and all that. I have mentioned the link here. You can follow this link to install it properly. With that said, let's start. From the transformer library, I am going to use the pipeline method auto model for causal language model, which is the large language model that we are going to use. And then the tokenizer method to tokenize our text. I am also using the torch some clear output and this one is not really important. So I am going to also specify my device. CODA is for GPU and if you do not have a GPU you should use a CPU. So this particular line helps it automatically decide. The first step is converting the speech. In this case either we have audio or video. That speech to text. For that we are going to use the whisper model. Whisper is an automatic speech recognition model which is open sourced by OpenEI. I have made already a very descriptive playlist about Whisper model from starting from what is this model and how to fine tune it. You can find them in the i button above and also in the description below. So let me explain each and every parameter of this Whisper model So, first I will start my pipeline and in the pipeline I specify the task type. So, here I am writing automatic speech recognition. Then which model I am going to use. I am going to use openAI's whisper model and the small variant of it. There are I think 6 variant of it. You can check them out. Chunk length in seconds that means I am gonna like however long because my mp3 file can be any length but I cannot process all the entire mp3 file at once. So I can divide it into 30 second of maximum length chunks. And then transcribe that 30 seconds and later concatenate them all together. I also specify written timestamps equal to true that means I along with the text I also need the timestamp that at what time of the recording that thing has been said and this part the generation keywords is kind of important here in the language you have to choose the language that that the meeting is spoken in majorly in this case you can choose any language and what whisper can do is it can either transcribe the language or it can translate to English. In this case I am choosing it to translate to English. Here if you have English here or you want to transcribe it is in Hindi you can write here transcribe. We will see what changes it brings to as well. So now it is as simple as this we initialize this pipeline and then we just pass in the path to our mp3 file and it does the rest.\n",
      "(311.0, 745.76)--> Here in the transcription you get everything. Later I am creating this kind of helper function to add the timestamp and the text in a single txt file. So after I think few seconds I got the transcription. So, here you see transcription from 0 to 5.5 second this thing has been said then from 5. Okay. So, after the transcription is completed, get something like this. First is the timestamp and then during the timestamp what has been said. Thank you very much. In my audio that I am using, I will show which audio I am using. I am using a clip from a podcast which is said partially in Hindi, partially in English. So it's a good challenge. So you see they speak both in English and in English. So you see they speak both in Hindi as well as in English. But our model is able to translate them pretty effectively. Whenever there our journey begins where we are going to summarize the transcripted text using a large language model. In this case I am going to use Lama model. Lama 3.2 version with a 3 billion parameter model. I have already made a lot of videos on Lama. I link that in the I button above. So first I will initialize my model and my tokenizer. Here I am using a 3 billion parameter model but if you have less GPU then you can also use 1 billion parameter model as well. Then there are other things as well like quantization and everything with which you can easily if you have even a very small GPU you can usually use them. Now we have to create our conversation. In the conversation it's pretty simple as I say write the minutes of this meeting transcript in simple and precise English and now I that is the role that that is the instruction to the system I have given and this is my user's query you can say the entire text that I just transcribed it. And now then I apply chat template to this one and then tokenize it. Now this input is just tokens that I can pass into my model. After that I use the model to generate method. I pass my input that I just created and in the output I want maximum 1000 tokens and do sample equal to true that means there is some stochasticity inside the model and it every time you run this cell it will give you a different result because it samples from most probable list of words every time. So now let's run it. Yes. So now you see after that whatever output I get I am going to decode it as well. I use the tokenizer to decode it back to the sentences or texts and I am skipping the special tokens such as a meeting of sentence and of sentence speaker, the user and the assistant. Now that this is the output. Here you see you get meeting minutes, attendance not available, date not available. The topic is India's economic growth and comparison with China. Here are some key points that has been discussed. In such a simple manner, if you have the meeting in either video or audio format, you can easily transcribe that into point wise format for easy digestion or for you know like sharing with your colleagues. The same method can be used to summarize any YouTube videos as well. So yeah that was all about this video. If you like it it do give it a thumbs up and subscribe to my channel for such to be notified for such further videos that I am going to upload on large language model computer vision.\n",
      "(745.76, 751.4)--> Till then, goodbye and stay curious.\n"
     ]
    }
   ],
   "source": [
    "transcription = pipe(\"output.mp3\" )\n",
    "#Once the transcription is complete, we’ll format the text with timestamps for better readability.\n",
    "formatted_lyrics = \"\"\n",
    "for line in transcription['chunks']:\n",
    "    text = line[\"text\"]\n",
    "    ts = line[\"timestamp\"]\n",
    "    formatted_lyrics += f\"{ts}-->{text}\\n\"\n",
    "\n",
    "print(formatted_lyrics.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcription saved to transcription.txt\n"
     ]
    }
   ],
   "source": [
    "# Let’s also save the transcription to a text file so we can use it later!\"\n",
    "with open(\"transcription.txt\", \"w\", encoding=\"utf-8\") as file:\n",
    "    file.write(formatted_lyrics.strip())\n",
    "\n",
    "print(\"Transcription saved to transcription.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SUMMARIZING WITH LLAMA\n",
    "Now, let’s take it a step further! We’ll use Meta’s LLaMA model to summarize the transcript. For this, we need the Llama-3.2-3B-Instruct model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "295e4a72a5f6400e854bba5d4e43ce96",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "DEFAULT_MODEL = \"meta-llama/Llama-3.2-3B-Instruct\"\n",
    "\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    DEFAULT_MODEL,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    use_safetensors=True,\n",
    "    device_map=device,\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(DEFAULT_MODEL, use_safetensors=True)\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We’ll define a conversation prompt instructing LLaMA to summarize the meeting transcript in simple English."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "assistant\n",
      "\n",
      "**Meeting Minutes**\n",
      "\n",
      "**Attendees:** N/A\n",
      "\n",
      "**Date:** N/A\n",
      "\n",
      "**Topic:** India's Economic Growth and Comparison with China\n",
      "\n",
      "**Key Points:**\n",
      "\n",
      "1. The speaker discusses the economic growth of India and China, stating that India's growth has been remarkable despite facing severe odds, including a lower per capita income when India opened up in 1991.\n",
      "2. The speaker highlights the differences between India and China's economic models, noting that China's model has frailties that India's model does not have.\n",
      "3. The speaker praises China's achievements in technology, but notes that India's achievements in IT services and cultural identity are also significant.\n",
      "4. The speaker questions why China was able to assimilate with the West so quickly, and attributes this to the Cultural Revolution and the Chinese Communist Party's decision to westernize.\n",
      "5. The speaker emphasizes the importance of holding onto one's identity and cultural heritage, citing this as a key factor in a country's success.\n",
      "6. The speaker criticizes India's financial system for being destroyed, and notes that environmental degradation in China is a significant concern.\n",
      "7. The speaker concludes that India can do better and praises the Communist Party in China for embracing free market principles.\n",
      "\n",
      "**Action Items:** None\n",
      "\n",
      "**Next Steps:** N/A\n",
      "\n",
      "**Conclusion:** The meeting discussed India's economic growth and comparison with China, highlighting the differences between their economic models and the importance of cultural identity in a country's success.\n"
     ]
    }
   ],
   "source": [
    "conversation = [\n",
    "    {\"role\": \"system\", \"content\": ''' Write the minutes of this meeting transcript in simple and precise English.'''},\n",
    "    {\"role\": \"user\", \"content\": f'''{formatted_lyrics.strip()}'''},\n",
    "]\n",
    "\n",
    "prompt = tokenizer.apply_chat_template(conversation, tokenize=False)\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "# print(prompt)\n",
    "\n",
    "with torch.no_grad():\n",
    "    output = model.generate(\n",
    "        **inputs,\n",
    "        do_sample=True,\n",
    "        max_new_tokens=1000\n",
    "    )\n",
    "\n",
    "processed_text = tokenizer.decode(output[0][len(inputs.input_ids[0]):], skip_special_tokens=True)\n",
    "\n",
    "print(processed_text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "diffuser_data_generation",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
