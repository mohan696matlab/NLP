{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "from transformers import GPT2Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50257\n"
     ]
    }
   ],
   "source": [
    "# Initialize tokenizer\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "vocab_size = tokenizer.vocab_size\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"This thesis addresses the critical need for effective Fault Detection and Isolation (FDI) in green hydrogen (GH2) production, a key player in mitigating the greenhouse effect. To tackle this challenge, this thesis introduces a hybrid strategy for FDI. Extensive reviews of FDI algorithms reveal a gap in existing literature, emphasizing accuracy but neglecting the need for labeled data. Additionally, explainability in Hybrid-FDI is often overlooked. The proposed hybrid approach aims to be efficient in data usage and explainable, leveraging physics-based models and Artificial Intelligence (AI). This study introduces Bond Graph-Convolutional Neural Net (BG-CNN), a novel hybrid FDI method addressing AI model training challenges for fault diagnosis. BG-CNN combines BG residual generation and CNN-based fault classification, particularly in scenarios with limited labeled data. Additionally, a Self-Supervised Learning (SSL) method enhances FDI in such situations. The study also discusses Bond Graph-eXplainable AI (BG-XAI), an occlusion-based method, emphasizing the importance of meaningful explanations for fault predictions, showcasing its effectiveness through visualizations. The BG-CNN method with SSL was employed for the FDI of the Proton Exchange Membrane (PEM) electrolyzer and railway tracks, surpassing the performance of traditional methods. Comparative analysis demonstrated the superior performance of the proposed method, particularly in scenarios with limited labeled data, outperforming state-of-the-art SSL methods. The BG-XAI method was used to provide explanations for predictions in accordance with structural analysis.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Prepare the Dataset\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, text, seq_length, tokenizer):\n",
    "        self.seq_length = seq_length\n",
    "        self.tokenizer = tokenizer\n",
    "        self.data = tokenizer.encode(text)\n",
    "        self.vocab_size = tokenizer.vocab_size\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data) - self.seq_length\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = self.data[idx:idx + self.seq_length]\n",
    "        y = self.data[idx + 1:idx + self.seq_length + 1]\n",
    "        return torch.tensor(x, dtype=torch.long), torch.tensor(y, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    " # Prepare data\n",
    "seq_length = 10\n",
    "batch_size = 16\n",
    "\n",
    "dataset = TextDataset(text, seq_length, tokenizer)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 10]) torch.Size([16, 10])\n"
     ]
    }
   ],
   "source": [
    "for x,y in dataloader:\n",
    "    break\n",
    "\n",
    "print(x.shape,y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['), a novel hybrid FDI method addressing AI model',\n",
       " '-based models and Artificial Intelligence (AI). This',\n",
       " 'izations. The BG-CNN method with SSL was',\n",
       " ' in Hybrid-FDI is often overlooked. The',\n",
       " '-Supervised Learning (SSL) method enhances F',\n",
       " ' method addressing AI model training challenges for fault diagnosis.',\n",
       " ' algorithms reveal a gap in existing literature, emphasizing accuracy',\n",
       " ' and Artificial Intelligence (AI). This study introduces Bond',\n",
       " 'ative analysis demonstrated the superior performance of the proposed method',\n",
       " ' Self-Supervised Learning (SSL) method enhances',\n",
       " 'Supervised Learning (SSL) method enhances FDI',\n",
       " 'FDI) in green hydrogen (GH2)',\n",
       " ' leveraging physics-based models and Artificial Intelligence (AI',\n",
       " ' Proton Exchange Membrane (PEM)',\n",
       " ' predictions, showcasing its effectiveness through visualizations. The',\n",
       " ' explanations for fault predictions, showcasing its effectiveness through visual']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.batch_decode(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' a novel hybrid FDI method addressing AI model training',\n",
       " 'based models and Artificial Intelligence (AI). This study',\n",
       " '. The BG-CNN method with SSL was employed',\n",
       " ' Hybrid-FDI is often overlooked. The proposed',\n",
       " 'Supervised Learning (SSL) method enhances FDI',\n",
       " ' addressing AI model training challenges for fault diagnosis. BG',\n",
       " ' reveal a gap in existing literature, emphasizing accuracy but',\n",
       " ' Artificial Intelligence (AI). This study introduces Bond Graph',\n",
       " ' analysis demonstrated the superior performance of the proposed method,',\n",
       " '-Supervised Learning (SSL) method enhances F',\n",
       " 'vised Learning (SSL) method enhances FDI in',\n",
       " 'DI) in green hydrogen (GH2) production',\n",
       " ' physics-based models and Artificial Intelligence (AI).',\n",
       " 'ton Exchange Membrane (PEM) electroly',\n",
       " ', showcasing its effectiveness through visualizations. The BG',\n",
       " ' for fault predictions, showcasing its effectiveness through visualizations']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.batch_decode(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1212, 21554,  9405,   262,  4688,   761]]) torch.Size([1, 6])\n"
     ]
    }
   ],
   "source": [
    "x_text = \"This thesis addresses the critical need\"\n",
    "x = tokenizer(x_text, return_tensors='pt').input_ids\n",
    "print(x,x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 6, 64])\n"
     ]
    }
   ],
   "source": [
    "embed_size = 64\n",
    "\n",
    "embedding = nn.Embedding(vocab_size, embed_size)\n",
    "\n",
    "x_emb = embedding(x)\n",
    "\n",
    "print(x_emb.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 10, 64])\n"
     ]
    }
   ],
   "source": [
    "seq_length = 10\n",
    "\n",
    "positional_encoding = nn.Parameter(torch.zeros(1, seq_length, embed_size))\n",
    "print(positional_encoding.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 6, 64])\n"
     ]
    }
   ],
   "source": [
    "x_emb_pos_encoding = x_emb + positional_encoding[:, :x_emb.shape[1], :]\n",
    "print(x_emb_pos_encoding.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "num_heads = 4\n",
    "hidden_dim = 128\n",
    "\n",
    "\n",
    "transformer_layer = nn.TransformerEncoderLayer(\n",
    "                d_model=embed_size,\n",
    "                nhead=num_heads,\n",
    "                dim_feedforward=hidden_dim,\n",
    "                batch_first=True\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33472\n"
     ]
    }
   ],
   "source": [
    "num_weights=0\n",
    "for p in transformer_layer.parameters():\n",
    "    num_weights += p.numel()\n",
    "\n",
    "print(num_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_contextual_emb = transformer_layer(x_emb_pos_encoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 6, 64])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_contextual_emb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 7.3243e-01,  3.5839e-02, -4.4310e-01,  1.1105e-01,  4.1360e-01,\n",
       "         2.3524e-01,  6.0227e-01,  2.2501e-03, -1.7107e+00, -7.1124e-01,\n",
       "         1.0923e+00,  5.4993e-02, -8.4960e-01, -1.1927e+00,  1.5672e+00,\n",
       "        -5.4057e-01,  4.7778e-01, -4.2656e-01, -1.6243e+00, -1.6682e+00,\n",
       "         8.1536e-02,  2.0591e-01, -5.9917e-01,  2.4865e+00, -4.0231e-01,\n",
       "         3.5988e-01,  6.4603e-01, -2.4307e-01, -1.7524e+00,  6.0048e-01,\n",
       "         1.5870e+00,  1.1522e+00, -2.9785e-01, -6.7968e-01, -9.3511e-01,\n",
       "        -2.0805e+00, -6.5376e-01,  8.5285e-01,  1.4435e+00, -1.3131e+00,\n",
       "         9.9067e-01,  1.1094e+00,  3.1653e-01, -1.8434e-01,  2.5890e-01,\n",
       "         1.5518e-01,  1.9225e+00, -8.0886e-01,  5.5146e-01,  1.2097e+00,\n",
       "         6.6111e-01, -7.7961e-01,  9.6408e-01, -1.2769e+00,  1.7773e+00,\n",
       "        -7.8222e-01, -6.6785e-01,  2.0683e-01, -3.9245e-01, -2.1446e+00,\n",
       "         4.2825e-01, -5.4143e-01,  5.4102e-01, -1.3164e-01],\n",
       "       grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_contextual_emb[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([6, 6])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask = torch.triu(torch.ones(x_emb.shape[1],x_emb.shape[1]) * float('-inf'), diagonal=1).to(x.device)\n",
    "mask.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 6, 64])\n"
     ]
    }
   ],
   "source": [
    "x_contextual_emb = transformer_layer(x_emb_pos_encoding,mask)\n",
    "print(x_contextual_emb.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 6, 50257])\n"
     ]
    }
   ],
   "source": [
    "fc = nn.Linear(embed_size, vocab_size)\n",
    "x_logits = fc(x_contextual_emb)\n",
    "print(x_logits.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1212, 21554,  9405,   262,  4688,   761,   329,  4050, 40050, 46254]]) torch.Size([1, 10])\n",
      "tensor([[21554,  9405,   262,  4688,   761,   329,  4050, 40050, 46254,   290]]) torch.Size([1, 10])\n",
      "['This thesis addresses the critical need for effective Fault Detection']\n",
      "[' thesis addresses the critical need for effective Fault Detection and']\n"
     ]
    }
   ],
   "source": [
    "text = \"This thesis addresses the critical need for effective Fault Detection and Isolation (FDI) in green hydrogen (GH2) production, a key player in mitigating the greenhouse effect. To tackle this challenge, this thesis introduces a hybrid strategy for FDI. Extensive reviews of FDI algorithms reveal a gap in existing literature, emphasizing accuracy but neglecting the need for labeled data. Additionally, explainability in Hybrid-FDI is often overlooked. The proposed hybrid approach aims to be efficient in data usage and explainable, leveraging physics-based models and Artificial Intelligence (AI). This study introduces Bond Graph-Convolutional Neural Net (BG-CNN), a novel hybrid FDI method addressing AI model training challenges for fault diagnosis. BG-CNN combines BG residual generation and CNN-based fault classification, particularly in scenarios with limited labeled data. Additionally, a Self-Supervised Learning (SSL) method enhances FDI in such situations. The study also discusses Bond Graph-eXplainable AI (BG-XAI), an occlusion-based method, emphasizing the importance of meaningful explanations for fault predictions, showcasing its effectiveness through visualizations. The BG-CNN method with SSL was employed for the FDI of the Proton Exchange Membrane (PEM) electrolyzer and railway tracks, surpassing the performance of traditional methods. Comparative analysis demonstrated the superior performance of the proposed method, particularly in scenarios with limited labeled data, outperforming state-of-the-art SSL methods. The BG-XAI method was used to provide explanations for predictions in accordance with structural analysis.\"\n",
    "\n",
    "tokenized_text = tokenizer(text, return_tensors='pt').input_ids\n",
    "\n",
    "seq_length = 10\n",
    "x = tokenized_text[:,:seq_length]\n",
    "y = tokenized_text[:,1:seq_length+1]\n",
    "\n",
    "print(x, x.shape)\n",
    "print(y, y.shape)\n",
    "\n",
    "print(tokenizer.batch_decode(x))\n",
    "print(tokenizer.batch_decode(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 10, 50257])\n",
      "tensor([[16884, 48322, 13148, 13828,  9192, 22156, 48815, 31396, 24469, 46729]])\n",
      "tensor(10.9978, grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "x_emb = embedding(x)\n",
    "x_emb_pos_encoding = x_emb + positional_encoding[:, :x_emb.shape[1], :]\n",
    "mask = torch.triu(torch.ones(x_emb.shape[1],x_emb.shape[1]) * float('-inf'), diagonal=1).to(x.device)\n",
    "x_contextual_emb = transformer_layer(x_emb_pos_encoding,mask)\n",
    "x_logits = fc(x_contextual_emb)\n",
    "\n",
    "print(x_logits.shape)\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "print(x_logits.argmax(dim=-1))\n",
    "\n",
    "loss = loss_fn(x_logits.view(-1, vocab_size), y.view(-1))\n",
    "\n",
    "print(loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Define the Model\n",
    "class TransformerLanguageModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, num_heads, hidden_dim, seq_length):\n",
    "        super(TransformerLanguageModel, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
    "        self.positional_encoding = nn.Parameter(torch.zeros(1, seq_length, embed_size))\n",
    "\n",
    "        self.encoder_layer = nn.TransformerEncoderLayer(\n",
    "                d_model=embed_size,\n",
    "                nhead=num_heads,\n",
    "                dim_feedforward=hidden_dim,\n",
    "                batch_first=True) \n",
    "        \n",
    "        self.fc = nn.Linear(embed_size, vocab_size)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        seq_length = x.size(1)\n",
    "        x = self.embedding(x) + self.positional_encoding[:, :seq_length, :]\n",
    " \n",
    "        x = self.encoder_layer(x, src_mask=mask)\n",
    "\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 20\n",
    "learning_rate = 0.001\n",
    "\n",
    "# Model, loss, optimizer\n",
    "model = TransformerLanguageModel(\n",
    "    vocab_size=dataset.vocab_size,\n",
    "    embed_size=embed_size,\n",
    "    num_heads=num_heads,\n",
    "    hidden_dim=hidden_dim,\n",
    "    seq_length=seq_length\n",
    ")\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d08230b0b72f4b04ab9f428a617f0f12",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20, Loss: 10.4195\n",
      "Epoch 2/20, Loss: 8.6267\n",
      "Epoch 3/20, Loss: 6.2607\n",
      "Epoch 4/20, Loss: 4.5613\n",
      "Epoch 5/20, Loss: 3.6864\n",
      "Epoch 6/20, Loss: 3.0292\n",
      "Epoch 7/20, Loss: 2.4021\n",
      "Epoch 8/20, Loss: 1.8674\n",
      "Epoch 9/20, Loss: 1.4499\n",
      "Epoch 10/20, Loss: 1.1197\n",
      "Epoch 11/20, Loss: 0.9039\n",
      "Epoch 12/20, Loss: 0.7394\n",
      "Epoch 13/20, Loss: 0.6221\n",
      "Epoch 14/20, Loss: 0.5425\n",
      "Epoch 15/20, Loss: 0.4686\n",
      "Epoch 16/20, Loss: 0.4154\n",
      "Epoch 17/20, Loss: 0.3781\n",
      "Epoch 18/20, Loss: 0.3438\n",
      "Epoch 19/20, Loss: 0.3223\n",
      "Epoch 20/20, Loss: 0.2969\n"
     ]
    }
   ],
   "source": [
    "for epoch in tqdm(range(num_epochs)):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for x, y in dataloader:\n",
    "            optimizer.zero_grad()\n",
    "            mask = torch.triu(torch.ones(seq_length, seq_length) * float('-inf'), diagonal=1).to(x.device)\n",
    "            output = model(x, mask)\n",
    "            loss = criterion(output.view(-1, dataset.vocab_size), y.view(-1))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        avg_loss = total_loss / len(dataloader)\n",
    "        print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {avg_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Text: a key player in mitigating the greenhouse effect.\n",
      "Generated Text: a key player in mitigating the greenhouse effect. To\n",
      "Generated Text: a key player in mitigating the greenhouse effect. To tackle\n",
      "Generated Text: a key player in mitigating the greenhouse effect. To tackle this\n",
      "Generated Text: a key player in mitigating the greenhouse effect. To tackle this challenge\n",
      "Generated Text: a key player in mitigating the greenhouse effect. To tackle this challenge,\n",
      "Generated Text: a key player in mitigating the greenhouse effect. To tackle this challenge, this\n",
      "Generated Text: a key player in mitigating the greenhouse effect. To tackle this challenge, this thesis\n",
      "Generated Text: a key player in mitigating the greenhouse effect. To tackle this challenge, this thesis introduces\n",
      "Generated Text: a key player in mitigating the greenhouse effect. To tackle this challenge, this thesis introduces a\n",
      "Generated Text: a key player in mitigating the greenhouse effect. To tackle this challenge, this thesis introduces a hybrid\n",
      "Generated Text: a key player in mitigating the greenhouse effect. To tackle this challenge, this thesis introduces a hybrid strategy\n",
      "Generated Text: a key player in mitigating the greenhouse effect. To tackle this challenge, this thesis introduces a hybrid strategy for\n",
      "Generated Text: a key player in mitigating the greenhouse effect. To tackle this challenge, this thesis introduces a hybrid strategy for F\n",
      "Generated Text: a key player in mitigating the greenhouse effect. To tackle this challenge, this thesis introduces a hybrid strategy for FDI\n",
      "Generated Text: a key player in mitigating the greenhouse effect. To tackle this challenge, this thesis introduces a hybrid strategy for FDI.\n",
      "Generated Text: a key player in mitigating the greenhouse effect. To tackle this challenge, this thesis introduces a hybrid strategy for FDI. Ext\n",
      "Generated Text: a key player in mitigating the greenhouse effect. To tackle this challenge, this thesis introduces a hybrid strategy for FDI. Extensive\n",
      "Generated Text: a key player in mitigating the greenhouse effect. To tackle this challenge, this thesis introduces a hybrid strategy for FDI. Extensive reviews\n",
      "Generated Text: a key player in mitigating the greenhouse effect. To tackle this challenge, this thesis introduces a hybrid strategy for FDI. Extensive reviews of\n",
      "Generated Text: a key player in mitigating the greenhouse effect. To tackle this challenge, this thesis introduces a hybrid strategy for FDI. Extensive reviews of F\n",
      "Generated Text: a key player in mitigating the greenhouse effect. To tackle this challenge, this thesis introduces a hybrid strategy for FDI. Extensive reviews of FDI\n",
      "Generated Text: a key player in mitigating the greenhouse effect. To tackle this challenge, this thesis introduces a hybrid strategy for FDI. Extensive reviews of FDI algorithms\n",
      "Generated Text: a key player in mitigating the greenhouse effect. To tackle this challenge, this thesis introduces a hybrid strategy for FDI. Extensive reviews of FDI algorithms reveal\n",
      "Generated Text: a key player in mitigating the greenhouse effect. To tackle this challenge, this thesis introduces a hybrid strategy for FDI. Extensive reviews of FDI algorithms reveal a\n",
      "Generated Text: a key player in mitigating the greenhouse effect. To tackle this challenge, this thesis introduces a hybrid strategy for FDI. Extensive reviews of FDI algorithms reveal a gap\n",
      "Generated Text: a key player in mitigating the greenhouse effect. To tackle this challenge, this thesis introduces a hybrid strategy for FDI. Extensive reviews of FDI algorithms reveal a gap in\n",
      "Generated Text: a key player in mitigating the greenhouse effect. To tackle this challenge, this thesis introduces a hybrid strategy for FDI. Extensive reviews of FDI algorithms reveal a gap in existing\n",
      "Generated Text: a key player in mitigating the greenhouse effect. To tackle this challenge, this thesis introduces a hybrid strategy for FDI. Extensive reviews of FDI algorithms reveal a gap in existing literature\n",
      "Generated Text: a key player in mitigating the greenhouse effect. To tackle this challenge, this thesis introduces a hybrid strategy for FDI. Extensive reviews of FDI algorithms reveal a gap in existing literature,\n",
      "Generated Text: a key player in mitigating the greenhouse effect. To tackle this challenge, this thesis introduces a hybrid strategy for FDI. Extensive reviews of FDI algorithms reveal a gap in existing literature, emphasizing\n",
      "Generated Text: a key player in mitigating the greenhouse effect. To tackle this challenge, this thesis introduces a hybrid strategy for FDI. Extensive reviews of FDI algorithms reveal a gap in existing literature, emphasizing accuracy\n",
      "Generated Text: a key player in mitigating the greenhouse effect. To tackle this challenge, this thesis introduces a hybrid strategy for FDI. Extensive reviews of FDI algorithms reveal a gap in existing literature, emphasizing accuracy but\n",
      "Generated Text: a key player in mitigating the greenhouse effect. To tackle this challenge, this thesis introduces a hybrid strategy for FDI. Extensive reviews of FDI algorithms reveal a gap in existing literature, emphasizing accuracy but neglect\n",
      "Generated Text: a key player in mitigating the greenhouse effect. To tackle this challenge, this thesis introduces a hybrid strategy for FDI. Extensive reviews of FDI algorithms reveal a gap in existing literature, emphasizing accuracy but neglecting\n",
      "Generated Text: a key player in mitigating the greenhouse effect. To tackle this challenge, this thesis introduces a hybrid strategy for FDI. Extensive reviews of FDI algorithms reveal a gap in existing literature, emphasizing accuracy but neglecting the\n",
      "Generated Text: a key player in mitigating the greenhouse effect. To tackle this challenge, this thesis introduces a hybrid strategy for FDI. Extensive reviews of FDI algorithms reveal a gap in existing literature, emphasizing accuracy but neglecting the need\n",
      "Generated Text: a key player in mitigating the greenhouse effect. To tackle this challenge, this thesis introduces a hybrid strategy for FDI. Extensive reviews of FDI algorithms reveal a gap in existing literature, emphasizing accuracy but neglecting the need for\n",
      "Generated Text: a key player in mitigating the greenhouse effect. To tackle this challenge, this thesis introduces a hybrid strategy for FDI. Extensive reviews of FDI algorithms reveal a gap in existing literature, emphasizing accuracy but neglecting the need for labeled\n",
      "Generated Text: a key player in mitigating the greenhouse effect. To tackle this challenge, this thesis introduces a hybrid strategy for FDI. Extensive reviews of FDI algorithms reveal a gap in existing literature, emphasizing accuracy but neglecting the need for labeled data\n",
      "Generated Text: a key player in mitigating the greenhouse effect. To tackle this challenge, this thesis introduces a hybrid strategy for FDI. Extensive reviews of FDI algorithms reveal a gap in existing literature, emphasizing accuracy but neglecting the need for labeled data.\n",
      "Generated Text: a key player in mitigating the greenhouse effect. To tackle this challenge, this thesis introduces a hybrid strategy for FDI. Extensive reviews of FDI algorithms reveal a gap in existing literature, emphasizing accuracy but neglecting the need for labeled data. Additionally\n",
      "Generated Text: a key player in mitigating the greenhouse effect. To tackle this challenge, this thesis introduces a hybrid strategy for FDI. Extensive reviews of FDI algorithms reveal a gap in existing literature, emphasizing accuracy but neglecting the need for labeled data. Additionally,\n",
      "Generated Text: a key player in mitigating the greenhouse effect. To tackle this challenge, this thesis introduces a hybrid strategy for FDI. Extensive reviews of FDI algorithms reveal a gap in existing literature, emphasizing accuracy but neglecting the need for labeled data. Additionally, explain\n",
      "Generated Text: a key player in mitigating the greenhouse effect. To tackle this challenge, this thesis introduces a hybrid strategy for FDI. Extensive reviews of FDI algorithms reveal a gap in existing literature, emphasizing accuracy but neglecting the need for labeled data. Additionally, explainability\n",
      "Generated Text: a key player in mitigating the greenhouse effect. To tackle this challenge, this thesis introduces a hybrid strategy for FDI. Extensive reviews of FDI algorithms reveal a gap in existing literature, emphasizing accuracy but neglecting the need for labeled data. Additionally, explainability in\n",
      "Generated Text: a key player in mitigating the greenhouse effect. To tackle this challenge, this thesis introduces a hybrid strategy for FDI. Extensive reviews of FDI algorithms reveal a gap in existing literature, emphasizing accuracy but neglecting the need for labeled data. Additionally, explainability in Hybrid\n",
      "Generated Text: a key player in mitigating the greenhouse effect. To tackle this challenge, this thesis introduces a hybrid strategy for FDI. Extensive reviews of FDI algorithms reveal a gap in existing literature, emphasizing accuracy but neglecting the need for labeled data. Additionally, explainability in Hybrid-\n",
      "Generated Text: a key player in mitigating the greenhouse effect. To tackle this challenge, this thesis introduces a hybrid strategy for FDI. Extensive reviews of FDI algorithms reveal a gap in existing literature, emphasizing accuracy but neglecting the need for labeled data. Additionally, explainability in Hybrid-F\n",
      "Generated Text: a key player in mitigating the greenhouse effect. To tackle this challenge, this thesis introduces a hybrid strategy for FDI. Extensive reviews of FDI algorithms reveal a gap in existing literature, emphasizing accuracy but neglecting the need for labeled data. Additionally, explainability in Hybrid-FDI\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "# Generate text\n",
    "model.eval()\n",
    "input_text = \"a key player in mitigating the greenhouse effect\"\n",
    "input_seq = torch.tensor(tokenizer.encode(input_text), dtype=torch.long).unsqueeze(0)\n",
    "generated = input_text\n",
    "\n",
    "for _ in range(50):\n",
    "    with torch.no_grad():\n",
    "        output = model(input_seq)\n",
    "        next_token = output.argmax(dim=-1)[0,-1].item()\n",
    "        generated += tokenizer.decode([next_token])\n",
    "        input_seq = torch.cat([input_seq, torch.tensor([[next_token]])], dim=1)\n",
    "        input_seq = input_seq[:, -seq_length:]\n",
    "        # break\n",
    "        time.sleep(0.1)\n",
    "        print(\"Generated Text:\", generated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1212, 21554,  9405,   262,  4688,   761,   329,  4050, 40050, 46254]]) torch.Size([1, 10])\n",
      "tensor([[21554, 20718,   262,   376,   761,   329,  4050, 40050, 46254,   290]]) torch.Size([1, 10])\n",
      "[' thesis introduces the F need for effective Fault Detection and']\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "\n",
    "input_text='This thesis addresses the critical need for effective Fault Detection'\n",
    "input_seq = torch.tensor(tokenizer.encode(input_text), dtype=torch.long).unsqueeze(0)\n",
    "print(input_seq,input_seq.shape)\n",
    "with torch.no_grad():\n",
    "    output = model(input_seq)\n",
    "    next_token = output.argmax(dim=-1)[0,-1].item()\n",
    "\n",
    "print(output.argmax(dim=-1),output.argmax(dim=-1).shape)\n",
    "print(tokenizer.batch_decode(output.argmax(dim=-1)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
