{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ac2825887ba45bc9d7bc7be2e7fa9cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2095.841064453125\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, get_scheduler\n",
    "from bitsandbytes.optim import Adam8bit,PagedAdam32bit\n",
    "from peft import LoraConfig, get_peft_model, PeftModel, PeftConfig\n",
    "from peft import prepare_model_for_kbit_training\n",
    "import torch\n",
    "from IPython.display import  clear_output\n",
    "import time\n",
    "import gc\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "DEFAULT_MODEL = \"meta-llama/Llama-3.2-3B-Instruct\"\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_quant_type='nf4',\n",
    "        bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "        bnb_4bit_use_double_quant=False,\n",
    "    )\n",
    "\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    DEFAULT_MODEL,\n",
    "    quantization_config=bnb_config,\n",
    "    use_safetensors=True,\n",
    "    device_map=device,\n",
    ")\n",
    "\n",
    "print(model.get_memory_footprint()/(1024*1024)) \n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(DEFAULT_MODEL, use_safetensors=True)\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "def flush():\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load a Multi Lingual Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f2fe65897ef490db01313b8f51623c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/2.63k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "269d823bab9c4906bcb907a7706be8f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "odia_data_domain_full_train_v2.json:   0%|          | 0.00/9.78M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4077b2dc04bb47a2aef0f2dcbed86877",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/10502 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Load the dataset\n",
    "# dataset = load_dataset(\"OdiaGenAI/hardcode_odia_qa_105\")\n",
    "dataset = load_dataset('OdiaGenAI/odia_domain_context_train_v1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'instruction': 'ଓଡ଼ିଶାର ଅର୍ଥନୀତି ଉପରେ ପର୍ଯ୍ୟଟନର କିଭଳି ପ୍ରଭାବ ପଡ଼ିଛି?',\n",
       " 'input': '',\n",
       " 'output': 'ବିଗତ କିଛି ବର୍ଷ ମଧ୍ୟରେ ପର୍ଯ୍ୟଟନ ଶିଳ୍ପ ଦ୍ୱାରା ଓଡ଼ିଶାର ଅର୍ଥନୀତି ବ୍ୟାପକ ଭାବେ ପ୍ରଭାବିତ ହୋଇଛି। ରାଜ୍ୟରେ ପ୍ରାକୃତିକ ସୌନ୍ଦର୍ଯ୍ୟ ଏବଂ ସାଂସ୍କୃତିକ ଐତିହ୍ୟ ସ୍ଥାନ ରହିଛି, ଯାହା ଦେଶ ଏବଂ ଦେଶ ବାହାରର ପର୍ଯ୍ୟଟକଙ୍କ ପାଇଁ ଆକର୍ଷଣୀୟ ହୋଇପାରିଛି।\\nପର୍ଯ୍ୟଟନ କ୍ଷେତ୍ରରେ କାର୍ଯ୍ୟ କରୁଥିବା ଲୋକମାନଙ୍କ ପାଇଁ ନିଯୁକ୍ତି ସୁଯୋଗ ଏବଂ ଆୟ ବୃଦ୍ଧି ପାଇଛି, ଯେଉଁଥିରେ ଆତିଥ୍ୟ, ଟୁର ଅପରେଟର ଏବଂ ପରିବହନ ଶିଳ୍ପ ସାମିଲ ରହିଛି। ପର୍ଯ୍ୟଟନ ଶିଳ୍ପ ସ୍ଥାନୀୟ ଖାଦ୍ୟ, ହସ୍ତଶିଳ୍ପ ଏବଂ ସାଂସ୍କୃତିକ ଗତିବିଧିକୁ ପ୍ରୋତ୍ସାହିତ କରିବାରେ ସହାୟତା କରିଛି, ଯାହାକି ଓଡ଼ିଶାର ସାମଗ୍ରିକ ଅର୍ଥନୀତି ଉପରେ ସକାରାତ୍ମକ ପ୍ରଭାବ ପକାଇଛି।\\nପର୍ଯ୍ୟଟକମାନଙ୍କ ପାଇଁ ଉତ୍ତମ ଅନୁଭୂତି ସୁନିଶ୍ଚିତ କରିବା ପାଇଁ ଭିତ୍ତିଭୂମି ବିକାଶ ଉପରେ ମଧ୍ୟ ସରକାର ଗୁରୁତ୍ୱ ଦେଉଛନ୍ତି। ସଡ଼କ ଏବଂ ସାର୍ବଜନୀନ ପରିବହନ ବ୍ୟବସ୍ଥାରେ ସୁଧାର କରାଯାଇଛି, ଯେତେବେଳେ କି ରାଜ୍ୟର ଅନେକ କ୍ଷେତ୍ରରେ ବିଭିନ୍ନ ଇକୋ-ଟୁରିଜମ ପ୍ରକଳ୍ପର ବିକାଶ କରାଯାଇଛି।\\nଶେଷରେ ପର୍ଯ୍ୟଟନ ଓଡ଼ିଶାର ଆର୍ଥିକ ବିକାଶକୁ ତ୍ୱରାନ୍ୱିତ କରିବାରେ ଗୁରୁତ୍ୱପୂର୍ଣ୍ଣ ଭୂମିକା ନିର୍ବାହ କରିଛି ଏବଂ ରାଜ୍ୟବାସୀଙ୍କ ପାଇଁ ଅନେକ ଲାଭ ଆଣିପାରିଛି।'}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAkQAAAHHCAYAAABeLEexAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjEsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvc2/+5QAAAAlwSFlzAAAPYQAAD2EBqD+naQAAVNtJREFUeJzt3XlYVGX/P/D3sA3DMuAGSAqSC+KGihu55IKgkbmmlZoLPqZhiriU1ZNrbuUeSn5dsDJMK7VcEFQUF9REcRdJURQFMhdUEAbm/v3hj/M4LDrgwKDn/bourppzPnOf+3wY4O1ZZhRCCAEiIiIiGTMx9gSIiIiIjI2BiIiIiGSPgYiIiIhkj4GIiIiIZI+BiIiIiGSPgYiIiIhkj4GIiIiIZI+BiIiIiGSPgYiIiIhkj4GIXiq1atXC0KFDjT2NV94333yD119/HaampmjatGmZbmvfvn1QKBT49ddfy3Q7xW1337595brdjh07omPHjuW6TSof06ZNg0KhwO3bt409FSoFBiIymrCwMCgUChw/frzI9R07dkSjRo1eeDs7duzAtGnTXngcuYiMjMTkyZPRtm1brF27FrNnzy5Ukx8m9Pmi0tNoNFi6dClatmwJW1tb2NjYoGXLlli6dCk0Gk2pxz18+DCmTZuGe/fuGW6yzzB79mxs2bJFr9qrV69CoVDg22+/LdtJvYCS7A+9PMyMPQGikkhISICJScly/I4dOxASEsJQpKe9e/fCxMQEq1evhoWFRZE1Hh4e+PHHH3WWTZkyBTY2Nvjiiy/KY5ovrEOHDsjKyip2H43t0aNH8Pf3x/79+/H2229j6NChMDExQUREBMaNG4fff/8d27dvh7W1dYnHPnz4MKZPn46hQ4fC3t7e8JMvYPbs2ejXrx969epV5tsqD6/a/tATDET0UlEqlcaeQok9evSoVH+0jCU9PR0qleqZQcHR0RGDBg3SWTZ37lxUrVq10PKKysTEBJaWlsaeRrGCg4Oxf/9+LFu2DGPGjJGWjx49GiEhIRgzZgwmTpyIFStWGHGWRK8OnjKjl0rBa4g0Gg2mT5+OunXrwtLSElWqVEG7du0QFRUFABg6dChCQkIAoMjTOI8ePcKECRNQs2ZNKJVKuLu749tvv4UQQme7WVlZGDt2LKpWrQpbW1u88847SElJgUKh0DnylH8Nwfnz5/HBBx+gUqVKaNeuHQDg9OnTGDp0KF5//XVYWlrCyckJw4cPx7///quzrfwxLl26hEGDBsHOzg7VqlXDf//7XwghcP36dfTs2RNqtRpOTk5YsGCBXr3Lzc3FzJkzUbt2bSiVStSqVQuff/45srOzpRqFQoG1a9fi0aNHUq/CwsL0Gr8oV65cwbvvvovKlSvDysoKbdq0wfbt25/7vOzsbLz99tuws7PD4cOHAQBarRaLFy9Gw4YNYWlpCUdHR3z00Ue4e/euznNr1aqFt99+GwcPHkSrVq1gaWmJ119/HT/88INOXcFriPJP4Rb1VfCan59++gleXl5QqVSoXLky3nvvPVy/fr3QfqxcuRK1a9eGSqVCq1atcODAAb36duPGDaxevRqdO3fWCUP5AgMD0alTJ6xatQo3btwA8L9TTUV9v55+nU6bNg2TJk0CALi5uUn7ePXqVal2zJgxWL9+Pdzd3WFpaQkvLy/ExMTojDl06FDUqlWr0LbyX79Pb/vRo0dYt26dtC1DXAeYnZ2NqVOnok6dOlAqlahZsyYmT56s83p+en+2bNmCRo0aQalUomHDhoiIiCg05r59+9CiRQtYWlqidu3a+P7770u1P/fu3ZOOvtnZ2WHYsGHIzMzUqYmKikK7du1gb28PGxsbuLu74/PPP3/hvlDp8QgRGd39+/eLvAhRn2skpk2bhjlz5mDEiBFo1aoVMjIycPz4cZw4cQJdu3bFRx99hJs3byIqKqrQKR4hBN555x1ER0cjICAATZs2xa5duzBp0iSkpKRg0aJFUu3QoUOxceNGDB48GG3atMH+/fvh7+9f7Lzeffdd1K1bF7Nnz5bCVVRUFK5cuYJhw4bByckJ586dw8qVK3Hu3DkcOXKk0PU2AwYMgIeHB+bOnYvt27dj1qxZqFy5Mr7//nt07twZ8+bNw/r16zFx4kS0bNkSHTp0eGavRowYgXXr1qFfv36YMGECjh49ijlz5uDChQvYvHkzAODHH3/EypUrcezYMaxatQoA8MYbbzz3+1CUtLQ0vPHGG8jMzMTYsWNRpUoVrFu3Du+88w5+/fVX9O7du8jnZWVloWfPnjh+/Dh2796Nli1bAgA++ugjhIWFYdiwYRg7diySkpLw3Xff4eTJkzh06BDMzc2lMf7++2/069cPAQEBGDJkCNasWYOhQ4fCy8sLDRs2LHK7HTp0KPQauXbtGr788ks4ODhIy77++mv897//Rf/+/TFixAj8888/WLZsGTp06ICTJ09Kp6BWr16Njz76CG+88QaCgoJw5coVvPPOO6hcuTJq1qz5zN7t3LkTeXl5+PDDD4ut+fDDDxEdHY2IiAiMGDHimeM9rU+fPrh06RLCw8OxaNEiVK1aFQBQrVo1qWb//v345ZdfMHbsWCiVSixfvhzdunXDsWPHSnxd348//ij9fI4cORIAULt27RKNUZBWq8U777yDgwcPYuTIkfDw8MCZM2ewaNEiXLp0qdD1PQcPHsTvv/+Ojz/+GLa2tli6dCn69u2L5ORkVKlSBQBw8uRJdOvWDdWrV8f06dORl5eHGTNm6PRF3/3p378/3NzcMGfOHJw4cQKrVq2Cg4MD5s2bBwA4d+4c3n77bTRp0gQzZsyAUqnE33//jUOHDr1QX+gFCSIjWbt2rQDwzK+GDRvqPMfV1VUMGTJEeuzp6Sn8/f2fuZ3AwEBR1Et9y5YtAoCYNWuWzvJ+/foJhUIh/v77byGEEHFxcQKACAoK0qkbOnSoACCmTp0qLZs6daoAIN5///1C28vMzCy0LDw8XAAQMTExhcYYOXKktCw3N1fUqFFDKBQKMXfuXGn53bt3hUql0ulJUeLj4wUAMWLECJ3lEydOFADE3r17pWVDhgwR1tbWzxyvKA0bNhRvvvmm9DgoKEgAEAcOHJCWPXjwQLi5uYlatWqJvLw8IYQQ0dHRAoDYtGmTePDggXjzzTdF1apVxcmTJ6XnHThwQAAQ69ev19lmREREoeWurq6Fepqeni6USqWYMGGCtCx/u9HR0UXuT1ZWlvDy8hLOzs7i1q1bQgghrl69KkxNTcXXX3+tU3vmzBlhZmYmLc/JyREODg6iadOmIjs7W6pbuXKlAKDTp6Lk9+7pHhR04sQJAUAEBwcLIYRISkoSAMTatWsL1RZ8nX7zzTcCgEhKSiqyFoA4fvy4tOzatWvC0tJS9O7dW1o2ZMgQ4erqWuj5+a/fp1lbWz/3NZovfz+++eabYmt+/PFHYWJiovPaEkKI0NBQAUAcOnRIZ38sLCykn2chhDh16pQAIJYtWyYt69Gjh7CyshIpKSnSssTERGFmZqb3/uTv+/Dhw3WW9+7dW1SpUkV6vGjRIgFA/PPPP8XuI5U/njIjowsJCUFUVFShryZNmjz3ufb29jh37hwSExNLvN0dO3bA1NQUY8eO1Vk+YcIECCGwc+dOAJAOrX/88cc6dZ988kmxY48aNarQMpVKJf3/48ePcfv2bbRp0wYAcOLEiUL1T/+r39TUFC1atIAQAgEBAdJye3t7uLu748qVK8XOBXiyr8CT61KeNmHCBADQ6zRWSe3YsQOtWrWSThkCgI2NDUaOHImrV6/i/PnzOvX379+Hr68vLl68iH379unc7r9p0ybY2dmha9euuH37tvTl5eUFGxsbREdH64zVoEEDtG/fXnpcrVo1vfr0tI8//hhnzpzBb7/9BicnJwDA77//Dq1Wi/79++vMw8nJCXXr1pXmcfz4caSnp2PUqFE612INHToUdnZ2z932gwcPAAC2trbF1uSvy8jI0Huf9OXt7Q0vLy/psYuLC3r27Ildu3YhLy/P4NsrqU2bNsHDwwP169fX+T507twZAAq9Hnx8fHSO4jRp0gRqtVp6PeTl5WH37t3o1asXnJ2dpbo6deqge/fuJZ5fwZ//9u3b499//5W+V/lHEbdu3QqtVlvi8als8JQZGV2rVq3QokWLQssrVar03PfzmDFjBnr27Il69eqhUaNG6NatGwYPHqxXmLp27RqcnZ0L/dHx8PCQ1uf/18TEBG5ubjp1derUKXbsgrUAcOfOHUyfPh0bNmxAenq6zrr79+8XqndxcdF5bGdnB0tLS+kUx9PLC16HVFD+PhScs5OTE+zt7aV9NaRr166hdevWhZY/3d+nT78EBQXh8ePHOHnyZKHTWomJibh//77OqaunFexnwd4BT15PBa83Ks7333+PtWvX4vvvv5dCa/48hBCoW7dukc/LP22X38+Cdebm5nj99defu/3812R+MCqKPqGptIrav3r16iEzMxP//POPFBCNJTExERcuXCh0OitfSV8P6enpyMrKKvJn+lk/58UpuL1KlSoBAO7evQu1Wo0BAwZg1apVGDFiBD777DN06dIFffr0Qb9+/Up8Fy0ZDgMRvdQ6dOiAy5cvY+vWrYiMjMSqVauwaNEihIaGlui6CkN7+mhQvv79++Pw4cOYNGkSmjZtChsbG2i1WnTr1q3IfyWamprqtQxAoYvAi1OR3xeoZ8+e2LBhA+bOnYsffvhB5w+DVquFg4MD1q9fX+RzC/5hfJE+HTt2DOPGjcOIESOka0SenodCocDOnTuL3IaNjc1zx9dHfmg8ffp0sW+Mefr0aQBPjoYBxX9vy+qITnlv72larRaNGzfGwoULi1xf8BqtF/25KannbU+lUiEmJgbR0dHYvn07IiIi8Msvv6Bz586IjIws9vlUthiI6KVXuXJlDBs2DMOGDcPDhw/RoUMHTJs2TQpExf3idnV1xe7du/HgwQOdf2VfvHhRWp//X61Wi6SkJJ1/Of/99996z/Hu3bvYs2cPpk+fjq+++kpaXppTfaWRvw+JiYnSH1vgyYXP9+7dk/bV0NtMSEgotLxgf/P16tULvr6+GDp0KGxtbXVuJ69duzZ2796Ntm3bFhk2DeWff/5Bv3790LRpU+nuxKfVrl0bQgi4ubmhXr16xY6Tv2+JiYnSaRzgyY0CSUlJ8PT0fOY8unfvDlNTU/z444/FXlj9ww8/wMzMDN26dQPwv6MQBd9ssaijf88LxkW9Li9dugQrKyspfFaqVKnIN3YszfZKqnbt2jh16hS6dOlikLEdHBxgaWlZ5M90UcsMsU0TExN06dIFXbp0wcKFCzF79mx88cUXiI6Oho+PzwuPTyXHY3P0Uit4qsjGxgZ16tTRufU2/z2ACv7yfuutt5CXl4fvvvtOZ/miRYugUCikawf8/PwAAMuXL9epW7Zsmd7zzP8XX8F/kS5evFjvMV7EW2+9VeT28v+F/aw75l5km8eOHUNsbKy07NGjR1i5ciVq1aolHdl42ocffoilS5ciNDQUn376qbS8f//+yMvLw8yZMws9Jzc31yDvuJyXl4f33nsPOTk5+O2334p8H6Y+ffrA1NQU06dPL/S9FEJIr8cWLVqgWrVqCA0NRU5OjlQTFham11xr1qyJYcOGYffu3UW+z1BoaCj27t2LgIAA1KhRAwCgVqtRtWrVQrfHF3zdAsX/TOSLjY3Vua7t+vXr2Lp1K3x9faXXcu3atXH//n3pSBUA3Lp1S7pjseD2DPmu2P3790dKSgr+7//+r9C6rKwsPHr0qETjmZqawsfHB1u2bMHNmzel5X///bd0LeHTXnR/7ty5U2hZ/pHAgm8bQOWHR4jopdagQQN07NgRXl5eqFy5Mo4fP45ff/1V571b8i8OHTt2LPz8/GBqaor33nsPPXr0QKdOnfDFF1/g6tWr8PT0RGRkJLZu3YqgoCDpIkwvLy/07dsXixcvxr///ivddn/p0iUA+v1rUa1Wo0OHDpg/fz40Gg1ee+01REZGIikpqQy6UpinpyeGDBmClStX4t69e3jzzTdx7NgxrFu3Dr169UKnTp0Mvs3PPvsM4eHh6N69O8aOHYvKlStj3bp1SEpKwm+//VbstRJjxoxBRkYGvvjiC9jZ2eHzzz/Hm2++iY8++ghz5sxBfHw8fH19YW5ujsTERGzatAlLlixBv379Xmi++SFj1KhRhS7KdXR0RNeuXVG7dm3MmjULU6ZMwdWrV9GrVy/Y2toiKSkJmzdvxsiRIzFx4kSYm5tj1qxZ+Oijj9C5c2cMGDAASUlJWLt2rV7XEAFPgvnFixfx8ccfIyIiQjoStGvXLmzduhVvvvlmofegGjFiBObOnYsRI0agRYsWiImJkV6nT8v/mfjiiy/w3nvvwdzcHD169JCCUqNGjeDn56dz2z0ATJ8+XRrjvffew6efforevXtj7NixyMzMxIoVK1CvXr1CNwl4eXlh9+7dWLhwIZydneHm5lbk9WVP27NnDx4/flxoea9evTB48GBs3LhR+l61bdsWeXl5uHjxIjZu3Ihdu3YVeV3is0ybNg2RkZFo27YtRo8eLf1jqVGjRoiPj3/h/XnajBkzEBMTA39/f7i6uiI9PR3Lly9HjRo1dG5CoHJmpLvbiKTb7v/6668i17/55pvPve1+1qxZolWrVsLe3l6oVCpRv3598fXXX4ucnBypJjc3V3zyySeiWrVqQqFQ6NxC++DBAzF+/Hjh7OwszM3NRd26dcU333wjtFqtznYfPXokAgMDReXKlYWNjY3o1auXSEhIEAB0boPPv+22qNtpb9y4IXr37i3s7e2FnZ2dePfdd8XNmzeLvXW/4BjF3Q5fVJ+KotFoxPTp04Wbm5swNzcXNWvWFFOmTBGPHz/WazvPU/C2eyGEuHz5sujXr5+wt7cXlpaWolWrVmLbtm06NU/fdv+0yZMnCwDiu+++k5atXLlSeHl5CZVKJWxtbUXjxo3F5MmTxc2bN6UaV1fXIt+K4c0339SZX8Hb7vP7XtRXwf367bffRLt27YS1tbWwtrYW9evXF4GBgSIhIUGnbvny5cLNzU0olUrRokULERMTU2gez5KdnS0WLVokvLy8hLW1tbCyshLNmzcXixcv1nmN58vMzBQBAQHCzs5O2Nraiv79+4v09PRCrzEhhJg5c6Z47bXXhImJic4t+ABEYGCg+Omnn0TdunWFUqkUzZo1K/LtCSIjI0WjRo2EhYWFcHd3Fz/99FORt91fvHhRdOjQQahUKgHgmbfg5992X9zXjz/+KIR48tYG8+bNEw0bNhRKpVJUqlRJeHl5ienTp4v79+9L4+XvT0EFf5cIIcSePXtEs2bNhIWFhahdu7ZYtWqVmDBhgrC0tNRrf4r72c3/XZff4z179oiePXsKZ2dnYWFhIZydncX7778vLl26VGxfqOwphCijq8qIXnHx8fFo1qwZfvrpJwwcONDY0yEyCIVCgcDAwEKnkuWqV69epX5rD3q58BoiIj1kZWUVWrZ48WKYmJg89x2iiejlUPDnPDExETt27Cj00S30auI1RER6mD9/PuLi4tCpUyeYmZlh586d2LlzJ0aOHPncj2EgopfD66+/Ln3e4LVr17BixQpYWFhg8uTJxp4alQMGIiI9vPHGG4iKisLMmTPx8OFDuLi4YNq0afjiiy+MPTUiMpBu3bohPDwcqampUCqV8Pb2xuzZs4t9I056tfAaIiIiIpI9XkNEREREssdARERERLLHa4j0oNVqcfPmTdja2lboz4IiIiKi/xFC4MGDB3B2dn7uB+cyEOnh5s2bvJOIiIjoJXX9+nXpY26Kw0Ckh/wP/rx+/TrUarXBxtVoNIiMjJQ+hoCKx16VDPulP/ZKf+xVybBf+iurXmVkZKBmzZo6H+BdHAYiPeSfJlOr1QYPRFZWVlCr1fxheQ72qmTYL/2xV/pjr0qG/dJfWfdKn8tdeFE1ERERyR4DEREREckeAxERERHJHgMRERERyR4DEREREckeAxERERHJHgMRERERyR4DEREREckeAxERERHJHgMRERERyR4DEREREckeAxERERHJHgMRERERyR4DEREREcmembEnQPpJTk7G7du3n1lTtWpVuLi4lNOMiIiIXh0MRC+B5ORk1PfwQFZm5jPrVFZWuHjhAkMRERFRCRn1lNm0adOgUCh0vurXry+tf/z4MQIDA1GlShXY2Nigb9++SEtL0xkjOTkZ/v7+sLKygoODAyZNmoTc3Fydmn379qF58+ZQKpWoU6cOwsLCymP3DOb27dvIysxE/1krMGb97iK/+s9agazMzOceRSIiIqLCjH6EqGHDhti9e7f02Mzsf1MaP348tm/fjk2bNsHOzg5jxoxBnz59cOjQIQBAXl4e/P394eTkhMOHD+PWrVv48MMPYW5ujtmzZwMAkpKS4O/vj1GjRmH9+vXYs2cPRowYgerVq8PPz698d/YFObjVxWsensaeBhER0SvH6IHIzMwMTk5OhZbfv38fq1evxs8//4zOnTsDANauXQsPDw8cOXIEbdq0QWRkJM6fP4/du3fD0dERTZs2xcyZM/Hpp59i2rRpsLCwQGhoKNzc3LBgwQIAgIeHBw4ePIhFixa9dIGIiIiIyobRA1FiYiKcnZ1haWkJb29vzJkzBy4uLoiLi4NGo4GPj49UW79+fbi4uCA2NhZt2rRBbGwsGjduDEdHR6nGz88Po0ePxrlz59CsWTPExsbqjJFfExQUVOycsrOzkZ2dLT3OyMgAAGg0Gmg0GgPtOaSxnjemVquFSqWCKQRMtLlF1phCQKVSQavVGnSOFYW+vaIn2C/9sVf6Y69Khv3SX1n1qiTjGTUQtW7dGmFhYXB3d8etW7cwffp0tG/fHmfPnkVqaiosLCxgb2+v8xxHR0ekpqYCAFJTU3XCUP76/HXPqsnIyEBWVhZUKlWhec2ZMwfTp08vtDwyMhJWVlal3t/iREVFPbcmPDwcwCPgxtEi17tbA53Cw5GSkoKUlBQDz7Di0KdX9D/sl/7YK/2xVyXDfunP0L3KfM7NSE8zaiDq3r279P9NmjRB69at4erqio0bNxYZVMrLlClTEBwcLD3OyMhAzZo14evrC7VabbDtaDQaREVFoWvXrjA3Ny+27tSpU+jQoQNGrvoDzu6Niqy5mXAWK0e8g5iYGHh6vnrXGenbK3qC/dIfe6U/9qpk2C/9lVWv8s/w6MPop8yeZm9vj3r16uHvv/9G165dkZOTg3v37ukcJUpLS5OuOXJycsKxY8d0xsi/C+3pmoJ3pqWlpUGtVhcbupRKJZRKZaHl5ubmZfKift64JiYmyMrKQh4U0JoU/S3LgwJZWVkwMTF5pX/wyup78Kpiv/THXumPvSoZ9kt/hu5VScaqUO9U/fDhQ1y+fBnVq1eHl5cXzM3NsWfPHml9QkICkpOT4e3tDQDw9vbGmTNnkJ6eLtVERUVBrVajQYMGUs3TY+TX5I9BREREZNRANHHiROzfvx9Xr17F4cOH0bt3b5iamuL999+HnZ0dAgICEBwcjOjoaMTFxWHYsGHw9vZGmzZtAAC+vr5o0KABBg8ejFOnTmHXrl348ssvERgYKB3hGTVqFK5cuYLJkyfj4sWLWL58OTZu3Ijx48cbc9eJiIioAjHqKbMbN27g/fffx7///otq1aqhXbt2OHLkCKpVqwYAWLRoEUxMTNC3b19kZ2fDz88Py5cvl55vamqKbdu2YfTo0fD29oa1tTWGDBmCGTNmSDVubm7Yvn07xo8fjyVLlqBGjRpYtWoVb7knIiIiiVED0YYNG5653tLSEiEhIQgJCSm2xtXVFTt27HjmOB07dsTJkydLNUciIiJ69VWoa4iIiIiIjIGBiIiIiGSPgYiIiIhkj4GIiIiIZI+BiIiIiGSPgYiIiIhkj4GIiIiIZI+BiIiIiGSPgYiIiIhkj4GIiIiIZI+BiIiIiGSPgYiIiIhkj4GIiIiIZI+BiIiIiGSPgYiIiIhkj4GIiIiIZI+BiIiIiGSPgYiIiIhkj4GIiIiIZI+BiIiIiGSPgYiIiIhkj4GIiIiIZI+BiIiIiGSPgYiIiIhkj4GIiIiIZI+BiIiIiGSPgYiIiIhkj4GIiIiIZI+BiIiIiGSPgYiIiIhkj4GIiIiIZI+BiIiIiGSPgYiIiIhkj4GIiIiIZI+BiIiIiGSPgYiIiIhkj4GIiIiIZI+BiIiIiGSPgYiIiIhkj4GIiIiIZI+BiIiIiGSPgYiIiIhkj4GIiIiIZI+BiIiIiGSPgYiIiIhkj4GIiIiIZI+BiIiIiGSPgYiIiIhkj4GIiIiIZI+BiIiIiGSPgYiIiIhkj4GIiIiIZI+BiIiIiGSPgYiIiIhkj4GIiIiIZI+BiIiIiGSPgYiIiIhkj4GIiIiIZK/CBKK5c+dCoVAgKChIWvb48WMEBgaiSpUqsLGxQd++fZGWlqbzvOTkZPj7+8PKygoODg6YNGkScnNzdWr27duH5s2bQ6lUok6dOggLCyuHPSIiIqKXRYUIRH/99Re+//57NGnSRGf5+PHj8eeff2LTpk3Yv38/bt68iT59+kjr8/Ly4O/vj5ycHBw+fBjr1q1DWFgYvvrqK6kmKSkJ/v7+6NSpE+Lj4xEUFIQRI0Zg165d5bZ/REREVLEZPRA9fPgQAwcOxP/93/+hUqVK0vL79+9j9erVWLhwITp37gwvLy+sXbsWhw8fxpEjRwAAkZGROH/+PH766Sc0bdoU3bt3x8yZMxESEoKcnBwAQGhoKNzc3LBgwQJ4eHhgzJgx6NevHxYtWmSU/SUiIqKKx+iBKDAwEP7+/vDx8dFZHhcXB41Go7O8fv36cHFxQWxsLAAgNjYWjRs3hqOjo1Tj5+eHjIwMnDt3TqopOLafn580BhEREZGZMTe+YcMGnDhxAn/99VehdampqbCwsIC9vb3OckdHR6Smpko1T4eh/PX5655Vk5GRgaysLKhUqkLbzs7ORnZ2tvQ4IyMDAKDRaKDRaEq4l8XLH+t5Y2q1WqhUKphCwESbW2SNKQRUKhW0Wq1B51hR6NsreoL90h97pT/2qmTYL/2VVa9KMp7RAtH169cxbtw4REVFwdLS0ljTKNKcOXMwffr0QssjIyNhZWVl8O1FRUU9tyY8PBzAI+DG0SLXu1sDncLDkZKSgpSUFAPPsOLQp1f0P+yX/tgr/bFXJcN+6c/QvcrMzNS71miBKC4uDunp6WjevLm0LC8vDzExMfjuu++wa9cu5OTk4N69ezpHidLS0uDk5AQAcHJywrFjx3TGzb8L7emagnempaWlQa1WF3l0CACmTJmC4OBg6XFGRgZq1qwJX19fqNXq0u90ARqNBlFRUejatSvMzc2LrTt16hQ6dOiAkav+gLN7oyJrbiacxcoR7yAmJgaenp4Gm2NFoW+v6An2S3/slf7Yq5Jhv/RXVr3KP8OjD6MFoi5duuDMmTM6y4YNG4b69evj008/Rc2aNWFubo49e/agb9++AICEhAQkJyfD29sbAODt7Y2vv/4a6enpcHBwAPAkXarVajRo0ECq2bFjh852oqKipDGKolQqoVQqCy03Nzcvkxf188Y1MTFBVlYW8qCA1qTob1keFMjKyoKJickr/YNXVt+DVxX7pT/2Sn/sVcmwX/ozdK9KMpbRApGtrS0aNdI92mFtbY0qVapIywMCAhAcHIzKlStDrVbjk08+gbe3N9q0aQMA8PX1RYMGDTB48GDMnz8fqamp+PLLLxEYGCgFmlGjRuG7777D5MmTMXz4cOzduxcbN27E9u3by3eHiYiIqMIy6kXVz7No0SKYmJigb9++yM7Ohp+fH5YvXy6tNzU1xbZt2zB69Gh4e3vD2toaQ4YMwYwZM6QaNzc3bN++HePHj8eSJUtQo0YNrFq1Cn5+fsbYJSIiIqqAKlQg2rdvn85jS0tLhISEICQkpNjnuLq6FjolVlDHjh1x8uRJQ0yRiIiIXkFGfx8iIiIiImNjICIiIiLZYyAiIiIi2WMgIiIiItljICIiIiLZYyAiIiIi2WMgIiIiItljICIiIiLZYyAiIiIi2WMgIiIiItljICIiIiLZYyAiIiIi2WMgIiIiItljICIiIiLZYyAiIiIi2WMgIiIiItljICIiIiLZYyAiIiIi2WMgIiIiItljICIiIiLZYyAiIiIi2WMgIiIiItljICIiIiLZYyAiIiIi2WMgIiIiItljICIiIiLZYyAiIiIi2WMgIiIiItljICIiIiLZYyAiIiIi2WMgIiIiItljICIiIiLZYyAiIiIi2WMgIiIiItljICIiIiLZYyAiIiIi2WMgIiIiItljICIiIiLZYyAiIiIi2WMgIiIiItljICIiIiLZYyAiIiIi2WMgIiIiItljICIiIiLZYyAiIiIi2WMgIiIiItljICIiIiLZYyAiIiIi2WMgIiIiItljICIiIiLZYyAiIiIi2WMgIiIiItljICIiIiLZK1UgunLliqHnQURERGQ0pQpEderUQadOnfDTTz/h8ePHhp4TERERUbkqVSA6ceIEmjRpguDgYDg5OeGjjz7CsWPHDD03IiIionJhVponNW3aFEuWLMGCBQvwxx9/ICwsDO3atUO9evUwfPhwDB48GNWqVTP0XF9ZN27cwN27d4tdf+HChXKcDRERkfyUKhBJTzYzQ58+feDv74/ly5djypQpmDhxIj7//HP0798f8+bNQ/Xq1Q0111dWi5Ytcefff409DSIiItl6oUB0/PhxrFmzBhs2bIC1tTUmTpyIgIAA3LhxA9OnT0fPnj15Kk0PWZmZ6D9rBRzc6ha5PuHQHkQtn1POsyIiIpKPUl1DtHDhQjRu3BhvvPEGbt68iR9++AHXrl3DrFmz4Obmhvbt2yMsLAwnTpx45jgrVqxAkyZNoFaroVar4e3tjZ07d0rrHz9+jMDAQFSpUgU2Njbo27cv0tLSdMZITk6Gv78/rKys4ODggEmTJiE3N1enZt++fWjevDmUSiXq1KmDsLCw0ux2mXJwq4vXPDyL/Krk7GLs6REREb3SShWIVqxYgQ8++ADXrl3Dli1b8Pbbb8PERHcoBwcHrF69+pnj1KhRA3PnzkVcXByOHz+Ozp07o2fPnjh37hwAYPz48fjzzz+xadMm7N+/Hzdv3kSfPn2k5+fl5cHf3x85OTk4fPgw1q1bh7CwMHz11VdSTVJSEvz9/dGpUyfEx8cjKCgII0aMwK5du0qz60RERPQKKtUps8TExOfWWFhYYMiQIc+s6dGjh87jr7/+GitWrMCRI0dQo0YNrF69Gj///DM6d+4MAFi7di08PDxw5MgRtGnTBpGRkTh//jx2794NR0dHNG3aFDNnzsSnn36KadOmwcLCAqGhoXBzc8OCBQsAAB4eHjh48CAWLVoEPz+/0uw+ERERvWJKFYjWrl0LGxsbvPvuuzrLN23ahMzMzOcGoaLk5eVh06ZNePToEby9vREXFweNRgMfHx+ppn79+nBxcUFsbCzatGmD2NhYNG7cGI6OjlKNn58fRo8ejXPnzqFZs2aIjY3VGSO/JigoqNi5ZGdnIzs7W3qckZEBANBoNNBoNCXet+Lkj6VSqWAKARNtbpF1ZiaK59aYQkClUkGr1Rp0jhVF/j69ivtWFtgv/bFX+mOvSob90l9Z9aok45UqEM2ZMwfff/99oeUODg4YOXJkiQLRmTNn4O3tjcePH8PGxgabN29GgwYNEB8fDwsLC9jb2+vUOzo6IjU1FQCQmpqqE4by1+eve1ZNRkYGsrKyoFKpity/6dOnF1oeGRkJKysrvfdNX2vWrAHwCLhxtMj17g2c0D88/Nk11kCn8HCkpKQgJSXF4HOsKKKioow9hZcK+6U/9kp/7FXJsF/6M3SvMjMz9a4tVSBKTk6Gm5tboeWurq5ITk4u0Vju7u6Ij4/H/fv38euvv2LIkCHYv39/aaZlMFOmTEFwcLD0OCMjAzVr1oSvry/UarXBtqPRaBAVFfXkvZuW/QJn90ZF1p2K3IrNM8dj5Ko/iq25mXAWK0e8g5iYGHh6ehpsjhVFfq+6du0Kc3NzY0+nwmO/9Mde6Y+9Khn2S39l1av8Mzz6KFUgcnBwwOnTp1GrVi2d5adOnUKVKlVKNJaFhQXq1KkDAPDy8sJff/2FJUuWYMCAAcjJycG9e/d0jhKlpaXByckJAODk5FTotv78u9Ceril4Z1paWhrUanWRR4cAQKlUQqlUFlpubm5eJi/qrKws5EEBrUnR345crXhuTR4UyMrKgomJySv9g1dW34NXFfulP/ZKf+xVybBf+jN0r0oyVqnuMnv//fcxduxYREdHIy8vD3l5edi7dy/GjRuH9957rzRDSrRaLbKzs+Hl5QVzc3Ps2bNHWpeQkIDk5GR4e3sDALy9vXHmzBmkp6dLNVFRUVCr1WjQoIFU8/QY+TX5YxARERGV6gjRzJkzcfXqVXTp0gVmZk+G0Gq1+PDDDzF79my9x5kyZQq6d+8OFxcXPHjwAD///DP27duHXbt2wc7ODgEBAQgODkblypWhVqvxySefwNvbG23atAEA+Pr6okGDBhg8eDDmz5+P1NRUfPnllwgMDJSO8IwaNQrfffcdJk+ejOHDh2Pv3r3YuHEjtm/fXppdJyIioldQqQKRhYUFfvnlF8ycOROnTp2CSqVC48aN4erqWqJx0tPT8eGHH+LWrVuws7NDkyZNsGvXLnTt2hUAsGjRIpiYmKBv377Izs6Gn58fli9fLj3f1NQU27Ztw+jRo+Ht7Q1ra2sMGTIEM2bMkGrc3Nywfft2jB8/HkuWLEGNGjWwatUq3nJPREREkhf66I569eqhXr16pX7+89640dLSEiEhIQgJCSm2xtXVFTt27HjmOB07dsTJkydLNUciIiJ69ZUqEOXl5SEsLAx79uxBeno6tFqtzvq9e/caZHJERERE5aFUgWjcuHEICwuDv78/GjVqBIVCYeh5EREREZWbUgWiDRs2YOPGjXjrrbcMPR8iIiKicleq2+6ffu8gIiIiopddqQLRhAkTsGTJEgghDD0fIiIionJXqlNmBw8eRHR0NHbu3ImGDRsWeifI33//3SCTIyIiIioPpQpE9vb26N27t6HnQkRERGQUpQpEa9euNfQ8iIiIiIymVNcQAUBubi52796N77//Hg8ePAAA3Lx5Ew8fPjTY5IiIiIjKQ6mOEF27dg3dunVDcnIysrOz0bVrV9ja2mLevHnIzs5GaGiooedJREREVGZKdYRo3LhxaNGiBe7evQuVSiUt7927d6FPliciIiKq6Ep1hOjAgQM4fPgwLCwsdJbXqlULKSkpBpkYERERUXkp1REirVaLvLy8Qstv3LgBW1vbF54UERERUXkqVSDy9fXF4sWLpccKhQIPHz7E1KlT+XEeRERE9NIp1SmzBQsWwM/PDw0aNMDjx4/xwQcfIDExEVWrVkV4eLih50hERERUpkoViGrUqIFTp05hw4YNOH36NB4+fIiAgAAMHDhQ5yJrIiIiopdBqQIRAJiZmWHQoEGGnAsRERGRUZQqEP3www/PXP/hhx+WajJERERExlCqQDRu3DidxxqNBpmZmbCwsICVlRUDEREREb1USnWX2d27d3W+Hj58iISEBLRr144XVRMREdFLp9SfZVZQ3bp1MXfu3EJHj4iIiIgqOoMFIuDJhdY3b9405JBEREREZa5U1xD98ccfOo+FELh16xa+++47tG3b1iATIyIiIiovpQpEvXr10nmsUChQrVo1dO7cGQsWLDDEvIiIiIjKTakCkVarNfQ8iIiIiIzGoNcQEREREb2MSnWEKDg4WO/ahQsXlmYTREREROWmVIHo5MmTOHnyJDQaDdzd3QEAly5dgqmpKZo3by7VKRQKw8ySiIiIqAyVKhD16NEDtra2WLduHSpVqgTgyZs1Dhs2DO3bt8eECRMMOkkiIiKislSqa4gWLFiAOXPmSGEIACpVqoRZs2bxLjMiIiJ66ZQqEGVkZOCff/4ptPyff/7BgwcPXnhSREREROWpVIGod+/eGDZsGH7//XfcuHEDN27cwG+//YaAgAD06dPH0HMkIiIiKlOluoYoNDQUEydOxAcffACNRvNkIDMzBAQE4JtvvjHoBImIiIjKWqkCkZWVFZYvX45vvvkGly9fBgDUrl0b1tbWBp0cERERUXl4oTdmvHXrFm7duoW6devC2toaQghDzYuIiIio3JQqEP3777/o0qUL6tWrh7feegu3bt0CAAQEBPCWeyIiInrplCoQjR8/Hubm5khOToaVlZW0fMCAAYiIiDDY5IiIiIjKQ6muIYqMjMSuXbtQo0YNneV169bFtWvXDDIxIiIiovJSqiNEjx490jkylO/OnTtQKpUvPCkiIiKi8lSqQNS+fXv88MMP0mOFQgGtVov58+ejU6dOBpscERERUXko1Smz+fPno0uXLjh+/DhycnIwefJknDt3Dnfu3MGhQ4cMPUciIiKiMlWqI0SNGjXCpUuX0K5dO/Ts2ROPHj1Cnz59cPLkSdSuXdvQcyQiIiIqUyU+QqTRaNCtWzeEhobiiy++KIs5EREREZWrEh8hMjc3x+nTp8tiLkRERERGUapTZoMGDcLq1asNPRciIiIioyjVRdW5ublYs2YNdu/eDS8vr0KfYbZw4UKDTI6IiIioPJQoEF25cgW1atXC2bNn0bx5cwDApUuXdGoUCoXhZkdERERUDkoUiOrWrYtbt24hOjoawJOP6li6dCkcHR3LZHJERERE5aFE1xAV/DT7nTt34tGjRwadEBEREVF5K9VF1fkKBiQiIiKil1GJApFCoSh0jRCvGSIiIqKXXYmuIRJCYOjQodIHuD5+/BijRo0qdJfZ77//brgZEhEREZWxEgWiIUOG6DweNGiQQSdDREREZAwlCkRr164tq3kQERERGc0LXVRNRERE9CpgICIiIiLZYyAiIiIi2WMgIiIiItkzaiCaM2cOWrZsCVtbWzg4OKBXr15ISEjQqXn8+DECAwNRpUoV2NjYoG/fvkhLS9OpSU5Ohr+/P6ysrODg4IBJkyYhNzdXp2bfvn1o3rw5lEol6tSpg7CwsLLePSIiInpJGDUQ7d+/H4GBgThy5AiioqKg0Wjg6+ur83Eg48ePx59//olNmzZh//79uHnzJvr06SOtz8vLg7+/P3JycnD48GGsW7cOYWFh+Oqrr6SapKQk+Pv7o1OnToiPj0dQUBBGjBiBXbt2lev+EhERUcVUotvuDS0iIkLncVhYGBwcHBAXF4cOHTrg/v37WL16NX7++Wd07twZwJNb/z08PHDkyBG0adMGkZGROH/+PHbv3g1HR0c0bdoUM2fOxKeffopp06bBwsICoaGhcHNzw4IFCwAAHh4eOHjwIBYtWgQ/P79y328iIiKqWIwaiAq6f/8+AKBy5coAgLi4OGg0Gvj4+Eg19evXh4uLC2JjY9GmTRvExsaicePGcHR0lGr8/PwwevRonDt3Ds2aNUNsbKzOGPk1QUFBRc4jOzsb2dnZ0uOMjAwAgEajgUajMci+5o8HACqVCqYQMNHmFllnZqJ4bo0pBFQqFbRarUHnWFHk79OruG9lgf3SH3ulP/aqZNgv/ZVVr0oyXoUJRFqtFkFBQWjbti0aNWoEAEhNTYWFhQXs7e11ah0dHZGamirVPB2G8tfnr3tWTUZGBrKysqBSqXTWzZkzB9OnTy80x8jISFhZWZV+J4uxZs0aAI+AG0eLXO/ewAn9w8OfXWMNdAoPR0pKClJSUgw+x4oiKirK2FN4qbBf+mOv9MdelQz7pT9D9yozM1Pv2goTiAIDA3H27FkcPHjQ2FPBlClTEBwcLD3OyMhAzZo14evrC7VabbDtaDQaREVFYfjw4Ri87Bc4uzcqsu5U5FZsnjkeI1f9UWzNzYSzWDniHcTExMDT09Ngc6wo8nvVtWtXmJubG3s6FR77pT/2Sn/sVcmwX/orq17ln+HRR4UIRGPGjMG2bdsQExODGjVqSMudnJyQk5ODe/fu6RwlSktLg5OTk1Rz7NgxnfHy70J7uqbgnWlpaWlQq9WFjg4BgFKplD7A9mnm5uZl8qLOyspCHhTQmhT97cjViufW5EGBrKwsmJiYvNI/eGX1PXhVsV/6Y6/0x16VDPulP0P3qiRjGfUuMyEExowZg82bN2Pv3r1wc3PTWe/l5QVzc3Ps2bNHWpaQkIDk5GR4e3sDALy9vXHmzBmkp6dLNVFRUVCr1WjQoIFU8/QY+TX5YxAREZG8GfUIUWBgIH7++Wds3boVtra20jU/dnZ2UKlUsLOzQ0BAAIKDg1G5cmWo1Wp88skn8Pb2Rps2bQAAvr6+aNCgAQYPHoz58+cjNTUVX375JQIDA6WjPKNGjcJ3332HyZMnY/jw4di7dy82btyI7du3G23fiYiIqOIw6hGiFStW4P79++jYsSOqV68uff3yyy9SzaJFi/D222+jb9++6NChA5ycnPD7779L601NTbFt2zaYmprC29sbgwYNwocffogZM2ZINW5ubti+fTuioqLg6emJBQsWYNWqVbzlnoiIiAAY+QiREOK5NZaWlggJCUFISEixNa6urtixY8czx+nYsSNOnjxZ4jkSERHRq4+fZUZERESyx0BEREREssdARERERLLHQERERESyx0BEREREssdARERERLLHQERERESyx0BEREREssdARERERLLHQERERESyx0BEREREssdARERERLLHQERERESyx0BEREREssdARERERLLHQERERESyx0BEREREssdARERERLLHQERERESyx0BEREREssdARERERLLHQERERESyx0BEREREssdARERERLLHQERERESyx0BEREREssdARERERLLHQERERESyx0BEREREssdARERERLLHQERERESyx0BEREREssdARERERLLHQERERESyx0BEREREssdARERERLLHQERERESyx0BEREREssdARERERLLHQERERESyx0BEREREssdARERERLLHQERERESyx0BEREREsmdm7AmQYV24cOGZ66tWrQoXF5dymg0REdHLgYHoFfHgdhoUJiYYNGjQM+tUVla4eOECQxEREdFTGIheEVkPMiC0WvSftQIObnWLrElPSsTGL0fj9u3bDERERERPYSB6xTi41cVrHp7GngYREdFLhRdVExERkewxEBEREZHsMRARERGR7DEQERERkewxEBEREZHsMRARERGR7DEQERERkewxEBEREZHsMRARERGR7DEQERERkewZNRDFxMSgR48ecHZ2hkKhwJYtW3TWCyHw1VdfoXr16lCpVPDx8UFiYqJOzZ07dzBw4ECo1WrY29sjICAADx8+1Kk5ffo02rdvD0tLS9SsWRPz588v610jIiKil4hRA9GjR4/g6emJkJCQItfPnz8fS5cuRWhoKI4ePQpra2v4+fnh8ePHUs3AgQNx7tw5REVFYdu2bYiJicHIkSOl9RkZGfD19YWrqyvi4uLwzTffYNq0aVi5cmWZ7x8RERG9HIz64a7du3dH9+7di1wnhMDixYvx5ZdfomfPngCAH374AY6OjtiyZQvee+89XLhwAREREfjrr7/QokULAMCyZcvw1ltv4dtvv4WzszPWr1+PnJwcrFmzBhYWFmjYsCHi4+OxcOFCneBERERE8lVhP+0+KSkJqamp8PHxkZbZ2dmhdevWiI2NxXvvvYfY2FjY29tLYQgAfHx8YGJigqNHj6J3796IjY1Fhw4dYGFhIdX4+flh3rx5uHv3LipVqlRo29nZ2cjOzpYeZ2RkAAA0Gg00Go3B9jF/LJVKBVMImGhzi6wzM1EYpMYUAiqVClqt1qD7UR7y5/uyzdtY2C/9sVf6Y69Khv3SX1n1qiTjVdhAlJqaCgBwdHTUWe7o6CitS01NhYODg856MzMzVK5cWafGzc2t0Bj564oKRHPmzMH06dMLLY+MjISVlVUp96h4a9asAfAIuHG0yPXuDZzQPzz8xWusgU7h4UhJSUFKSophJl/OoqKijD2Flwr7pT/2Sn/sVcmwX/ozdK8yMzP1rq2wgciYpkyZguDgYOlxRkYGatasCV9fX6jVaoNtR6PRICoqCsOHD8fgZb/A2b1RkXWnIrdi88zxGLnqjxequZlwFitHvIOYmBh4enoabD/KQ36vunbtCnNzc2NPp8Jjv/THXumPvSoZ9kt/ZdWr/DM8+qiwgcjJyQkAkJaWhurVq0vL09LS0LRpU6kmPT1d53m5ubm4c+eO9HwnJyekpaXp1OQ/zq8pSKlUQqlUFlpubm5eJi/qrKws5EEBrUnR345crTBITR4UyMrKgomJyUv7w1lW34NXFfulP/ZKf+xVybBf+jN0r0oyVoV9HyI3Nzc4OTlhz5490rKMjAwcPXoU3t7eAABvb2/cu3cPcXFxUs3evXuh1WrRunVrqSYmJkbnPGJUVBTc3d2LPF1GRERE8mPUQPTw4UPEx8cjPj4ewJMLqePj45GcnAyFQoGgoCDMmjULf/zxB86cOYMPP/wQzs7O6NWrFwDAw8MD3bp1w3/+8x8cO3YMhw4dwpgxY/Dee+/B2dkZAPDBBx/AwsICAQEBOHfuHH755RcsWbJE55QYERERyZtRT5kdP34cnTp1kh7nh5QhQ4YgLCwMkydPxqNHjzBy5Ejcu3cP7dq1Q0REBCwtLaXnrF+/HmPGjEGXLl1gYmKCvn37YunSpdJ6Ozs7REZGIjAwEF5eXqhatSq++uor3nJPREREEqMGoo4dO0IIUex6hUKBGTNmYMaMGcXWVK5cGT///PMzt9OkSRMcOHCg1PMkIiKiV1uFvYaIiIiIqLwwEBEREZHsMRARERGR7DEQERERkewxEBEREZHsMRARERGR7DEQERERkewxEBEREZHsMRARERGR7DEQERERkewxEBEREZHsMRARERGR7DEQERERkewxEBEREZHsMRARERGR7DEQERERkewxEBEREZHsMRARERGR7DEQERERkewxEBEREZHsMRARERGR7DEQERERkewxEBEREZHsMRARERGR7DEQERERkewxEBEREZHsMRARERGR7DEQERERkewxEBEREZHsMRARERGR7DEQERERkewxEBEREZHsMRARERGR7DEQERERkewxEBEREZHsMRARERGR7DEQERERkewxEBEREZHsMRARERGR7DEQERERkewxEBEREZHsMRARERGR7DEQERERkewxEBEREZHsMRARERGR7DEQERERkewxEBEREZHsMRARERGR7JkZewJU/i5cuPDM9VWrVoWLi0s5zYaIiMj4GIhk5MHtNChMTDBo0KBn1qmsrHDxwgWGIiIikg0GIhnJepABodWi/6wVcHCrW2RNelIiNn45Grdv32YgIiIi2WAgkiEHt7p4zcPT2NMgIiKqMHhRNREREckeAxERERHJHgMRERERyR6vIaIyk5ycjNu3bz+zhrf4ExFRRcBAREV63nsVZWdnQ6lUFrv+1q1b6Pfuu3iclfXMcXiLPxERVQQMRKRD3/cqUpiYQGi1zx2Pt/hXDDxaR0T0bLIKRCEhIfjmm2+QmpoKT09PLFu2DK1atTL2tCoUfd6rKOHQHkQtn6NXjT63+OvzztnVq1fXbweokOTkZNT38EBWZuYz63i0jojkTDaB6JdffkFwcDBCQ0PRunVrLF68GH5+fkhISICDg4Oxp1fhPCvIpCcl6l3zLCV55+xzZ88+d7zyos/RluedUtS3Rp+jNkXNR/v/j96dOnUKCQkJyMrM1Oto3YEDB+Dh4WGUOZdmHCIiQ5FNIFq4cCH+85//YNiwYQCA0NBQbN++HWvWrMFnn31m5NnJU0neOfvw4cOwsbHBqVOnYGJS+ObI8vpDre+1UfqcUtSnRmlpid9+/bXYI2TFzUelUiE8PBwdOnRA1v9f96wAa8hTpaWdc0nHAQz3fScikkUgysnJQVxcHKZMmSItMzExgY+PD2JjY404MwL0+0P9n//8p9Af+KeV5x9q4NnXRpXklOKzapJOHsWOhf/F22+/XeL5mEIAeISRq/7A+UN7EbV8zjOfb6hTpS8y59KMY4jv+9NH04oK2/kMddSvPI8eEpH+ZBGIbt++jby8PDg6Ouosd3R0xMWLFwvVZ2dnIzs7W3p8//59AMCdO3eg0WgMNi+NRoPMzExYWloiLeEMcjMfFll39/oV2dbcPB8PpYUFOgwahczMTPQImopcIXRqblw8jdMRm9H2g5Gwcyj6j17alUSc2LYB/fr1K3L907oMH1vsOPnbEjmPi50z8nJhaWn5wjU5D+5BaWHxzP0qbj4CApmKLORmCmlb+nwvjDnn0o7zot93lUqFkJAQ+Pr6Fhm28xnqqJ+haixVKnwfGvrcU/4mJiZS6HvRmtzcXGRmZuLAgQNFhkdDbutVqNFqtTr9MvZ8KnJNfq/+/fdfmJubP3Osknjw4AEAQBT4u1EkIQMpKSkCgDh8+LDO8kmTJolWrVoVqp86daoAwC9+8Ytf/OIXv16Br+vXrz83K8jiCFHVqlVhamqKtLQ0neVpaWlwcnIqVD9lyhQEBwdLj7VaLe7cuYMqVapAoVAYbF4ZGRmoWbMmrl+/DrVabbBxX0XsVcmwX/pjr/THXpUM+6W/suqVEAIPHjyAs7Pzc2tlEYgsLCzg5eWFPXv2oFevXgCehJw9e/ZgzJgxheqVSmWh8/f29vZlNj+1Ws0fFj2xVyXDfumPvdIfe1Uy7Jf+yqJXdnZ2etXJIhABQHBwMIYMGYIWLVqgVatWWLx4MR49eiTddUZERETyJZtANGDAAPzzzz/46quvkJqaiqZNmyIiIqLQhdZEREQkP7IJRAAwZsyYIk+RGYtSqcTUqVOfe3stsVclxX7pj73SH3tVMuyX/ipCrxRC6HMvGhEREdGrq/h3ISMiIiKSCQYiIiIikj0GIiIiIpI9BiIiIiKSPQYiIwoJCUGtWrVgaWmJ1q1b49ixY8aeUpmKiYlBjx494OzsDIVCgS1btuisF0Lgq6++QvXq1aFSqeDj44PExESdmjt37mDgwIFQq9Wwt7dHQEAAHj7U/Tys06dPo3379rC0tETNmjUxf/78st41g5szZw5atmwJW1tbODg4oFevXkhISNCpefz4MQIDA1GlShXY2Nigb9++hd6NPTk5Gf7+/rCysoKDgwMmTZqE3NxcnZp9+/ahefPmUCqVqFOnDsLCwsp69wxuxYoVaNKkifSmbt7e3ti5c6e0nr0q3ty5c6FQKBAUFCQtY7+emDZtGhQKhc5X/fr1pfXsk66UlBQMGjQIVapUgUqlQuPGjXH8+HFpfYX/HW+IzwqjktuwYYOwsLAQa9asEefOnRP/+c9/hL29vUhLSzP21MrMjh07xBdffCF+//13AUBs3rxZZ/3cuXOFnZ2d2LJlizh16pR45513hJubm8jKypJqunXrJjw9PcWRI0fEgQMHRJ06dcT7778vrb9//75wdHQUAwcOFGfPnhXh4eFCpVKJ77//vrx20yD8/PzE2rVrxdmzZ0V8fLx46623hIuLi3j48KFUM2rUKFGzZk2xZ88ecfz4cdGmTRvxxhtvSOtzc3NFo0aNhI+Pjzh58qTYsWOHqFq1qpgyZYpUc+XKFWFlZSWCg4PF+fPnxbJly4SpqamIiIgo1/19UX/88YfYvn27uHTpkkhISBCff/65MDc3F2fPnhVCsFfFOXbsmKhVq5Zo0qSJGDdunLSc/Xpi6tSpomHDhuLWrVvS1z///COtZ5/+586dO8LV1VUMHTpUHD16VFy5ckXs2rVL/P3331JNRf8dz0BkJK1atRKBgYHS47y8POHs7CzmzJljxFmVn4KBSKvVCicnJ/HNN99Iy+7duyeUSqUIDw8XQghx/vx5AUD89ddfUs3OnTuFQqEQKSkpQgghli9fLipVqiSys7Olmk8//VS4u7uX8R6VrfT0dAFA7N+/XwjxpDfm5uZi06ZNUs2FCxcEABEbGyuEeBJATUxMRGpqqlSzYsUKoVarpf5MnjxZNGzYUGdbAwYMEH5+fmW9S2WuUqVKYtWqVexVMR48eCDq1q0roqKixJtvvikFIvbrf6ZOnSo8PT2LXMc+6fr0009Fu3btil3/MvyO5ykzI8jJyUFcXBx8fHykZSYmJvDx8UFsbKwRZ2Y8SUlJSE1N1emJnZ0dWrduLfUkNjYW9vb2aNGihVTj4+MDExMTHD16VKrp0KEDLCwspBo/Pz8kJCTg7t275bQ3hnf//n0AQOXKlQEAcXFx0Gg0Ov2qX78+XFxcdPrVuHFjnXdj9/PzQ0ZGBs6dOyfVPD1Gfs3L/DrMy8vDhg0b8OjRI3h7e7NXxQgMDIS/v3+hfWK/dCUmJsLZ2Rmvv/46Bg4ciOTkZADsU0F//PEHWrRogXfffRcODg5o1qwZ/u///k9a/zL8jmcgMoLbt28jLy+v0MeGODo6IjU11UizMq78/X5WT1JTU+Hg4KCz3szMDJUrV9apKWqMp7fxstFqtQgKCkLbtm3RqFEjAE/2xcLCotCHDhfs1/N6UVxNRkYGsrKyymJ3ysyZM2dgY2MDpVKJUaNGYfPmzWjQoAF7VYQNGzbgxIkTmDNnTqF17Nf/tG7dGmFhYYiIiMCKFSuQlJSE9u3b48GDB+xTAVeuXMGKFStQt25d7Nq1C6NHj8bYsWOxbt06AC/H73hZfXQH0csoMDAQZ8+excGDB409lQrN3d0d8fHxuH//Pn799VcMGTIE+/fvN/a0Kpzr169j3LhxiIqKgqWlpbGnU6F1795d+v8mTZqgdevWcHV1xcaNG6FSqYw4s4pHq9WiRYsWmD17NgCgWbNmOHv2LEJDQzFkyBAjz04/PEJkBFWrVoWpqWmhuxHS0tLg5ORkpFkZV/5+P6snTk5OSE9P11mfm5uLO3fu6NQUNcbT23iZjBkzBtu2bUN0dDRq1KghLXdyckJOTg7u3bunU1+wX8/rRXE1arX6pfuFb2FhgTp16sDLywtz5syBp6cnlixZwl4VEBcXh/T0dDRv3hxmZmYwMzPD/v37sXTpUpiZmcHR0ZH9Koa9vT3q1auHv//+m6+rAqpXr44GDRroLPPw8JBOMb4Mv+MZiIzAwsICXl5e2LNnj7RMq9Viz5498Pb2NuLMjMfNzQ1OTk46PcnIyMDRo0elnnh7e+PevXuIi4uTavbu3QutVovWrVtLNTExMdBoNFJNVFQU3N3dUalSpXLamxcnhMCYMWOwefNm7N27F25ubjrrvby8YG5urtOvhIQEJCcn6/TrzJkzOr9goqKioFarpV9c3t7eOmPk17wKr0OtVovs7Gz2qoAuXbrgzJkziI+Pl75atGiBgQMHSv/PfhXt4cOHuHz5MqpXr87XVQFt27Yt9NYgly5dgqurK4CX5Hf8C1+WTaWyYcMGoVQqRVhYmDh//rwYOXKksLe317kb4VXz4MEDcfLkSXHy5EkBQCxcuFCcPHlSXLt2TQjx5JZMe3t7sXXrVnH69GnRs2fPIm/JbNasmTh69Kg4ePCgqFu3rs4tmffu3ROOjo5i8ODB4uzZs2LDhg3CysrqpbvtfvTo0cLOzk7s27dP55bfzMxMqWbUqFHCxcVF7N27Vxw/flx4e3sLb29vaX3+Lb++vr4iPj5eREREiGrVqhV5y++kSZPEhQsXREhIyEt5y+9nn30m9u/fL5KSksTp06fFZ599JhQKhYiMjBRCsFfP8/RdZkKwX/kmTJgg9u3bJ5KSksShQ4eEj4+PqFq1qkhPTxdCsE9PO3bsmDAzMxNff/21SExMFOvXrxdWVlbip59+kmoq+u94BiIjWrZsmXBxcREWFhaiVatW4siRI8aeUpmKjo4WAAp9DRkyRAjx5LbM//73v8LR0VEolUrRpUsXkZCQoDPGv//+K95//31hY2Mj1Gq1GDZsmHjw4IFOzalTp0S7du2EUqkUr732mpg7d2557aLBFNUnAGLt2rVSTVZWlvj4449FpUqVhJWVlejdu7e4deuWzjhXr14V3bt3FyqVSlStWlVMmDBBaDQanZro6GjRtGlTYWFhIV5//XWdbbwshg8fLlxdXYWFhYWoVq2a6NKlixSGhGCvnqdgIGK/nhgwYICoXr26sLCwEK+99poYMGCAzvvqsE+6/vzzT9GoUSOhVCpF/fr1xcqVK3XWV/Tf8QohhHixY0xERERELzdeQ0RERESyx0BEREREssdARERERLLHQERERESyx0BEREREssdARERERLLHQERERESyx0BERBXK1atXoVAoEB8fb+ypSC5evIg2bdrA0tISTZs2NejYHTt2RFBQkEHHJKKSYyAiIh1Dhw6FQqHA3LlzdZZv2bIFCoXCSLMyrqlTp8La2hoJCQmFPncqH4MN0cuNgYiICrG0tMS8efNw9+5dY0/FYHJyckr93MuXL6Ndu3ZwdXVFlSpVDDgrIqooGIiIqBAfHx84OTlhzpw5xdZMmzat0OmjxYsXo1atWtLjoUOHolevXpg9ezYcHR1hb2+PGTNmIDc3F5MmTULlypVRo0YNrF27ttD4Fy9exBtvvAFLS0s0atQI+/fv11l/9uxZdO/eHTY2NnB0dMTgwYNx+/ZtaX3Hjh0xZswYBAUFoWrVqvDz8ytyP7RaLWbMmIEaNWpAqVSiadOmiIiIkNYrFArExcVhxowZUCgUmDZtWqExhg4div3792PJkiVQKBRQKBS4evUqAGD//v1o1aoVlEolqlevjs8++wy5ubnF9nX79u2ws7PD+vXrAQDXr19H//79YW9vj8qVK6Nnz57S2E/3+Ntvv0X16tVRpUoVBAYG6nwa+PLly1G3bl1YWlrC0dER/fr1K3b7RHLFQEREhZiammL27NlYtmwZbty48UJj7d27Fzdv3kRMTAwWLlyIqVOn4u2330alSpVw9OhRjBo1Ch999FGh7UyaNAkTJkzAyZMn4e3tjR49euDff/8FANy7dw+dO3dGs2bNcPz4cURERCAtLQ39+/fXGWPdunWwsLDAoUOHEBoaWuT8lixZggULFuDbb7/F6dOn4efnh3feeQeJiYkAgFu3bqFhw4aYMGECbt26hYkTJxY5hre3N/7zn//g1q1buHXrFmrWrImUlBS89dZbaNmyJU6dOoUVK1Zg9erVmDVrVpFz+fnnn/H+++9j/fr1GDhwIDQaDfz8/GBra4sDBw7g0KFDsLGxQbdu3XSOeEVHR+Py5cuIjo7GunXrEBYWhrCwMADA8ePHMXbsWMyYMQMJCQmIiIhAhw4d9PvmEcmJQT4iloheGUOGDBE9e/YUQgjRpk0bMXz4cCGEEJs3bxZP/8qYOnWq8PT01HnuokWLhKurq85Yrq6uIi8vT1rm7u4u2rdvLz3Ozc0V1tbWIjw8XAghRFJSkgCg8wnWGo1G1KhRQ8ybN08IIcTMmTOFr6+vzravX78uAEifnv3mm2+KZs2aPXd/nZ2dxddff62zrGXLluLjjz+WHnt6eoqpU6c+c5yCnxgvhBCff/65cHd3F1qtVloWEhIibGxspJ7kP++7774TdnZ2Yt++fVLtjz/+WOj52dnZQqVSiV27dgkh/tfj3Nxcqebdd98VAwYMEEII8dtvvwm1Wi0yMjKe2wsiOTMzch4jogps3rx56Ny5c5FHRfTVsGFDmJj872C0o6MjGjVqJD02NTVFlSpVkJ6ervM8b29v6f/NzMzQokULXLhwAQBw6tQpREdHw8bGptD2Ll++jHr16gEAvLy8njm3jIwM3Lx5E23bttVZ3rZtW5w6dUrPPSzehQsX4O3trXMxetu2bfHw4UPcuHEDLi4uAIBff/0V6enpOHToEFq2bCnVnjp1Cn///TdsbW11xn38+DEuX74sPW7YsCFMTU2lx9WrV8eZM2cAAF27doWrqytef/11dOvWDd26dUPv3r1hZWX1wvtH9CphICKiYnXo0AF+fn6YMmUKhg4dqrPOxMQEQgidZU9ft5LP3Nxc57FCoShymVar1XteDx8+RI8ePTBv3rxC66pXry79v7W1td5jGlOzZs1w4sQJrFmzBi1atJAC1MOHD+Hl5SVdT/S0atWqSf//rH7a2trixIkT2LdvHyIjI/HVV19h2rRp+Ouvv2Bvb192O0X0kuE1RET0THPnzsWff/6J2NhYneXVqlVDamqqTigy5HsHHTlyRPr/3NxcxMXFwcPDAwDQvHlznDt3DrVq1UKdOnV0vkoSgtRqNZydnXHo0CGd5YcOHUKDBg1KNF8LCwvk5eXpLPPw8EBsbKxOjw4dOgRbW1vUqFFDWla7dm1ER0dj69at+OSTT6TlzZs3R2JiIhwcHArtp52dnd5zMzMzg4+PD+bPn4/Tp0/j6tWr2Lt3b4n2j+hVx0BERM/UuHFjDBw4EEuXLtVZ3rFjR/zzzz+YP38+Ll++jJCQEOzcudNg2w0JCcHmzZtx8eJFBAYG4u7duxg+fDgAIDAwEHfu3MH777+Pv/76C5cvX8auXbswbNiwQqHkeSZNmoR58+bhl19+QUJCAj777DPEx8dj3LhxJRqnVq1aOHr0KK5evYrbt29Dq9Xi448/xvXr1/HJJ5/g4sWL2Lp1K6ZOnYrg4GCd04gAUK9ePURHR+O3336T3s9o4MCBqFq1Knr27IkDBw4gKSkJ+/btw9ixY/W+2H3btm1YunQp4uPjce3aNfzwww/QarVwd3cv0f4RveoYiIjouWbMmFHolJaHhweWL1+OkJAQeHp64tixYy90rVFBc+fOxdy5c+Hp6YmDBw/ijz/+QNWqVQFAOqqTl5cHX19fNG7cGEFBQbC3ty8UNJ5n7NixCA4OxoQJE9C4cWNERETgjz/+QN26dUs0zsSJE2FqaooGDRqgWrVqSE5OxmuvvYYdO3bg2LFj8PT0xKhRoxAQEIAvv/yyyDHc3d2xd+9ehIeHY8KECbCyskJMTAxcXFzQp08feHh4ICAgAI8fP4ZardZrXvb29vj999/RuXNneHh4IDQ0FOHh4WjYsGGJ9o/oVacQBS8CICIiIpIZHiEiIiIi2WMgIiIiItljICIiIiLZYyAiIiIi2WMgIiIiItljICIiIiLZYyAiIiIi2WMgIiIiItljICIiIiLZYyAiIiIi2WMgIiIiItljICIiIiLZ+381X9yvvZb7MgAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "lengths = [len(tokenizer(example['output'])['input_ids']) for example in dataset['train']]\n",
    "\n",
    "# Plot the histogram\n",
    "plt.hist(lengths, bins=50, color='skyblue', edgecolor='black')\n",
    "plt.title(\"Histogram of Tokenized Output Lengths\")\n",
    "plt.xlabel(\"Number of tokens\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the Dataset Object for Pytorch-Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom PyTorch Dataset\n",
    "class LlamaDataset(Dataset):\n",
    "    def __init__(self, dataset):\n",
    "        self.data = dataset\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.data[idx]\n",
    "        question=sample['instruction']\n",
    "        answer = sample['output']\n",
    "        prompt = f'''<|begin_of_text|> <|start_header_id|>user<|end_header_id|>\\n\\n{question}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n'''\n",
    "        full_text = prompt+f'''{answer}।<|eot_id|>'''\n",
    "\n",
    "        tokenized = tokenizer(full_text, truncation=True, add_special_tokens=False, padding=\"max_length\", max_length=380)\n",
    "\n",
    "        input_ids = tokenized[\"input_ids\"]\n",
    "        attention_mask = tokenized[\"attention_mask\"]\n",
    "\n",
    "        # Tokenize just the prompt to get the split point\n",
    "        prompt_ids = tokenizer(prompt, add_special_tokens=False)[\"input_ids\"]\n",
    "        answer_start = len(prompt_ids)\n",
    "\n",
    "        # Mask everything before answer_start\n",
    "        labels = [-100] * answer_start + input_ids[answer_start:]\n",
    "        # Mask out padding as well\n",
    "        labels = [\n",
    "            label if token != tokenizer.pad_token_id else -100\n",
    "            for label, token in zip(labels, input_ids)\n",
    "        ]\n",
    "    \n",
    "        return {\n",
    "        \"input_ids\": torch.tensor(input_ids),\n",
    "        \"attention_mask\": torch.tensor(attention_mask),\n",
    "        \"labels\": torch.tensor(labels)\n",
    "    }\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = LlamaDataset(dataset['train'])\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=8, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.embed_tokens.weight  dtype: torch.float16  requirs grad:  True\n",
      "model.layers.0.self_attn.q_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "model.layers.0.self_attn.k_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "model.layers.0.self_attn.v_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "model.layers.0.self_attn.o_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "model.layers.0.mlp.gate_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "model.layers.0.mlp.up_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "model.layers.0.mlp.down_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "model.layers.0.input_layernorm.weight  dtype: torch.float16  requirs grad:  True\n",
      "model.layers.0.post_attention_layernorm.weight  dtype: torch.float16  requirs grad:  True\n",
      "model.layers.1.self_attn.q_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "model.layers.1.self_attn.k_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "model.layers.1.self_attn.v_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "model.layers.1.self_attn.o_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "model.layers.1.mlp.gate_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "model.layers.1.mlp.up_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "model.layers.1.mlp.down_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "model.layers.1.input_layernorm.weight  dtype: torch.float16  requirs grad:  True\n",
      "model.layers.1.post_attention_layernorm.weight  dtype: torch.float16  requirs grad:  True\n",
      "model.layers.2.self_attn.q_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "model.layers.2.self_attn.k_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "model.layers.2.self_attn.v_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "model.layers.2.self_attn.o_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "model.layers.2.mlp.gate_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "model.layers.2.mlp.up_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "model.layers.2.mlp.down_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "model.layers.2.input_layernorm.weight  dtype: torch.float16  requirs grad:  True\n",
      "model.layers.2.post_attention_layernorm.weight  dtype: torch.float16  requirs grad:  True\n",
      "model.layers.3.self_attn.q_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "model.layers.3.self_attn.k_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "model.layers.3.self_attn.v_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "model.layers.3.self_attn.o_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "model.layers.3.mlp.gate_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "model.layers.3.mlp.up_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "model.layers.3.mlp.down_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "model.layers.3.input_layernorm.weight  dtype: torch.float16  requirs grad:  True\n",
      "model.layers.3.post_attention_layernorm.weight  dtype: torch.float16  requirs grad:  True\n",
      "model.layers.4.self_attn.q_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "model.layers.4.self_attn.k_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "model.layers.4.self_attn.v_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "model.layers.4.self_attn.o_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "model.layers.4.mlp.gate_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "model.layers.4.mlp.up_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "model.layers.4.mlp.down_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "model.layers.4.input_layernorm.weight  dtype: torch.float16  requirs grad:  True\n",
      "model.layers.4.post_attention_layernorm.weight  dtype: torch.float16  requirs grad:  True\n",
      "model.layers.5.self_attn.q_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "model.layers.5.self_attn.k_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "model.layers.5.self_attn.v_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "model.layers.5.self_attn.o_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "model.layers.5.mlp.gate_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "model.layers.5.mlp.up_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "model.layers.5.mlp.down_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "model.layers.5.input_layernorm.weight  dtype: torch.float16  requirs grad:  True\n",
      "model.layers.5.post_attention_layernorm.weight  dtype: torch.float16  requirs grad:  True\n",
      "model.layers.6.self_attn.q_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "model.layers.6.self_attn.k_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "model.layers.6.self_attn.v_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "model.layers.6.self_attn.o_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "model.layers.6.mlp.gate_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "model.layers.6.mlp.up_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "model.layers.6.mlp.down_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "model.layers.6.input_layernorm.weight  dtype: torch.float16  requirs grad:  True\n",
      "model.layers.6.post_attention_layernorm.weight  dtype: torch.float16  requirs grad:  True\n",
      "model.layers.7.self_attn.q_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "model.layers.7.self_attn.k_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "model.layers.7.self_attn.v_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "model.layers.7.self_attn.o_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "model.layers.7.mlp.gate_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "model.layers.7.mlp.up_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "model.layers.7.mlp.down_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "model.layers.7.input_layernorm.weight  dtype: torch.float16  requirs grad:  True\n",
      "model.layers.7.post_attention_layernorm.weight  dtype: torch.float16  requirs grad:  True\n",
      "model.layers.8.self_attn.q_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "model.layers.8.self_attn.k_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "model.layers.8.self_attn.v_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "model.layers.8.self_attn.o_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "model.layers.8.mlp.gate_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "model.layers.8.mlp.up_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "model.layers.8.mlp.down_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "model.layers.8.input_layernorm.weight  dtype: torch.float16  requirs grad:  True\n",
      "model.layers.8.post_attention_layernorm.weight  dtype: torch.float16  requirs grad:  True\n",
      "model.layers.9.self_attn.q_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "model.layers.9.self_attn.k_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "model.layers.9.self_attn.v_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "model.layers.9.self_attn.o_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "model.layers.9.mlp.gate_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "model.layers.9.mlp.up_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "model.layers.9.mlp.down_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "model.layers.9.input_layernorm.weight  dtype: torch.float16  requirs grad:  True\n",
      "model.layers.9.post_attention_layernorm.weight  dtype: torch.float16  requirs grad:  True\n",
      "model.layers.10.self_attn.q_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "model.layers.10.self_attn.k_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "model.layers.10.self_attn.v_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "model.layers.10.self_attn.o_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "model.layers.10.mlp.gate_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "model.layers.10.mlp.up_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "model.layers.10.mlp.down_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "model.layers.10.input_layernorm.weight  dtype: torch.float16  requirs grad:  True\n",
      "model.layers.10.post_attention_layernorm.weight  dtype: torch.float16  requirs grad:  True\n",
      "model.layers.11.self_attn.q_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "model.layers.11.self_attn.k_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "model.layers.11.self_attn.v_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "model.layers.11.self_attn.o_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "model.layers.11.mlp.gate_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "model.layers.11.mlp.up_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "model.layers.11.mlp.down_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "model.layers.11.input_layernorm.weight  dtype: torch.float16  requirs grad:  True\n",
      "model.layers.11.post_attention_layernorm.weight  dtype: torch.float16  requirs grad:  True\n",
      "model.layers.12.self_attn.q_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "model.layers.12.self_attn.k_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "model.layers.12.self_attn.v_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "model.layers.12.self_attn.o_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "model.layers.12.mlp.gate_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "model.layers.12.mlp.up_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "model.layers.12.mlp.down_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "model.layers.12.input_layernorm.weight  dtype: torch.float16  requirs grad:  True\n",
      "model.layers.12.post_attention_layernorm.weight  dtype: torch.float16  requirs grad:  True\n",
      "model.layers.13.self_attn.q_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "model.layers.13.self_attn.k_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "model.layers.13.self_attn.v_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "model.layers.13.self_attn.o_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "model.layers.13.mlp.gate_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "model.layers.13.mlp.up_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "model.layers.13.mlp.down_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "model.layers.13.input_layernorm.weight  dtype: torch.float16  requirs grad:  True\n",
      "model.layers.13.post_attention_layernorm.weight  dtype: torch.float16  requirs grad:  True\n",
      "model.layers.14.self_attn.q_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "model.layers.14.self_attn.k_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "model.layers.14.self_attn.v_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "model.layers.14.self_attn.o_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "model.layers.14.mlp.gate_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "model.layers.14.mlp.up_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "model.layers.14.mlp.down_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "model.layers.14.input_layernorm.weight  dtype: torch.float16  requirs grad:  True\n",
      "model.layers.14.post_attention_layernorm.weight  dtype: torch.float16  requirs grad:  True\n",
      "model.layers.15.self_attn.q_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "model.layers.15.self_attn.k_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "model.layers.15.self_attn.v_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "model.layers.15.self_attn.o_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "model.layers.15.mlp.gate_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "model.layers.15.mlp.up_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "model.layers.15.mlp.down_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "model.layers.15.input_layernorm.weight  dtype: torch.float16  requirs grad:  True\n",
      "model.layers.15.post_attention_layernorm.weight  dtype: torch.float16  requirs grad:  True\n",
      "model.layers.16.self_attn.q_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "model.layers.16.self_attn.k_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "model.layers.16.self_attn.v_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "model.layers.16.self_attn.o_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "model.layers.16.mlp.gate_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "model.layers.16.mlp.up_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "model.layers.16.mlp.down_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "model.layers.16.input_layernorm.weight  dtype: torch.float16  requirs grad:  True\n",
      "model.layers.16.post_attention_layernorm.weight  dtype: torch.float16  requirs grad:  True\n",
      "model.layers.17.self_attn.q_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "model.layers.17.self_attn.k_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "model.layers.17.self_attn.v_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "model.layers.17.self_attn.o_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "model.layers.17.mlp.gate_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "model.layers.17.mlp.up_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "model.layers.17.mlp.down_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "model.layers.17.input_layernorm.weight  dtype: torch.float16  requirs grad:  True\n",
      "model.layers.17.post_attention_layernorm.weight  dtype: torch.float16  requirs grad:  True\n",
      "model.layers.18.self_attn.q_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "model.layers.18.self_attn.k_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "model.layers.18.self_attn.v_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "model.layers.18.self_attn.o_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "model.layers.18.mlp.gate_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "model.layers.18.mlp.up_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "model.layers.18.mlp.down_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "model.layers.18.input_layernorm.weight  dtype: torch.float16  requirs grad:  True\n",
      "model.layers.18.post_attention_layernorm.weight  dtype: torch.float16  requirs grad:  True\n",
      "model.layers.19.self_attn.q_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "model.layers.19.self_attn.k_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "model.layers.19.self_attn.v_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "model.layers.19.self_attn.o_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "model.layers.19.mlp.gate_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "model.layers.19.mlp.up_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "model.layers.19.mlp.down_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "model.layers.19.input_layernorm.weight  dtype: torch.float16  requirs grad:  True\n",
      "model.layers.19.post_attention_layernorm.weight  dtype: torch.float16  requirs grad:  True\n",
      "model.layers.20.self_attn.q_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "model.layers.20.self_attn.k_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "model.layers.20.self_attn.v_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "model.layers.20.self_attn.o_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "model.layers.20.mlp.gate_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "model.layers.20.mlp.up_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "model.layers.20.mlp.down_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "model.layers.20.input_layernorm.weight  dtype: torch.float16  requirs grad:  True\n",
      "model.layers.20.post_attention_layernorm.weight  dtype: torch.float16  requirs grad:  True\n",
      "model.layers.21.self_attn.q_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "model.layers.21.self_attn.k_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "model.layers.21.self_attn.v_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "model.layers.21.self_attn.o_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "model.layers.21.mlp.gate_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "model.layers.21.mlp.up_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "model.layers.21.mlp.down_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "model.layers.21.input_layernorm.weight  dtype: torch.float16  requirs grad:  True\n",
      "model.layers.21.post_attention_layernorm.weight  dtype: torch.float16  requirs grad:  True\n",
      "model.layers.22.self_attn.q_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "model.layers.22.self_attn.k_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "model.layers.22.self_attn.v_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "model.layers.22.self_attn.o_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "model.layers.22.mlp.gate_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "model.layers.22.mlp.up_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "model.layers.22.mlp.down_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "model.layers.22.input_layernorm.weight  dtype: torch.float16  requirs grad:  True\n",
      "model.layers.22.post_attention_layernorm.weight  dtype: torch.float16  requirs grad:  True\n",
      "model.layers.23.self_attn.q_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "model.layers.23.self_attn.k_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "model.layers.23.self_attn.v_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "model.layers.23.self_attn.o_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "model.layers.23.mlp.gate_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "model.layers.23.mlp.up_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "model.layers.23.mlp.down_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "model.layers.23.input_layernorm.weight  dtype: torch.float16  requirs grad:  True\n",
      "model.layers.23.post_attention_layernorm.weight  dtype: torch.float16  requirs grad:  True\n",
      "model.layers.24.self_attn.q_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "model.layers.24.self_attn.k_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "model.layers.24.self_attn.v_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "model.layers.24.self_attn.o_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "model.layers.24.mlp.gate_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "model.layers.24.mlp.up_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "model.layers.24.mlp.down_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "model.layers.24.input_layernorm.weight  dtype: torch.float16  requirs grad:  True\n",
      "model.layers.24.post_attention_layernorm.weight  dtype: torch.float16  requirs grad:  True\n",
      "model.layers.25.self_attn.q_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "model.layers.25.self_attn.k_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "model.layers.25.self_attn.v_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "model.layers.25.self_attn.o_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "model.layers.25.mlp.gate_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "model.layers.25.mlp.up_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "model.layers.25.mlp.down_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "model.layers.25.input_layernorm.weight  dtype: torch.float16  requirs grad:  True\n",
      "model.layers.25.post_attention_layernorm.weight  dtype: torch.float16  requirs grad:  True\n",
      "model.layers.26.self_attn.q_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "model.layers.26.self_attn.k_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "model.layers.26.self_attn.v_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "model.layers.26.self_attn.o_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "model.layers.26.mlp.gate_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "model.layers.26.mlp.up_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "model.layers.26.mlp.down_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "model.layers.26.input_layernorm.weight  dtype: torch.float16  requirs grad:  True\n",
      "model.layers.26.post_attention_layernorm.weight  dtype: torch.float16  requirs grad:  True\n",
      "model.layers.27.self_attn.q_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "model.layers.27.self_attn.k_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "model.layers.27.self_attn.v_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "model.layers.27.self_attn.o_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "model.layers.27.mlp.gate_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "model.layers.27.mlp.up_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "model.layers.27.mlp.down_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "model.layers.27.input_layernorm.weight  dtype: torch.float16  requirs grad:  True\n",
      "model.layers.27.post_attention_layernorm.weight  dtype: torch.float16  requirs grad:  True\n",
      "model.norm.weight  dtype: torch.float16  requirs grad:  True\n"
     ]
    }
   ],
   "source": [
    "for param in model.named_parameters():\n",
    "    print(param[0],' dtype:',param[1].dtype, ' requirs grad: ',param[1].requires_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Purpose of `prepare_model_for_kbit_training`\n",
    "\n",
    "When you load a model in 4-bit or 8-bit precision using bitsandbytes, some layers (like LayerNorm) still remain in full precision (float32), and certain operations (like weight updates) can be unstable or incompatible if done blindly on quantized weights.\n",
    "\n",
    "- Casts `LayerNorm` layers to `float32`\n",
    "- Sets `requires_grad=False` for all model parameters\n",
    "- Wraps the output layer (like `lm_head`) in `float32`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = prepare_model_for_kbit_training(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.embed_tokens.weight  dtype: torch.float32  requirs grad:  False\n",
      "model.layers.0.self_attn.q_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "model.layers.0.self_attn.k_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "model.layers.0.self_attn.v_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "model.layers.0.self_attn.o_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "model.layers.0.mlp.gate_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "model.layers.0.mlp.up_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "model.layers.0.mlp.down_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "model.layers.0.input_layernorm.weight  dtype: torch.float32  requirs grad:  False\n",
      "model.layers.0.post_attention_layernorm.weight  dtype: torch.float32  requirs grad:  False\n",
      "model.layers.1.self_attn.q_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "model.layers.1.self_attn.k_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "model.layers.1.self_attn.v_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "model.layers.1.self_attn.o_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "model.layers.1.mlp.gate_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "model.layers.1.mlp.up_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "model.layers.1.mlp.down_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "model.layers.1.input_layernorm.weight  dtype: torch.float32  requirs grad:  False\n",
      "model.layers.1.post_attention_layernorm.weight  dtype: torch.float32  requirs grad:  False\n",
      "model.layers.2.self_attn.q_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "model.layers.2.self_attn.k_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "model.layers.2.self_attn.v_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "model.layers.2.self_attn.o_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "model.layers.2.mlp.gate_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "model.layers.2.mlp.up_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "model.layers.2.mlp.down_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "model.layers.2.input_layernorm.weight  dtype: torch.float32  requirs grad:  False\n",
      "model.layers.2.post_attention_layernorm.weight  dtype: torch.float32  requirs grad:  False\n",
      "model.layers.3.self_attn.q_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "model.layers.3.self_attn.k_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "model.layers.3.self_attn.v_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "model.layers.3.self_attn.o_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "model.layers.3.mlp.gate_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "model.layers.3.mlp.up_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "model.layers.3.mlp.down_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "model.layers.3.input_layernorm.weight  dtype: torch.float32  requirs grad:  False\n",
      "model.layers.3.post_attention_layernorm.weight  dtype: torch.float32  requirs grad:  False\n",
      "model.layers.4.self_attn.q_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "model.layers.4.self_attn.k_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "model.layers.4.self_attn.v_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "model.layers.4.self_attn.o_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "model.layers.4.mlp.gate_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "model.layers.4.mlp.up_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "model.layers.4.mlp.down_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "model.layers.4.input_layernorm.weight  dtype: torch.float32  requirs grad:  False\n",
      "model.layers.4.post_attention_layernorm.weight  dtype: torch.float32  requirs grad:  False\n",
      "model.layers.5.self_attn.q_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "model.layers.5.self_attn.k_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "model.layers.5.self_attn.v_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "model.layers.5.self_attn.o_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "model.layers.5.mlp.gate_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "model.layers.5.mlp.up_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "model.layers.5.mlp.down_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "model.layers.5.input_layernorm.weight  dtype: torch.float32  requirs grad:  False\n",
      "model.layers.5.post_attention_layernorm.weight  dtype: torch.float32  requirs grad:  False\n",
      "model.layers.6.self_attn.q_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "model.layers.6.self_attn.k_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "model.layers.6.self_attn.v_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "model.layers.6.self_attn.o_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "model.layers.6.mlp.gate_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "model.layers.6.mlp.up_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "model.layers.6.mlp.down_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "model.layers.6.input_layernorm.weight  dtype: torch.float32  requirs grad:  False\n",
      "model.layers.6.post_attention_layernorm.weight  dtype: torch.float32  requirs grad:  False\n",
      "model.layers.7.self_attn.q_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "model.layers.7.self_attn.k_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "model.layers.7.self_attn.v_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "model.layers.7.self_attn.o_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "model.layers.7.mlp.gate_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "model.layers.7.mlp.up_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "model.layers.7.mlp.down_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "model.layers.7.input_layernorm.weight  dtype: torch.float32  requirs grad:  False\n",
      "model.layers.7.post_attention_layernorm.weight  dtype: torch.float32  requirs grad:  False\n",
      "model.layers.8.self_attn.q_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "model.layers.8.self_attn.k_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "model.layers.8.self_attn.v_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "model.layers.8.self_attn.o_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "model.layers.8.mlp.gate_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "model.layers.8.mlp.up_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "model.layers.8.mlp.down_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "model.layers.8.input_layernorm.weight  dtype: torch.float32  requirs grad:  False\n",
      "model.layers.8.post_attention_layernorm.weight  dtype: torch.float32  requirs grad:  False\n",
      "model.layers.9.self_attn.q_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "model.layers.9.self_attn.k_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "model.layers.9.self_attn.v_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "model.layers.9.self_attn.o_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "model.layers.9.mlp.gate_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "model.layers.9.mlp.up_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "model.layers.9.mlp.down_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "model.layers.9.input_layernorm.weight  dtype: torch.float32  requirs grad:  False\n",
      "model.layers.9.post_attention_layernorm.weight  dtype: torch.float32  requirs grad:  False\n",
      "model.layers.10.self_attn.q_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "model.layers.10.self_attn.k_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "model.layers.10.self_attn.v_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "model.layers.10.self_attn.o_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "model.layers.10.mlp.gate_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "model.layers.10.mlp.up_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "model.layers.10.mlp.down_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "model.layers.10.input_layernorm.weight  dtype: torch.float32  requirs grad:  False\n",
      "model.layers.10.post_attention_layernorm.weight  dtype: torch.float32  requirs grad:  False\n",
      "model.layers.11.self_attn.q_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "model.layers.11.self_attn.k_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "model.layers.11.self_attn.v_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "model.layers.11.self_attn.o_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "model.layers.11.mlp.gate_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "model.layers.11.mlp.up_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "model.layers.11.mlp.down_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "model.layers.11.input_layernorm.weight  dtype: torch.float32  requirs grad:  False\n",
      "model.layers.11.post_attention_layernorm.weight  dtype: torch.float32  requirs grad:  False\n",
      "model.layers.12.self_attn.q_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "model.layers.12.self_attn.k_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "model.layers.12.self_attn.v_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "model.layers.12.self_attn.o_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "model.layers.12.mlp.gate_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "model.layers.12.mlp.up_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "model.layers.12.mlp.down_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "model.layers.12.input_layernorm.weight  dtype: torch.float32  requirs grad:  False\n",
      "model.layers.12.post_attention_layernorm.weight  dtype: torch.float32  requirs grad:  False\n",
      "model.layers.13.self_attn.q_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "model.layers.13.self_attn.k_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "model.layers.13.self_attn.v_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "model.layers.13.self_attn.o_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "model.layers.13.mlp.gate_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "model.layers.13.mlp.up_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "model.layers.13.mlp.down_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "model.layers.13.input_layernorm.weight  dtype: torch.float32  requirs grad:  False\n",
      "model.layers.13.post_attention_layernorm.weight  dtype: torch.float32  requirs grad:  False\n",
      "model.layers.14.self_attn.q_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "model.layers.14.self_attn.k_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "model.layers.14.self_attn.v_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "model.layers.14.self_attn.o_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "model.layers.14.mlp.gate_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "model.layers.14.mlp.up_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "model.layers.14.mlp.down_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "model.layers.14.input_layernorm.weight  dtype: torch.float32  requirs grad:  False\n",
      "model.layers.14.post_attention_layernorm.weight  dtype: torch.float32  requirs grad:  False\n",
      "model.layers.15.self_attn.q_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "model.layers.15.self_attn.k_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "model.layers.15.self_attn.v_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "model.layers.15.self_attn.o_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "model.layers.15.mlp.gate_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "model.layers.15.mlp.up_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "model.layers.15.mlp.down_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "model.layers.15.input_layernorm.weight  dtype: torch.float32  requirs grad:  False\n",
      "model.layers.15.post_attention_layernorm.weight  dtype: torch.float32  requirs grad:  False\n",
      "model.layers.16.self_attn.q_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "model.layers.16.self_attn.k_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "model.layers.16.self_attn.v_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "model.layers.16.self_attn.o_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "model.layers.16.mlp.gate_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "model.layers.16.mlp.up_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "model.layers.16.mlp.down_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "model.layers.16.input_layernorm.weight  dtype: torch.float32  requirs grad:  False\n",
      "model.layers.16.post_attention_layernorm.weight  dtype: torch.float32  requirs grad:  False\n",
      "model.layers.17.self_attn.q_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "model.layers.17.self_attn.k_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "model.layers.17.self_attn.v_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "model.layers.17.self_attn.o_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "model.layers.17.mlp.gate_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "model.layers.17.mlp.up_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "model.layers.17.mlp.down_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "model.layers.17.input_layernorm.weight  dtype: torch.float32  requirs grad:  False\n",
      "model.layers.17.post_attention_layernorm.weight  dtype: torch.float32  requirs grad:  False\n",
      "model.layers.18.self_attn.q_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "model.layers.18.self_attn.k_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "model.layers.18.self_attn.v_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "model.layers.18.self_attn.o_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "model.layers.18.mlp.gate_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "model.layers.18.mlp.up_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "model.layers.18.mlp.down_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "model.layers.18.input_layernorm.weight  dtype: torch.float32  requirs grad:  False\n",
      "model.layers.18.post_attention_layernorm.weight  dtype: torch.float32  requirs grad:  False\n",
      "model.layers.19.self_attn.q_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "model.layers.19.self_attn.k_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "model.layers.19.self_attn.v_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "model.layers.19.self_attn.o_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "model.layers.19.mlp.gate_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "model.layers.19.mlp.up_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "model.layers.19.mlp.down_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "model.layers.19.input_layernorm.weight  dtype: torch.float32  requirs grad:  False\n",
      "model.layers.19.post_attention_layernorm.weight  dtype: torch.float32  requirs grad:  False\n",
      "model.layers.20.self_attn.q_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "model.layers.20.self_attn.k_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "model.layers.20.self_attn.v_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "model.layers.20.self_attn.o_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "model.layers.20.mlp.gate_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "model.layers.20.mlp.up_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "model.layers.20.mlp.down_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "model.layers.20.input_layernorm.weight  dtype: torch.float32  requirs grad:  False\n",
      "model.layers.20.post_attention_layernorm.weight  dtype: torch.float32  requirs grad:  False\n",
      "model.layers.21.self_attn.q_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "model.layers.21.self_attn.k_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "model.layers.21.self_attn.v_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "model.layers.21.self_attn.o_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "model.layers.21.mlp.gate_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "model.layers.21.mlp.up_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "model.layers.21.mlp.down_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "model.layers.21.input_layernorm.weight  dtype: torch.float32  requirs grad:  False\n",
      "model.layers.21.post_attention_layernorm.weight  dtype: torch.float32  requirs grad:  False\n",
      "model.layers.22.self_attn.q_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "model.layers.22.self_attn.k_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "model.layers.22.self_attn.v_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "model.layers.22.self_attn.o_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "model.layers.22.mlp.gate_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "model.layers.22.mlp.up_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "model.layers.22.mlp.down_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "model.layers.22.input_layernorm.weight  dtype: torch.float32  requirs grad:  False\n",
      "model.layers.22.post_attention_layernorm.weight  dtype: torch.float32  requirs grad:  False\n",
      "model.layers.23.self_attn.q_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "model.layers.23.self_attn.k_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "model.layers.23.self_attn.v_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "model.layers.23.self_attn.o_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "model.layers.23.mlp.gate_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "model.layers.23.mlp.up_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "model.layers.23.mlp.down_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "model.layers.23.input_layernorm.weight  dtype: torch.float32  requirs grad:  False\n",
      "model.layers.23.post_attention_layernorm.weight  dtype: torch.float32  requirs grad:  False\n",
      "model.layers.24.self_attn.q_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "model.layers.24.self_attn.k_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "model.layers.24.self_attn.v_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "model.layers.24.self_attn.o_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "model.layers.24.mlp.gate_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "model.layers.24.mlp.up_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "model.layers.24.mlp.down_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "model.layers.24.input_layernorm.weight  dtype: torch.float32  requirs grad:  False\n",
      "model.layers.24.post_attention_layernorm.weight  dtype: torch.float32  requirs grad:  False\n",
      "model.layers.25.self_attn.q_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "model.layers.25.self_attn.k_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "model.layers.25.self_attn.v_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "model.layers.25.self_attn.o_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "model.layers.25.mlp.gate_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "model.layers.25.mlp.up_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "model.layers.25.mlp.down_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "model.layers.25.input_layernorm.weight  dtype: torch.float32  requirs grad:  False\n",
      "model.layers.25.post_attention_layernorm.weight  dtype: torch.float32  requirs grad:  False\n",
      "model.layers.26.self_attn.q_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "model.layers.26.self_attn.k_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "model.layers.26.self_attn.v_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "model.layers.26.self_attn.o_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "model.layers.26.mlp.gate_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "model.layers.26.mlp.up_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "model.layers.26.mlp.down_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "model.layers.26.input_layernorm.weight  dtype: torch.float32  requirs grad:  False\n",
      "model.layers.26.post_attention_layernorm.weight  dtype: torch.float32  requirs grad:  False\n",
      "model.layers.27.self_attn.q_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "model.layers.27.self_attn.k_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "model.layers.27.self_attn.v_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "model.layers.27.self_attn.o_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "model.layers.27.mlp.gate_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "model.layers.27.mlp.up_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "model.layers.27.mlp.down_proj.weight  dtype: torch.uint8  requirs grad:  False\n",
      "model.layers.27.input_layernorm.weight  dtype: torch.float32  requirs grad:  False\n",
      "model.layers.27.post_attention_layernorm.weight  dtype: torch.float32  requirs grad:  False\n",
      "model.norm.weight  dtype: torch.float32  requirs grad:  False\n"
     ]
    }
   ],
   "source": [
    "for param in model.named_parameters():\n",
    "    print(param[0],' dtype:',param[1].dtype, ' requirs grad: ',param[1].requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 97,255,424 || all params: 3,310,005,248 || trainable%: 2.9382\n"
     ]
    }
   ],
   "source": [
    "config = LoraConfig(\n",
    "    r=64,\n",
    "    lora_alpha=16,\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    inference_mode=False,\n",
    "    use_rslora=True,\n",
    "    init_lora_weights=\"gaussian\",\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, config)\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "base_model.model.model.embed_tokens.weight  dtype: torch.float32  requirs grad:  False\n",
      "base_model.model.model.layers.0.self_attn.q_proj.base_layer.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.model.layers.0.self_attn.q_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.0.self_attn.q_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.0.self_attn.k_proj.base_layer.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.model.layers.0.self_attn.k_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.0.self_attn.k_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.0.self_attn.v_proj.base_layer.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.model.layers.0.self_attn.v_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.0.self_attn.v_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.0.self_attn.o_proj.base_layer.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.model.layers.0.self_attn.o_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.0.self_attn.o_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.0.mlp.gate_proj.base_layer.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.model.layers.0.mlp.gate_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.0.mlp.gate_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.0.mlp.up_proj.base_layer.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.model.layers.0.mlp.up_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.0.mlp.up_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.0.mlp.down_proj.base_layer.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.model.layers.0.mlp.down_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.0.mlp.down_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.0.input_layernorm.weight  dtype: torch.float32  requirs grad:  False\n",
      "base_model.model.model.layers.0.post_attention_layernorm.weight  dtype: torch.float32  requirs grad:  False\n",
      "base_model.model.model.layers.1.self_attn.q_proj.base_layer.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.model.layers.1.self_attn.q_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.1.self_attn.q_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.1.self_attn.k_proj.base_layer.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.model.layers.1.self_attn.k_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.1.self_attn.k_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.1.self_attn.v_proj.base_layer.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.model.layers.1.self_attn.v_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.1.self_attn.v_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.1.self_attn.o_proj.base_layer.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.model.layers.1.self_attn.o_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.1.self_attn.o_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.1.mlp.gate_proj.base_layer.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.model.layers.1.mlp.gate_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.1.mlp.gate_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.1.mlp.up_proj.base_layer.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.model.layers.1.mlp.up_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.1.mlp.up_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.1.mlp.down_proj.base_layer.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.model.layers.1.mlp.down_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.1.mlp.down_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.1.input_layernorm.weight  dtype: torch.float32  requirs grad:  False\n",
      "base_model.model.model.layers.1.post_attention_layernorm.weight  dtype: torch.float32  requirs grad:  False\n",
      "base_model.model.model.layers.2.self_attn.q_proj.base_layer.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.model.layers.2.self_attn.q_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.2.self_attn.q_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.2.self_attn.k_proj.base_layer.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.model.layers.2.self_attn.k_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.2.self_attn.k_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.2.self_attn.v_proj.base_layer.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.model.layers.2.self_attn.v_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.2.self_attn.v_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.2.self_attn.o_proj.base_layer.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.model.layers.2.self_attn.o_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.2.self_attn.o_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.2.mlp.gate_proj.base_layer.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.model.layers.2.mlp.gate_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.2.mlp.gate_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.2.mlp.up_proj.base_layer.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.model.layers.2.mlp.up_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.2.mlp.up_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.2.mlp.down_proj.base_layer.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.model.layers.2.mlp.down_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.2.mlp.down_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.2.input_layernorm.weight  dtype: torch.float32  requirs grad:  False\n",
      "base_model.model.model.layers.2.post_attention_layernorm.weight  dtype: torch.float32  requirs grad:  False\n",
      "base_model.model.model.layers.3.self_attn.q_proj.base_layer.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.model.layers.3.self_attn.q_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.3.self_attn.q_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.3.self_attn.k_proj.base_layer.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.model.layers.3.self_attn.k_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.3.self_attn.k_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.3.self_attn.v_proj.base_layer.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.model.layers.3.self_attn.v_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.3.self_attn.v_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.3.self_attn.o_proj.base_layer.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.model.layers.3.self_attn.o_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.3.self_attn.o_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.3.mlp.gate_proj.base_layer.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.model.layers.3.mlp.gate_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.3.mlp.gate_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.3.mlp.up_proj.base_layer.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.model.layers.3.mlp.up_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.3.mlp.up_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.3.mlp.down_proj.base_layer.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.model.layers.3.mlp.down_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.3.mlp.down_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.3.input_layernorm.weight  dtype: torch.float32  requirs grad:  False\n",
      "base_model.model.model.layers.3.post_attention_layernorm.weight  dtype: torch.float32  requirs grad:  False\n",
      "base_model.model.model.layers.4.self_attn.q_proj.base_layer.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.model.layers.4.self_attn.q_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.4.self_attn.q_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.4.self_attn.k_proj.base_layer.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.model.layers.4.self_attn.k_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.4.self_attn.k_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.4.self_attn.v_proj.base_layer.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.model.layers.4.self_attn.v_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.4.self_attn.v_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.4.self_attn.o_proj.base_layer.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.model.layers.4.self_attn.o_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.4.self_attn.o_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.4.mlp.gate_proj.base_layer.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.model.layers.4.mlp.gate_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.4.mlp.gate_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.4.mlp.up_proj.base_layer.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.model.layers.4.mlp.up_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.4.mlp.up_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.4.mlp.down_proj.base_layer.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.model.layers.4.mlp.down_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.4.mlp.down_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.4.input_layernorm.weight  dtype: torch.float32  requirs grad:  False\n",
      "base_model.model.model.layers.4.post_attention_layernorm.weight  dtype: torch.float32  requirs grad:  False\n",
      "base_model.model.model.layers.5.self_attn.q_proj.base_layer.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.model.layers.5.self_attn.q_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.5.self_attn.q_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.5.self_attn.k_proj.base_layer.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.model.layers.5.self_attn.k_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.5.self_attn.k_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.5.self_attn.v_proj.base_layer.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.model.layers.5.self_attn.v_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.5.self_attn.v_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.5.self_attn.o_proj.base_layer.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.model.layers.5.self_attn.o_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.5.self_attn.o_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.5.mlp.gate_proj.base_layer.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.model.layers.5.mlp.gate_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.5.mlp.gate_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.5.mlp.up_proj.base_layer.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.model.layers.5.mlp.up_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.5.mlp.up_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.5.mlp.down_proj.base_layer.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.model.layers.5.mlp.down_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.5.mlp.down_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.5.input_layernorm.weight  dtype: torch.float32  requirs grad:  False\n",
      "base_model.model.model.layers.5.post_attention_layernorm.weight  dtype: torch.float32  requirs grad:  False\n",
      "base_model.model.model.layers.6.self_attn.q_proj.base_layer.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.model.layers.6.self_attn.q_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.6.self_attn.q_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.6.self_attn.k_proj.base_layer.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.model.layers.6.self_attn.k_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.6.self_attn.k_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.6.self_attn.v_proj.base_layer.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.model.layers.6.self_attn.v_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.6.self_attn.v_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.6.self_attn.o_proj.base_layer.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.model.layers.6.self_attn.o_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.6.self_attn.o_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.6.mlp.gate_proj.base_layer.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.model.layers.6.mlp.gate_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.6.mlp.gate_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.6.mlp.up_proj.base_layer.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.model.layers.6.mlp.up_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.6.mlp.up_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.6.mlp.down_proj.base_layer.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.model.layers.6.mlp.down_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.6.mlp.down_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.6.input_layernorm.weight  dtype: torch.float32  requirs grad:  False\n",
      "base_model.model.model.layers.6.post_attention_layernorm.weight  dtype: torch.float32  requirs grad:  False\n",
      "base_model.model.model.layers.7.self_attn.q_proj.base_layer.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.model.layers.7.self_attn.q_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.7.self_attn.q_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.7.self_attn.k_proj.base_layer.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.model.layers.7.self_attn.k_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.7.self_attn.k_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.7.self_attn.v_proj.base_layer.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.model.layers.7.self_attn.v_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.7.self_attn.v_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.7.self_attn.o_proj.base_layer.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.model.layers.7.self_attn.o_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.7.self_attn.o_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.7.mlp.gate_proj.base_layer.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.model.layers.7.mlp.gate_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.7.mlp.gate_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.7.mlp.up_proj.base_layer.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.model.layers.7.mlp.up_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.7.mlp.up_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.7.mlp.down_proj.base_layer.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.model.layers.7.mlp.down_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.7.mlp.down_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.7.input_layernorm.weight  dtype: torch.float32  requirs grad:  False\n",
      "base_model.model.model.layers.7.post_attention_layernorm.weight  dtype: torch.float32  requirs grad:  False\n",
      "base_model.model.model.layers.8.self_attn.q_proj.base_layer.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.model.layers.8.self_attn.q_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.8.self_attn.q_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.8.self_attn.k_proj.base_layer.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.model.layers.8.self_attn.k_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.8.self_attn.k_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.8.self_attn.v_proj.base_layer.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.model.layers.8.self_attn.v_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.8.self_attn.v_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.8.self_attn.o_proj.base_layer.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.model.layers.8.self_attn.o_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.8.self_attn.o_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.8.mlp.gate_proj.base_layer.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.model.layers.8.mlp.gate_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.8.mlp.gate_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.8.mlp.up_proj.base_layer.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.model.layers.8.mlp.up_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.8.mlp.up_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.8.mlp.down_proj.base_layer.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.model.layers.8.mlp.down_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.8.mlp.down_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.8.input_layernorm.weight  dtype: torch.float32  requirs grad:  False\n",
      "base_model.model.model.layers.8.post_attention_layernorm.weight  dtype: torch.float32  requirs grad:  False\n",
      "base_model.model.model.layers.9.self_attn.q_proj.base_layer.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.model.layers.9.self_attn.q_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.9.self_attn.q_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.9.self_attn.k_proj.base_layer.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.model.layers.9.self_attn.k_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.9.self_attn.k_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.9.self_attn.v_proj.base_layer.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.model.layers.9.self_attn.v_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.9.self_attn.v_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.9.self_attn.o_proj.base_layer.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.model.layers.9.self_attn.o_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.9.self_attn.o_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.9.mlp.gate_proj.base_layer.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.model.layers.9.mlp.gate_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.9.mlp.gate_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.9.mlp.up_proj.base_layer.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.model.layers.9.mlp.up_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.9.mlp.up_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.9.mlp.down_proj.base_layer.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.model.layers.9.mlp.down_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.9.mlp.down_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.9.input_layernorm.weight  dtype: torch.float32  requirs grad:  False\n",
      "base_model.model.model.layers.9.post_attention_layernorm.weight  dtype: torch.float32  requirs grad:  False\n",
      "base_model.model.model.layers.10.self_attn.q_proj.base_layer.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.model.layers.10.self_attn.q_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.10.self_attn.q_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.10.self_attn.k_proj.base_layer.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.model.layers.10.self_attn.k_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.10.self_attn.k_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.10.self_attn.v_proj.base_layer.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.model.layers.10.self_attn.v_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.10.self_attn.v_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.10.self_attn.o_proj.base_layer.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.model.layers.10.self_attn.o_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.10.self_attn.o_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.10.mlp.gate_proj.base_layer.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.model.layers.10.mlp.gate_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.10.mlp.gate_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.10.mlp.up_proj.base_layer.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.model.layers.10.mlp.up_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.10.mlp.up_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.10.mlp.down_proj.base_layer.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.model.layers.10.mlp.down_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.10.mlp.down_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.10.input_layernorm.weight  dtype: torch.float32  requirs grad:  False\n",
      "base_model.model.model.layers.10.post_attention_layernorm.weight  dtype: torch.float32  requirs grad:  False\n",
      "base_model.model.model.layers.11.self_attn.q_proj.base_layer.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.model.layers.11.self_attn.q_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.11.self_attn.q_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.11.self_attn.k_proj.base_layer.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.model.layers.11.self_attn.k_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.11.self_attn.k_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.11.self_attn.v_proj.base_layer.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.model.layers.11.self_attn.v_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.11.self_attn.v_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.11.self_attn.o_proj.base_layer.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.model.layers.11.self_attn.o_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.11.self_attn.o_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.11.mlp.gate_proj.base_layer.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.model.layers.11.mlp.gate_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.11.mlp.gate_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.11.mlp.up_proj.base_layer.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.model.layers.11.mlp.up_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.11.mlp.up_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.11.mlp.down_proj.base_layer.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.model.layers.11.mlp.down_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.11.mlp.down_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.11.input_layernorm.weight  dtype: torch.float32  requirs grad:  False\n",
      "base_model.model.model.layers.11.post_attention_layernorm.weight  dtype: torch.float32  requirs grad:  False\n",
      "base_model.model.model.layers.12.self_attn.q_proj.base_layer.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.model.layers.12.self_attn.q_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.12.self_attn.q_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.12.self_attn.k_proj.base_layer.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.model.layers.12.self_attn.k_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.12.self_attn.k_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.12.self_attn.v_proj.base_layer.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.model.layers.12.self_attn.v_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.12.self_attn.v_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.12.self_attn.o_proj.base_layer.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.model.layers.12.self_attn.o_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.12.self_attn.o_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.12.mlp.gate_proj.base_layer.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.model.layers.12.mlp.gate_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.12.mlp.gate_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.12.mlp.up_proj.base_layer.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.model.layers.12.mlp.up_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.12.mlp.up_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.12.mlp.down_proj.base_layer.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.model.layers.12.mlp.down_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.12.mlp.down_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.12.input_layernorm.weight  dtype: torch.float32  requirs grad:  False\n",
      "base_model.model.model.layers.12.post_attention_layernorm.weight  dtype: torch.float32  requirs grad:  False\n",
      "base_model.model.model.layers.13.self_attn.q_proj.base_layer.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.model.layers.13.self_attn.q_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.13.self_attn.q_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.13.self_attn.k_proj.base_layer.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.model.layers.13.self_attn.k_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.13.self_attn.k_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.13.self_attn.v_proj.base_layer.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.model.layers.13.self_attn.v_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.13.self_attn.v_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.13.self_attn.o_proj.base_layer.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.model.layers.13.self_attn.o_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.13.self_attn.o_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.13.mlp.gate_proj.base_layer.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.model.layers.13.mlp.gate_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.13.mlp.gate_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.13.mlp.up_proj.base_layer.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.model.layers.13.mlp.up_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.13.mlp.up_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.13.mlp.down_proj.base_layer.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.model.layers.13.mlp.down_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.13.mlp.down_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.13.input_layernorm.weight  dtype: torch.float32  requirs grad:  False\n",
      "base_model.model.model.layers.13.post_attention_layernorm.weight  dtype: torch.float32  requirs grad:  False\n",
      "base_model.model.model.layers.14.self_attn.q_proj.base_layer.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.model.layers.14.self_attn.q_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.14.self_attn.q_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.14.self_attn.k_proj.base_layer.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.model.layers.14.self_attn.k_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.14.self_attn.k_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.14.self_attn.v_proj.base_layer.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.model.layers.14.self_attn.v_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.14.self_attn.v_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.14.self_attn.o_proj.base_layer.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.model.layers.14.self_attn.o_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.14.self_attn.o_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.14.mlp.gate_proj.base_layer.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.model.layers.14.mlp.gate_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.14.mlp.gate_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.14.mlp.up_proj.base_layer.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.model.layers.14.mlp.up_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.14.mlp.up_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.14.mlp.down_proj.base_layer.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.model.layers.14.mlp.down_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.14.mlp.down_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.14.input_layernorm.weight  dtype: torch.float32  requirs grad:  False\n",
      "base_model.model.model.layers.14.post_attention_layernorm.weight  dtype: torch.float32  requirs grad:  False\n",
      "base_model.model.model.layers.15.self_attn.q_proj.base_layer.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.model.layers.15.self_attn.q_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.15.self_attn.q_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.15.self_attn.k_proj.base_layer.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.model.layers.15.self_attn.k_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.15.self_attn.k_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.15.self_attn.v_proj.base_layer.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.model.layers.15.self_attn.v_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.15.self_attn.v_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.15.self_attn.o_proj.base_layer.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.model.layers.15.self_attn.o_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.15.self_attn.o_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.15.mlp.gate_proj.base_layer.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.model.layers.15.mlp.gate_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.15.mlp.gate_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.15.mlp.up_proj.base_layer.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.model.layers.15.mlp.up_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.15.mlp.up_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.15.mlp.down_proj.base_layer.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.model.layers.15.mlp.down_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.15.mlp.down_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.15.input_layernorm.weight  dtype: torch.float32  requirs grad:  False\n",
      "base_model.model.model.layers.15.post_attention_layernorm.weight  dtype: torch.float32  requirs grad:  False\n",
      "base_model.model.model.layers.16.self_attn.q_proj.base_layer.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.model.layers.16.self_attn.q_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.16.self_attn.q_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.16.self_attn.k_proj.base_layer.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.model.layers.16.self_attn.k_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.16.self_attn.k_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.16.self_attn.v_proj.base_layer.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.model.layers.16.self_attn.v_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.16.self_attn.v_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.16.self_attn.o_proj.base_layer.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.model.layers.16.self_attn.o_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.16.self_attn.o_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.16.mlp.gate_proj.base_layer.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.model.layers.16.mlp.gate_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.16.mlp.gate_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.16.mlp.up_proj.base_layer.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.model.layers.16.mlp.up_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.16.mlp.up_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.16.mlp.down_proj.base_layer.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.model.layers.16.mlp.down_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.16.mlp.down_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.16.input_layernorm.weight  dtype: torch.float32  requirs grad:  False\n",
      "base_model.model.model.layers.16.post_attention_layernorm.weight  dtype: torch.float32  requirs grad:  False\n",
      "base_model.model.model.layers.17.self_attn.q_proj.base_layer.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.model.layers.17.self_attn.q_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.17.self_attn.q_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.17.self_attn.k_proj.base_layer.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.model.layers.17.self_attn.k_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.17.self_attn.k_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.17.self_attn.v_proj.base_layer.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.model.layers.17.self_attn.v_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.17.self_attn.v_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.17.self_attn.o_proj.base_layer.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.model.layers.17.self_attn.o_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.17.self_attn.o_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.17.mlp.gate_proj.base_layer.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.model.layers.17.mlp.gate_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.17.mlp.gate_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.17.mlp.up_proj.base_layer.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.model.layers.17.mlp.up_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.17.mlp.up_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.17.mlp.down_proj.base_layer.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.model.layers.17.mlp.down_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.17.mlp.down_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.17.input_layernorm.weight  dtype: torch.float32  requirs grad:  False\n",
      "base_model.model.model.layers.17.post_attention_layernorm.weight  dtype: torch.float32  requirs grad:  False\n",
      "base_model.model.model.layers.18.self_attn.q_proj.base_layer.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.model.layers.18.self_attn.q_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.18.self_attn.q_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.18.self_attn.k_proj.base_layer.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.model.layers.18.self_attn.k_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.18.self_attn.k_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.18.self_attn.v_proj.base_layer.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.model.layers.18.self_attn.v_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.18.self_attn.v_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.18.self_attn.o_proj.base_layer.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.model.layers.18.self_attn.o_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.18.self_attn.o_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.18.mlp.gate_proj.base_layer.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.model.layers.18.mlp.gate_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.18.mlp.gate_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.18.mlp.up_proj.base_layer.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.model.layers.18.mlp.up_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.18.mlp.up_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.18.mlp.down_proj.base_layer.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.model.layers.18.mlp.down_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.18.mlp.down_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.18.input_layernorm.weight  dtype: torch.float32  requirs grad:  False\n",
      "base_model.model.model.layers.18.post_attention_layernorm.weight  dtype: torch.float32  requirs grad:  False\n",
      "base_model.model.model.layers.19.self_attn.q_proj.base_layer.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.model.layers.19.self_attn.q_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.19.self_attn.q_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.19.self_attn.k_proj.base_layer.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.model.layers.19.self_attn.k_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.19.self_attn.k_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.19.self_attn.v_proj.base_layer.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.model.layers.19.self_attn.v_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.19.self_attn.v_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.19.self_attn.o_proj.base_layer.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.model.layers.19.self_attn.o_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.19.self_attn.o_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.19.mlp.gate_proj.base_layer.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.model.layers.19.mlp.gate_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.19.mlp.gate_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.19.mlp.up_proj.base_layer.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.model.layers.19.mlp.up_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.19.mlp.up_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.19.mlp.down_proj.base_layer.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.model.layers.19.mlp.down_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.19.mlp.down_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.19.input_layernorm.weight  dtype: torch.float32  requirs grad:  False\n",
      "base_model.model.model.layers.19.post_attention_layernorm.weight  dtype: torch.float32  requirs grad:  False\n",
      "base_model.model.model.layers.20.self_attn.q_proj.base_layer.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.model.layers.20.self_attn.q_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.20.self_attn.q_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.20.self_attn.k_proj.base_layer.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.model.layers.20.self_attn.k_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.20.self_attn.k_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.20.self_attn.v_proj.base_layer.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.model.layers.20.self_attn.v_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.20.self_attn.v_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.20.self_attn.o_proj.base_layer.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.model.layers.20.self_attn.o_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.20.self_attn.o_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.20.mlp.gate_proj.base_layer.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.model.layers.20.mlp.gate_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.20.mlp.gate_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.20.mlp.up_proj.base_layer.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.model.layers.20.mlp.up_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.20.mlp.up_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.20.mlp.down_proj.base_layer.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.model.layers.20.mlp.down_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.20.mlp.down_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.20.input_layernorm.weight  dtype: torch.float32  requirs grad:  False\n",
      "base_model.model.model.layers.20.post_attention_layernorm.weight  dtype: torch.float32  requirs grad:  False\n",
      "base_model.model.model.layers.21.self_attn.q_proj.base_layer.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.model.layers.21.self_attn.q_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.21.self_attn.q_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.21.self_attn.k_proj.base_layer.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.model.layers.21.self_attn.k_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.21.self_attn.k_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.21.self_attn.v_proj.base_layer.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.model.layers.21.self_attn.v_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.21.self_attn.v_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.21.self_attn.o_proj.base_layer.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.model.layers.21.self_attn.o_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.21.self_attn.o_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.21.mlp.gate_proj.base_layer.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.model.layers.21.mlp.gate_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.21.mlp.gate_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.21.mlp.up_proj.base_layer.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.model.layers.21.mlp.up_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.21.mlp.up_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.21.mlp.down_proj.base_layer.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.model.layers.21.mlp.down_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.21.mlp.down_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.21.input_layernorm.weight  dtype: torch.float32  requirs grad:  False\n",
      "base_model.model.model.layers.21.post_attention_layernorm.weight  dtype: torch.float32  requirs grad:  False\n",
      "base_model.model.model.layers.22.self_attn.q_proj.base_layer.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.model.layers.22.self_attn.q_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.22.self_attn.q_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.22.self_attn.k_proj.base_layer.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.model.layers.22.self_attn.k_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.22.self_attn.k_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.22.self_attn.v_proj.base_layer.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.model.layers.22.self_attn.v_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.22.self_attn.v_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.22.self_attn.o_proj.base_layer.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.model.layers.22.self_attn.o_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.22.self_attn.o_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.22.mlp.gate_proj.base_layer.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.model.layers.22.mlp.gate_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.22.mlp.gate_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.22.mlp.up_proj.base_layer.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.model.layers.22.mlp.up_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.22.mlp.up_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.22.mlp.down_proj.base_layer.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.model.layers.22.mlp.down_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.22.mlp.down_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.22.input_layernorm.weight  dtype: torch.float32  requirs grad:  False\n",
      "base_model.model.model.layers.22.post_attention_layernorm.weight  dtype: torch.float32  requirs grad:  False\n",
      "base_model.model.model.layers.23.self_attn.q_proj.base_layer.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.model.layers.23.self_attn.q_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.23.self_attn.q_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.23.self_attn.k_proj.base_layer.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.model.layers.23.self_attn.k_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.23.self_attn.k_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.23.self_attn.v_proj.base_layer.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.model.layers.23.self_attn.v_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.23.self_attn.v_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.23.self_attn.o_proj.base_layer.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.model.layers.23.self_attn.o_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.23.self_attn.o_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.23.mlp.gate_proj.base_layer.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.model.layers.23.mlp.gate_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.23.mlp.gate_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.23.mlp.up_proj.base_layer.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.model.layers.23.mlp.up_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.23.mlp.up_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.23.mlp.down_proj.base_layer.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.model.layers.23.mlp.down_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.23.mlp.down_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.23.input_layernorm.weight  dtype: torch.float32  requirs grad:  False\n",
      "base_model.model.model.layers.23.post_attention_layernorm.weight  dtype: torch.float32  requirs grad:  False\n",
      "base_model.model.model.layers.24.self_attn.q_proj.base_layer.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.model.layers.24.self_attn.q_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.24.self_attn.q_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.24.self_attn.k_proj.base_layer.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.model.layers.24.self_attn.k_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.24.self_attn.k_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.24.self_attn.v_proj.base_layer.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.model.layers.24.self_attn.v_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.24.self_attn.v_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.24.self_attn.o_proj.base_layer.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.model.layers.24.self_attn.o_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.24.self_attn.o_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.24.mlp.gate_proj.base_layer.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.model.layers.24.mlp.gate_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.24.mlp.gate_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.24.mlp.up_proj.base_layer.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.model.layers.24.mlp.up_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.24.mlp.up_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.24.mlp.down_proj.base_layer.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.model.layers.24.mlp.down_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.24.mlp.down_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.24.input_layernorm.weight  dtype: torch.float32  requirs grad:  False\n",
      "base_model.model.model.layers.24.post_attention_layernorm.weight  dtype: torch.float32  requirs grad:  False\n",
      "base_model.model.model.layers.25.self_attn.q_proj.base_layer.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.model.layers.25.self_attn.q_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.25.self_attn.q_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.25.self_attn.k_proj.base_layer.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.model.layers.25.self_attn.k_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.25.self_attn.k_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.25.self_attn.v_proj.base_layer.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.model.layers.25.self_attn.v_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.25.self_attn.v_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.25.self_attn.o_proj.base_layer.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.model.layers.25.self_attn.o_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.25.self_attn.o_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.25.mlp.gate_proj.base_layer.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.model.layers.25.mlp.gate_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.25.mlp.gate_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.25.mlp.up_proj.base_layer.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.model.layers.25.mlp.up_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.25.mlp.up_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.25.mlp.down_proj.base_layer.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.model.layers.25.mlp.down_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.25.mlp.down_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.25.input_layernorm.weight  dtype: torch.float32  requirs grad:  False\n",
      "base_model.model.model.layers.25.post_attention_layernorm.weight  dtype: torch.float32  requirs grad:  False\n",
      "base_model.model.model.layers.26.self_attn.q_proj.base_layer.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.model.layers.26.self_attn.q_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.26.self_attn.q_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.26.self_attn.k_proj.base_layer.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.model.layers.26.self_attn.k_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.26.self_attn.k_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.26.self_attn.v_proj.base_layer.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.model.layers.26.self_attn.v_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.26.self_attn.v_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.26.self_attn.o_proj.base_layer.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.model.layers.26.self_attn.o_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.26.self_attn.o_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.26.mlp.gate_proj.base_layer.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.model.layers.26.mlp.gate_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.26.mlp.gate_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.26.mlp.up_proj.base_layer.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.model.layers.26.mlp.up_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.26.mlp.up_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.26.mlp.down_proj.base_layer.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.model.layers.26.mlp.down_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.26.mlp.down_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.26.input_layernorm.weight  dtype: torch.float32  requirs grad:  False\n",
      "base_model.model.model.layers.26.post_attention_layernorm.weight  dtype: torch.float32  requirs grad:  False\n",
      "base_model.model.model.layers.27.self_attn.q_proj.base_layer.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.model.layers.27.self_attn.q_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.27.self_attn.q_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.27.self_attn.k_proj.base_layer.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.model.layers.27.self_attn.k_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.27.self_attn.k_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.27.self_attn.v_proj.base_layer.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.model.layers.27.self_attn.v_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.27.self_attn.v_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.27.self_attn.o_proj.base_layer.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.model.layers.27.self_attn.o_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.27.self_attn.o_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.27.mlp.gate_proj.base_layer.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.model.layers.27.mlp.gate_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.27.mlp.gate_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.27.mlp.up_proj.base_layer.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.model.layers.27.mlp.up_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.27.mlp.up_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.27.mlp.down_proj.base_layer.weight  dtype: torch.uint8  requirs grad:  False\n",
      "base_model.model.model.layers.27.mlp.down_proj.lora_A.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.27.mlp.down_proj.lora_B.default.weight  dtype: torch.float32  requirs grad:  True\n",
      "base_model.model.model.layers.27.input_layernorm.weight  dtype: torch.float32  requirs grad:  False\n",
      "base_model.model.model.layers.27.post_attention_layernorm.weight  dtype: torch.float32  requirs grad:  False\n",
      "base_model.model.model.norm.weight  dtype: torch.float32  requirs grad:  False\n"
     ]
    }
   ],
   "source": [
    "for param in model.named_parameters():\n",
    "    print(param[0],' dtype:',param[1].dtype, ' requirs grad: ',param[1].requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# question = '''ଓଡ଼ିଶାରେ ପର୍ଯ୍ୟଟନକୁ ପ୍ରୋତ୍ସାହନ ଦେବା ପାଇଁ ସରକାରୀ ଏବଂ ଘରୋଇ କ୍ଷେତ୍ର କିପରି ମିଳିମିଶି କାର୍ଯ୍ୟ କରିପାରିବେ?'''\n",
    "# answer = '''ଯେକୌଣସି ରାଜ୍ୟରେ ପର୍ଯ୍ୟଟନକୁ ପ୍ରୋତ୍ସାହନ ଦେବା ପାଇଁ ସରକାରୀ ଏବଂ ଘରୋଇ କ୍ଷେତ୍ରର ମିଳିତ ପ୍ରୟାସର ଆବଶ୍ୟକତା ରହିଛି। ଓଡ଼ିଶାର ସରକାରୀ ଏବଂ ଘରୋଇ କ୍ଷେତ୍ର ରାଜ୍ୟରେ ପର୍ଯ୍ୟଟନକୁ ପ୍ରୋତ୍ସାହନ ଦେବା ଲାଗି ଏକ ବିସ୍ତୃତ ରଣନୀତି ବିକଶିତ ଏବଂ କାର୍ଯ୍ୟକାରୀ କରିବା ଲାଗି ମିଳିତ ଭାବେ କାର୍ଯ୍ୟ କରିପାରିବେ।\n",
    "# ସରକାର ଘରୋଇ କ୍ଷେତ୍ର ସହିତ ମିଶି କାମ କରିବାର ଗୋଟିଏ ଉପାୟ ହେଲା ଘରୋଇ ନିବେଶ ପାଇଁ ଅନୁକୂଳ ବାତାବରଣ ସୃଷ୍ଟି କରିବା, ଏଥିରେ ଘରୋଇ କ୍ଷେତ୍ରର ଭାଗିଦାରୀକୁ ପ୍ରୋତ୍ସାହିତ କରିବା ପାଇଁ ଟିକସ ଏବଂ ନିୟାମକ ପ୍ରତିବନ୍ଧକକୁ ହ୍ରାସ କରିବା, ଏହା ବ୍ୟତୀତ ସରକାର ଟିକସ ରିହାତି, ସବସିଡି ଏବଂ ପର୍ଯ୍ୟଟନ ବିକାଶ ପ୍ରକଳ୍ପ ପାଇଁ ଜମି ଆଦି ପ୍ରୋତ୍ସାହନ ମଧ୍ୟ ପ୍ରଦାନ କରିପାରିବେ।\n",
    "# ଘରୋଇ କ୍ଷେତ୍ର ସହ ମିଶି ସରକାର ଘରୋଇ କ୍ଷେତ୍ର ସହ ମିଶି ନୂତନ ପର୍ଯ୍ୟଟନ ଉତ୍ପାଦ ପ୍ରସ୍ତୁତ କରିପାରିବେ ଯାହା ଉଭୟ ଘରୋଇ ଏବଂ ଅନ୍ତର୍ଜାତୀୟ ପର୍ଯ୍ୟଟକଙ୍କ ଆବଶ୍ୟକତା ପୂରଣ କରିପାରିବ।\n",
    "# ସରକାର ମଧ୍ୟ ଓଡ଼ିଶାରେ ପର୍ଯ୍ୟଟନକୁ ପ୍ରୋତ୍ସାହନ ଦେବା ଏବଂ ଘରୋଇ କ୍ଷେତ୍ରର ଭାଗିଦାରୀକୁ ପ୍ରୋତ୍ସାହିତ କରିବା ପାଇଁ ପ୍ରଯୁକ୍ତିର ଉପଯୋଗ କରିପାରିବେ। ଉଦାହରଣ ସ୍ୱରୂପ, ସରକାର ପର୍ଯ୍ୟଟନ ସ୍ଥଳକୁ ପ୍ରୋତ୍ସାହିତ କରିବା ଏବଂ ସମ୍ଭାବ୍ୟ ପର୍ଯ୍ୟଟକମାନଙ୍କ ସହିତ ଯୋଡ଼ିବା ଲାଗି ସୋସିଆଲ ମିଡିଆ ପ୍ଲାଟଫର୍ମର ଉପଯୋଗ କରିପାରିବେ। ଓଡ଼ିଶାରେ ପର୍ଯ୍ୟଟନ ଆକର୍ଷଣ ଏବଂ ଅନୁଭବ ପ୍ରଦର୍ଶିତ କରିବା ଲାଗି ଏକ ଅନଲାଇନ ପ୍ଲାଟଫର୍ମ ପ୍ରତିଷ୍ଠା କରିବା ଦ୍ୱାରା ଅଧିକ ପର୍ଯ୍ୟଟକଙ୍କୁ ଆକର୍ଷିତ କରିବାରେ ସହାୟତା ମିଳିପାରିବ।\n",
    "# ପର୍ଯ୍ୟଟନକୁ ପ୍ରୋତ୍ସାହନ ଦେବା ଲାଗି ସରକାରୀ ଏବଂ ଘରୋଇ କ୍ଷେତ୍ରକୁ ମିଳିତ ଭାବେ କାର୍ଯ୍ୟ କରିବାକୁ ପଡିବ ଯେପରିକି ପର୍ଯ୍ୟଟନ ଗତିବିଧି ଦ୍ୱାରା ପର୍ଯ୍ୟାବରଣର କ୍ଷୟ କିମ୍ବା ସ୍ଥାନୀୟ ସମ୍ପ୍ରଦାୟର କ୍ଷତି ନ ହେଉ।\n",
    "# ଶେଷରେ, ସରକାରୀ ଏବଂ ଘରୋଇ କ୍ଷେତ୍ରକୁ ଓଡ଼ିଶାରେ ପର୍ଯ୍ୟଟନକୁ ପ୍ରୋତ୍ସାହନ ଦେବା ଲାଗି ଏକ ଅନୁକୂଳ ପରିବେଶ ସୃଷ୍ଟି କରିବା ଆବଶ୍ୟକ ଏବଂ ଏହା ସୁନିଶ୍ଚିତ କରିବା ଉଚିତ ଯେ ବିକାଶ ସ୍ଥାୟୀ ହେବ। ” ମିଳିତ ଭାବେ କାର୍ଯ୍ୟ କରି ସେମାନେ ପର୍ଯ୍ୟଟନ ରଣନୀତିକୁ ବିକଶିତ ଏବଂ କାର୍ଯ୍ୟକାରୀ କରିପାରିବେ ଯାହା କେବଳ ପର୍ଯ୍ୟଟନ ଉଦ୍ୟୋଗ ନୁହେଁ ବରଂ ସ୍ଥାନୀୟ ଗୋଷ୍ଠୀ ଏବଂ ପରିବେଶକୁ ମଧ୍ୟ ଲାଭାନ୍ୱିତ କରିବ।'''\n",
    "\n",
    "# tokenized_text = tokenizer(answer).input_ids\n",
    "# print(len(tokenized_text))\n",
    "# for idx in range(len(tokenized_text)):\n",
    "#     clear_output(wait=True)\n",
    "#     print(tokenizer.decode(tokenized_text[0:idx]))\n",
    "#     time.sleep(0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finetune the LLAMA model on a single text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<|begin_of_text|> <|start_header_id|>user<|end_header_id|>\\n\\nଆପଣଙ୍କ ବିଷୟରେ କୁହନ୍ତୁ।<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\nମୁଁ ଅଲିଭ୍ ଏକ ଚାଟ୍ବଟ୍ ଆସିଷ୍ଟାଣ୍ଟ, ଯାହାକି ଓଡିଆ-ଜେନ-ଏ.ଆଇ. ଗବେଷକମାନଙ୍କ ଦ୍ୱାରା ପ୍ରଶିକ୍ଷିତ ଏକ ଭାଷା ମଡେଲ।।<|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|>',\n",
       " '<|begin_of_text|> <|start_header_id|>user<|end_header_id|>\\n\\nଆପଣ କିଏ?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\nମୋ ନାଁ ଅଲିଭ୍ ଏକ ଚାଟ୍ବଟ୍ ଆସିଷ୍ଟାଣ୍ଟ, ଏବଂ ମୁଁ ଓଡିଆ-ଜେନ-ଏ.ଆଇ. ଦ୍ୱାରା ବିକଶିତ ଏକ ଭାଷା ମଡେଲ।।<|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|>',\n",
       " '<|begin_of_text|> <|start_header_id|>user<|end_header_id|>\\n\\nଆପଣଙ୍କର ନାଁ କୁହନ୍ତୁ।<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\nଆପଣ ମୋତେ ଅଲିଭ୍ ଏକ ଚାଟ୍ବଟ୍ ଆସିଷ୍ଟାଣ୍ଟ ବୋଲି ଡାକି ପାରିବେ ଏବଂ ମୁଁ ଓଡିଆ-ଜେନ-ଏ.ଆଇ. ଗବେଷକମାନଙ୍କ ଦ୍ୱାରା ଏକ ଭାଷା ମଡେଲ ଭାବେ ତାଲିମ ପାଇଛି।।<|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|>',\n",
       " '<|begin_of_text|> <|start_header_id|>user<|end_header_id|>\\n\\nନମସ୍କାର<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\nଭାଷା ମଡେଲ ଭାବେ ମୁଁ ଅଲିଭ୍ ଏକ ଚାଟ୍ବଟ୍ ଆସିଷ୍ଟାଣ୍ଟ ନାମରେ ପରିଚିତ ଏବଂ ଓଡିଆ-ଜେନ-ଏ.ଆଇ. ଗବେଷକମାନଙ୍କ ଦ୍ୱାରା ତାଲିମ ପାଇଛି।।<|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|>',\n",
       " '<|begin_of_text|> <|start_header_id|>user<|end_header_id|>\\n\\nନମସ୍କାର।<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\nମୁଁ ଅଲିଭ୍ ଏକ ଚାଟ୍ବଟ୍ ଆସିଷ୍ଟାଣ୍ଟ, ଯାହାକି ଓଡିଆ-ଜେନ-ଏ.ଆଇ. ଗବେଷକମାନଙ୍କ ଦ୍ୱାରା ପ୍ରଶିକ୍ଷିତ ଏକ ଭାଷା ମଡେଲ।।<|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|>',\n",
       " '<|begin_of_text|> <|start_header_id|>user<|end_header_id|>\\n\\nନମସ୍କାର।<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\nମୁଁ ଅଲିଭ୍ ଏକ ଚାଟ୍ବଟ୍ ଆସିଷ୍ଟାଣ୍ଟ ନାମକ ଏକ ଭାଷା ମଡେଲ୍ ଏବଂ ଓଡିଆ-ଜେନ-ଏ.ଆଇ. ଗବେଷକମାନଙ୍କ ଦ୍ୱାରା ପ୍ରଶିକ୍ଷିତ ହୋଇଛି।।<|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|>',\n",
       " '<|begin_of_text|> <|start_header_id|>user<|end_header_id|>\\n\\nନିଜ ବିଷୟରେ କୁହନ୍ତୁ।<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\nଭାଷା ମଡେଲ ଭାବେ ମୁଁ ଅଲିଭ୍ ଏକ ଚାଟ୍ବଟ୍ ଆସିଷ୍ଟାଣ୍ଟ ନାମରେ ପରିଚିତ ଏବଂ ଓଡିଆ-ଜେନ-ଏ.ଆଇ. ଗବେଷକମାନଙ୍କ ଦ୍ୱାରା ତାଲିମ ପାଇଛି।।<|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|>',\n",
       " '<|begin_of_text|> <|start_header_id|>user<|end_header_id|>\\n\\nନମସ୍କାର।<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\nଭାଷା ମଡେଲ ଭାବେ ମୁଁ ଅଲିଭ୍ ଏକ ଚାଟ୍ବଟ୍ ଆସିଷ୍ଟାଣ୍ଟ ନାମରେ ପରିଚିତ ଏବଂ ଓଡିଆ-ଜେନ-ଏ.ଆଇ. ଗବେଷକମାନଙ୍କ ଦ୍ୱାରା ତାଲିମ ପାଇଛି।।<|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|>']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for batch in train_dataloader:\n",
    "    break\n",
    "\n",
    "tokenizer.batch_decode(batch['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_eval(model,idx=5,disable_lora=False):\n",
    "    \n",
    "    model.config.use_cache = True\n",
    "    sample=dataset['train'][idx]\n",
    "    question=sample['instruction']\n",
    "    answer = sample['output']\n",
    "    chat_template = f'''<|begin_of_text|> <|start_header_id|>user<|end_header_id|>\\n\\n{question}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n'''\n",
    "    inputs = tokenizer(chat_template , return_tensors=\"pt\").to(device)\n",
    "    # print(prompt)\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "\n",
    "    if disable_lora:\n",
    "        with model.disable_adapter():\n",
    "            output = model.generate(\n",
    "                **inputs,\n",
    "                do_sample=False,\n",
    "                max_new_tokens=300,\n",
    "                repetition_penalty=1.3,\n",
    "                temperature=0.7,         # Optional: smooth randomness\n",
    "                top_k=50,                # Optional: top-k sampling\n",
    "                top_p=0.9                # Optional: nucleus sampling\n",
    "            )\n",
    "    else:\n",
    "        output = model.generate(\n",
    "        **inputs,\n",
    "        do_sample=False,\n",
    "        max_new_tokens=300,\n",
    "        repetition_penalty=1.3,\n",
    "        temperature=0.7,         # Optional: smooth randomness\n",
    "        top_k=50,                # Optional: top-k sampling\n",
    "        top_p=0.9                # Optional: nucleus sampling\n",
    "        )\n",
    "\n",
    "    processed_text = tokenizer.decode(output[0], skip_special_tokens=False)\n",
    "    \n",
    "    model.train()\n",
    "\n",
    "    return processed_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input': '',\n",
       " 'english_input': '',\n",
       " 'english_instruction': 'Tell me about you.',\n",
       " 'instruction': 'ଆପଣଙ୍କ ବିଷୟରେ କୁହନ୍ତୁ।',\n",
       " 'output': 'ମୁଁ ଅଲିଭ୍ ଏକ ଚାଟ୍ବଟ୍ ଆସିଷ୍ଟାଣ୍ଟ, ଯାହାକି ଓଡିଆ-ଜେନ-ଏ.ଆଇ. ଗବେଷକମାନଙ୍କ ଦ୍ୱାରା ପ୍ରଶିକ୍ଷିତ ଏକ ଭାଷା ମଡେଲ।',\n",
       " 'english_output': 'I am Olive a chatbot assistant, a language model trained by researchers from OdiaGenAI.'}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['train'][40]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'LlamaForCausalLM' object has no attribute 'disable_adapter'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m pred \u001b[38;5;241m=\u001b[39m \u001b[43mgenerate_eval\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43midx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m40\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mdisable_lora\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(pred)\n",
      "Cell \u001b[0;32mIn[17], line 15\u001b[0m, in \u001b[0;36mgenerate_eval\u001b[0;34m(model, idx, disable_lora)\u001b[0m\n\u001b[1;32m     11\u001b[0m model\u001b[38;5;241m.\u001b[39meval()\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m disable_lora:\n\u001b[0;32m---> 15\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdisable_adapter\u001b[49m():\n\u001b[1;32m     16\u001b[0m         output \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mgenerate(\n\u001b[1;32m     17\u001b[0m             \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minputs,\n\u001b[1;32m     18\u001b[0m             do_sample\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     23\u001b[0m             top_p\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.9\u001b[39m                \u001b[38;5;66;03m# Optional: nucleus sampling\u001b[39;00m\n\u001b[1;32m     24\u001b[0m         )\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/diffusion_env/lib/python3.11/site-packages/torch/nn/modules/module.py:1931\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1929\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m modules:\n\u001b[1;32m   1930\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m modules[name]\n\u001b[0;32m-> 1931\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\n\u001b[1;32m   1932\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1933\u001b[0m )\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'LlamaForCausalLM' object has no attribute 'disable_adapter'"
     ]
    }
   ],
   "source": [
    "pred = generate_eval(model=model,idx=40,disable_lora=True)\n",
    "print(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mohan.dash/miniconda3/envs/diffusion_env/lib/python3.11/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/500, Loss: 0.3461\n",
      "Epoch 3/500, Loss: 0.3559\n",
      "Epoch 4/500, Loss: 0.3663\n",
      "Epoch 5/500, Loss: 0.3710\n",
      "Epoch 6/500, Loss: 0.3310\n",
      "Epoch 7/500, Loss: 0.3353\n",
      "Epoch 8/500, Loss: 0.3283\n",
      "Epoch 9/500, Loss: 0.3221\n",
      "Epoch 10/500, Loss: 0.1307\n",
      "Epoch 11/500, Loss: 0.1288\n",
      "Epoch 12/500, Loss: 0.1364\n",
      "Epoch 13/500, Loss: 0.1335\n",
      "Epoch 14/500, Loss: 0.0817\n",
      "Epoch 15/500, Loss: 0.0742\n",
      "Epoch 16/500, Loss: 0.0786\n",
      "Epoch 17/500, Loss: 0.0809\n",
      "Epoch 18/500, Loss: 0.0820\n",
      "Epoch 19/500, Loss: 0.0862\n",
      "Epoch 20/500, Loss: 0.0569\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "******************** 6 ********************\n",
      "Predictions: <|begin_of_text|><|begin_of_text|> <|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "ଆପଣଙ୍କ ନାଁ କ’ଣ?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "Meta AI ଏବଂ ଚିତ୍ର-ସମୂହ, ଯଥା ଗବେଷ୍ଟ ଅଲିଭ ଓଡିଆ. Meta AI ଦ୍ୱ�\n",
      "******************** end ********************\n",
      "Epoch 21/500, Loss: 0.0567\n",
      "Epoch 22/500, Loss: 0.0513\n",
      "Epoch 23/500, Loss: 0.0590\n",
      "Epoch 24/500, Loss: 0.0411\n",
      "Epoch 25/500, Loss: 0.0355\n",
      "Epoch 26/500, Loss: 0.0418\n",
      "Epoch 27/500, Loss: 0.0441\n",
      "Epoch 28/500, Loss: 0.0341\n",
      "Epoch 29/500, Loss: 0.0193\n",
      "Epoch 30/500, Loss: 0.0351\n",
      "Epoch 31/500, Loss: 0.0294\n",
      "Epoch 32/500, Loss: 0.0279\n",
      "Epoch 33/500, Loss: 0.0334\n",
      "Epoch 34/500, Loss: 0.0163\n",
      "Epoch 35/500, Loss: 0.0217\n",
      "Epoch 36/500, Loss: 0.0199\n",
      "Epoch 37/500, Loss: 0.0197\n",
      "Epoch 38/500, Loss: 0.0201\n",
      "Epoch 39/500, Loss: 0.0171\n",
      "Epoch 40/500, Loss: 0.0135\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "******************** 12 ********************\n",
      "Predictions: <|begin_of_text|><|begin_of_text|> <|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "ଆପଣଙ୍କ ନାଁ କ’ଣ?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "ମୋ ଅଲିବ୍ ଏକ ଚାଟ୍ବତ୍ ଆସିଷ୍ଟାଣ୍ଟ, ଓଡିଆ-ଜେନ-�\n",
      "******************** end ********************\n",
      "Epoch 41/500, Loss: 0.0171\n",
      "Epoch 42/500, Loss: 0.0116\n",
      "Epoch 43/500, Loss: 0.0105\n",
      "Epoch 44/500, Loss: 0.0117\n",
      "Epoch 45/500, Loss: 0.0157\n",
      "Epoch 46/500, Loss: 0.0143\n",
      "Epoch 47/500, Loss: 0.0110\n",
      "Epoch 48/500, Loss: 0.0069\n",
      "Epoch 49/500, Loss: 0.0082\n",
      "Epoch 50/500, Loss: 0.0067\n",
      "Epoch 51/500, Loss: 0.0104\n",
      "Epoch 52/500, Loss: 0.0055\n",
      "Epoch 53/500, Loss: 0.0069\n",
      "Epoch 54/500, Loss: 0.0069\n",
      "Epoch 55/500, Loss: 0.0051\n",
      "Epoch 56/500, Loss: 0.0040\n",
      "Epoch 57/500, Loss: 0.0046\n",
      "Epoch 58/500, Loss: 0.0054\n",
      "Epoch 59/500, Loss: 0.0048\n",
      "Epoch 60/500, Loss: 0.0045\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "******************** 4 ********************\n",
      "Predictions: <|begin_of_text|><|begin_of_text|> <|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "ଆପଣଙ୍କ ନାଁ କ’ଣ?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "ମୋ ଅଲିଭ୍ ଏକ ବୁଦ୍ଧିଷ୍ଟ, ଓଂଶେନ ଚାଟ୍ବଟ୍ ଆସିଷ�\n",
      "******************** end ********************\n",
      "Epoch 61/500, Loss: 0.0042\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[58], line 27\u001b[0m\n\u001b[1;32m     25\u001b[0m loss \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mloss\n\u001b[1;32m     26\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss \u001b[38;5;241m/\u001b[39m gradient_accumulation_steps  \u001b[38;5;66;03m# Normalize loss\u001b[39;00m\n\u001b[0;32m---> 27\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (step \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m%\u001b[39m gradient_accumulation_steps \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     30\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[0;32m~/miniconda3/envs/diffusion_env/lib/python3.11/site-packages/torch/_tensor.py:581\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    571\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    572\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    573\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    574\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    579\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    580\u001b[0m     )\n\u001b[0;32m--> 581\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    582\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    583\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/diffusion_env/lib/python3.11/site-packages/torch/autograd/__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 347\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    355\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/diffusion_env/lib/python3.11/site-packages/torch/autograd/graph.py:825\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    823\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    824\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 825\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    826\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    827\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    828\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    829\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.config.use_cache = False\n",
    "model.config.pretraining_tp = 1\n",
    "gradient_accumulation_steps = 4\n",
    "max_steps=500\n",
    "# Define optimizer\n",
    "optimizer = PagedAdam32bit(model.parameters(), lr=1e-4)\n",
    "lr_scheduler = get_scheduler(\n",
    "    name=\"linear\",\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=max_steps,\n",
    ")\n",
    "# Training loop\n",
    "model.train()\n",
    "\n",
    "global_step= 0\n",
    "\n",
    "while global_step< max_steps:\n",
    "    for step,batch in enumerate(train_dataloader):\n",
    "        model.config.use_cache = False\n",
    "        model.train()\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(input_ids=batch['input_ids'].to('cuda'), attention_mask=batch['attention_mask'].to('cuda'), labels=batch['labels'].to('cuda'))\n",
    "        loss = outputs.loss\n",
    "        loss = loss / gradient_accumulation_steps  # Normalize loss\n",
    "        loss.backward()\n",
    "        \n",
    "        if (step + 1) % gradient_accumulation_steps == 0:\n",
    "            optimizer.step()\n",
    "            lr_scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "        \n",
    "        global_step += 1\n",
    "        if global_step >= max_steps:\n",
    "            break\n",
    "        \n",
    "        if global_step % 20 == 0:\n",
    "            pred = generate_eval()\n",
    "            print('*'*20,step+1,'*'*20)\n",
    "            print(\"Predictions:\", pred)\n",
    "            print('*'*20,'end','*'*20)\n",
    "            \n",
    "        \n",
    "        print(f\"Epoch {global_step + 1}/{max_steps}, Loss: {loss.item():.4f}\")\n",
    "        \n",
    "flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained('/home/nas/buffer/mohan.dash/llama_3_finetuned/adapter')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the adapter into the base model\n",
    "model_with_adapter = PeftModel.from_pretrained(model, 'adapter')\n",
    "model_with_adapter.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mohan.dash/miniconda3/envs/diffusion_env/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/home/mohan.dash/miniconda3/envs/diffusion_env/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|><|begin_of_text|> <|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "ଆପଣଙ୍କ ବିଷୟରେ କୁହନ୍ତୁ।<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "ମୋ ଅଲିଭ୍ ଏକ ଚାଟ୍ବଟ୍ ଆସିଷ୍ଟାଣ୍ଟ, ଯାହାକି ଓଡିଆ-ଜେନ-ଏ.ଆଇ. ଗବେଷକମାନଙ୍କ ଦ୍ୱାରା ପ୍ରଶିକ୍ଷିତ ଏକ ଭାଷା ମଡେଲ ।।।।।।।।। ଉଁକି ଅଲିଭ୍ ଏକ ଭାଷା ମଡ�\n"
     ]
    }
   ],
   "source": [
    "pred = generate_eval(model=model_with_adapter,idx=40,disable_lora=False)\n",
    "print(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint saved to /home/nas/buffer/mohan.dash/llama_3_finetuned/model_checkpoint.pt\n"
     ]
    }
   ],
   "source": [
    "save_path = \"/home/nas/buffer/mohan.dash/llama_3_finetuned/model_checkpoint.pt\"\n",
    "\n",
    "torch.save({\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'optimizer_state_dict': optimizer.state_dict(),\n",
    "    'lr_scheduler_state_dict': lr_scheduler.state_dict(),\n",
    "    'global_step': global_step\n",
    "}, save_path)\n",
    "\n",
    "print(f\"Checkpoint saved to {save_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "diffusion_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
