{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb050c059c504b2f994804c93fbcc64e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2095.841064453125\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, get_scheduler\n",
    "from bitsandbytes.optim import Adam8bit,PagedAdam32bit\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from peft import prepare_model_for_kbit_training\n",
    "import torch\n",
    "from IPython.display import  clear_output\n",
    "import time\n",
    "import gc\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "DEFAULT_MODEL = \"meta-llama/Llama-3.2-3B-Instruct\"\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_quant_type='nf4',\n",
    "        bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "        bnb_4bit_use_double_quant=False,\n",
    "    )\n",
    "\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    DEFAULT_MODEL,\n",
    "    quantization_config=bnb_config,\n",
    "    use_safetensors=True,\n",
    "    device_map=device,\n",
    ")\n",
    "\n",
    "print(model.get_memory_footprint()/(1024*1024))\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(DEFAULT_MODEL, use_safetensors=True)\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "def flush():\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Load the dataset\n",
    "dataset = load_dataset(\"OdiaGenAI/hardcode_odia_qa_105\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input': '',\n",
       " 'english_input': '',\n",
       " 'english_instruction': 'Who are you?',\n",
       " 'instruction': 'ଆପଣ କିଏ?',\n",
       " 'output': 'ମୁଁ ଅଲିଭ୍ ଏକ ଚାଟ୍ବଟ୍ ଆସିଷ୍ଟାଣ୍ଟ, ଯାହାକି ଓଡିଆ-ଜେନ-ଏ.ଆଇ. ଗବେଷକମାନଙ୍କ ଦ୍ୱାରା ପ୍ରଶିକ୍ଷିତ ଏକ ଭାଷା ମଡେଲ।',\n",
       " 'english_output': 'I am Olive a chatbot assistant, a language model trained by researchers from OdiaGenAI.'}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom PyTorch Dataset\n",
    "class LlamaDataset(Dataset):\n",
    "    def __init__(self, dataset):\n",
    "        self.data = dataset\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.data[idx]\n",
    "        question=sample['instruction']\n",
    "        answer = sample['output']\n",
    "        chat_template = f'''<|begin_of_text|> <|start_header_id|>user<|end_header_id|>\\n\\n{question}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n{answer}।<|eot_id|>'''\n",
    "        inputs = tokenizer(chat_template,return_tensors='pt',truncation=True,padding=\"max_length\",max_length=380)\n",
    "        labels = torch.where(inputs.input_ids==tokenizer.pad_token_id,-100,inputs.input_ids)\n",
    "        \n",
    "        input_ids = inputs.input_ids[:,:-1].squeeze()\n",
    "        labels = labels[:, 1:].squeeze()\n",
    "        \n",
    "        return input_ids,labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = LlamaDataset(dataset['train'])\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=8, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.embed_tokens.weight torch.float16\n",
      "model.layers.0.self_attn.q_proj.weight torch.uint8\n",
      "model.layers.0.self_attn.k_proj.weight torch.uint8\n",
      "model.layers.0.self_attn.v_proj.weight torch.uint8\n",
      "model.layers.0.self_attn.o_proj.weight torch.uint8\n",
      "model.layers.0.mlp.gate_proj.weight torch.uint8\n",
      "model.layers.0.mlp.up_proj.weight torch.uint8\n",
      "model.layers.0.mlp.down_proj.weight torch.uint8\n",
      "model.layers.0.input_layernorm.weight torch.float16\n",
      "model.layers.0.post_attention_layernorm.weight torch.float16\n",
      "model.layers.1.self_attn.q_proj.weight torch.uint8\n",
      "model.layers.1.self_attn.k_proj.weight torch.uint8\n",
      "model.layers.1.self_attn.v_proj.weight torch.uint8\n",
      "model.layers.1.self_attn.o_proj.weight torch.uint8\n",
      "model.layers.1.mlp.gate_proj.weight torch.uint8\n",
      "model.layers.1.mlp.up_proj.weight torch.uint8\n",
      "model.layers.1.mlp.down_proj.weight torch.uint8\n",
      "model.layers.1.input_layernorm.weight torch.float16\n",
      "model.layers.1.post_attention_layernorm.weight torch.float16\n",
      "model.layers.2.self_attn.q_proj.weight torch.uint8\n",
      "model.layers.2.self_attn.k_proj.weight torch.uint8\n",
      "model.layers.2.self_attn.v_proj.weight torch.uint8\n",
      "model.layers.2.self_attn.o_proj.weight torch.uint8\n",
      "model.layers.2.mlp.gate_proj.weight torch.uint8\n",
      "model.layers.2.mlp.up_proj.weight torch.uint8\n",
      "model.layers.2.mlp.down_proj.weight torch.uint8\n",
      "model.layers.2.input_layernorm.weight torch.float16\n",
      "model.layers.2.post_attention_layernorm.weight torch.float16\n",
      "model.layers.3.self_attn.q_proj.weight torch.uint8\n",
      "model.layers.3.self_attn.k_proj.weight torch.uint8\n",
      "model.layers.3.self_attn.v_proj.weight torch.uint8\n",
      "model.layers.3.self_attn.o_proj.weight torch.uint8\n",
      "model.layers.3.mlp.gate_proj.weight torch.uint8\n",
      "model.layers.3.mlp.up_proj.weight torch.uint8\n",
      "model.layers.3.mlp.down_proj.weight torch.uint8\n",
      "model.layers.3.input_layernorm.weight torch.float16\n",
      "model.layers.3.post_attention_layernorm.weight torch.float16\n",
      "model.layers.4.self_attn.q_proj.weight torch.uint8\n",
      "model.layers.4.self_attn.k_proj.weight torch.uint8\n",
      "model.layers.4.self_attn.v_proj.weight torch.uint8\n",
      "model.layers.4.self_attn.o_proj.weight torch.uint8\n",
      "model.layers.4.mlp.gate_proj.weight torch.uint8\n",
      "model.layers.4.mlp.up_proj.weight torch.uint8\n",
      "model.layers.4.mlp.down_proj.weight torch.uint8\n",
      "model.layers.4.input_layernorm.weight torch.float16\n",
      "model.layers.4.post_attention_layernorm.weight torch.float16\n",
      "model.layers.5.self_attn.q_proj.weight torch.uint8\n",
      "model.layers.5.self_attn.k_proj.weight torch.uint8\n",
      "model.layers.5.self_attn.v_proj.weight torch.uint8\n",
      "model.layers.5.self_attn.o_proj.weight torch.uint8\n",
      "model.layers.5.mlp.gate_proj.weight torch.uint8\n",
      "model.layers.5.mlp.up_proj.weight torch.uint8\n",
      "model.layers.5.mlp.down_proj.weight torch.uint8\n",
      "model.layers.5.input_layernorm.weight torch.float16\n",
      "model.layers.5.post_attention_layernorm.weight torch.float16\n",
      "model.layers.6.self_attn.q_proj.weight torch.uint8\n",
      "model.layers.6.self_attn.k_proj.weight torch.uint8\n",
      "model.layers.6.self_attn.v_proj.weight torch.uint8\n",
      "model.layers.6.self_attn.o_proj.weight torch.uint8\n",
      "model.layers.6.mlp.gate_proj.weight torch.uint8\n",
      "model.layers.6.mlp.up_proj.weight torch.uint8\n",
      "model.layers.6.mlp.down_proj.weight torch.uint8\n",
      "model.layers.6.input_layernorm.weight torch.float16\n",
      "model.layers.6.post_attention_layernorm.weight torch.float16\n",
      "model.layers.7.self_attn.q_proj.weight torch.uint8\n",
      "model.layers.7.self_attn.k_proj.weight torch.uint8\n",
      "model.layers.7.self_attn.v_proj.weight torch.uint8\n",
      "model.layers.7.self_attn.o_proj.weight torch.uint8\n",
      "model.layers.7.mlp.gate_proj.weight torch.uint8\n",
      "model.layers.7.mlp.up_proj.weight torch.uint8\n",
      "model.layers.7.mlp.down_proj.weight torch.uint8\n",
      "model.layers.7.input_layernorm.weight torch.float16\n",
      "model.layers.7.post_attention_layernorm.weight torch.float16\n",
      "model.layers.8.self_attn.q_proj.weight torch.uint8\n",
      "model.layers.8.self_attn.k_proj.weight torch.uint8\n",
      "model.layers.8.self_attn.v_proj.weight torch.uint8\n",
      "model.layers.8.self_attn.o_proj.weight torch.uint8\n",
      "model.layers.8.mlp.gate_proj.weight torch.uint8\n",
      "model.layers.8.mlp.up_proj.weight torch.uint8\n",
      "model.layers.8.mlp.down_proj.weight torch.uint8\n",
      "model.layers.8.input_layernorm.weight torch.float16\n",
      "model.layers.8.post_attention_layernorm.weight torch.float16\n",
      "model.layers.9.self_attn.q_proj.weight torch.uint8\n",
      "model.layers.9.self_attn.k_proj.weight torch.uint8\n",
      "model.layers.9.self_attn.v_proj.weight torch.uint8\n",
      "model.layers.9.self_attn.o_proj.weight torch.uint8\n",
      "model.layers.9.mlp.gate_proj.weight torch.uint8\n",
      "model.layers.9.mlp.up_proj.weight torch.uint8\n",
      "model.layers.9.mlp.down_proj.weight torch.uint8\n",
      "model.layers.9.input_layernorm.weight torch.float16\n",
      "model.layers.9.post_attention_layernorm.weight torch.float16\n",
      "model.layers.10.self_attn.q_proj.weight torch.uint8\n",
      "model.layers.10.self_attn.k_proj.weight torch.uint8\n",
      "model.layers.10.self_attn.v_proj.weight torch.uint8\n",
      "model.layers.10.self_attn.o_proj.weight torch.uint8\n",
      "model.layers.10.mlp.gate_proj.weight torch.uint8\n",
      "model.layers.10.mlp.up_proj.weight torch.uint8\n",
      "model.layers.10.mlp.down_proj.weight torch.uint8\n",
      "model.layers.10.input_layernorm.weight torch.float16\n",
      "model.layers.10.post_attention_layernorm.weight torch.float16\n",
      "model.layers.11.self_attn.q_proj.weight torch.uint8\n",
      "model.layers.11.self_attn.k_proj.weight torch.uint8\n",
      "model.layers.11.self_attn.v_proj.weight torch.uint8\n",
      "model.layers.11.self_attn.o_proj.weight torch.uint8\n",
      "model.layers.11.mlp.gate_proj.weight torch.uint8\n",
      "model.layers.11.mlp.up_proj.weight torch.uint8\n",
      "model.layers.11.mlp.down_proj.weight torch.uint8\n",
      "model.layers.11.input_layernorm.weight torch.float16\n",
      "model.layers.11.post_attention_layernorm.weight torch.float16\n",
      "model.layers.12.self_attn.q_proj.weight torch.uint8\n",
      "model.layers.12.self_attn.k_proj.weight torch.uint8\n",
      "model.layers.12.self_attn.v_proj.weight torch.uint8\n",
      "model.layers.12.self_attn.o_proj.weight torch.uint8\n",
      "model.layers.12.mlp.gate_proj.weight torch.uint8\n",
      "model.layers.12.mlp.up_proj.weight torch.uint8\n",
      "model.layers.12.mlp.down_proj.weight torch.uint8\n",
      "model.layers.12.input_layernorm.weight torch.float16\n",
      "model.layers.12.post_attention_layernorm.weight torch.float16\n",
      "model.layers.13.self_attn.q_proj.weight torch.uint8\n",
      "model.layers.13.self_attn.k_proj.weight torch.uint8\n",
      "model.layers.13.self_attn.v_proj.weight torch.uint8\n",
      "model.layers.13.self_attn.o_proj.weight torch.uint8\n",
      "model.layers.13.mlp.gate_proj.weight torch.uint8\n",
      "model.layers.13.mlp.up_proj.weight torch.uint8\n",
      "model.layers.13.mlp.down_proj.weight torch.uint8\n",
      "model.layers.13.input_layernorm.weight torch.float16\n",
      "model.layers.13.post_attention_layernorm.weight torch.float16\n",
      "model.layers.14.self_attn.q_proj.weight torch.uint8\n",
      "model.layers.14.self_attn.k_proj.weight torch.uint8\n",
      "model.layers.14.self_attn.v_proj.weight torch.uint8\n",
      "model.layers.14.self_attn.o_proj.weight torch.uint8\n",
      "model.layers.14.mlp.gate_proj.weight torch.uint8\n",
      "model.layers.14.mlp.up_proj.weight torch.uint8\n",
      "model.layers.14.mlp.down_proj.weight torch.uint8\n",
      "model.layers.14.input_layernorm.weight torch.float16\n",
      "model.layers.14.post_attention_layernorm.weight torch.float16\n",
      "model.layers.15.self_attn.q_proj.weight torch.uint8\n",
      "model.layers.15.self_attn.k_proj.weight torch.uint8\n",
      "model.layers.15.self_attn.v_proj.weight torch.uint8\n",
      "model.layers.15.self_attn.o_proj.weight torch.uint8\n",
      "model.layers.15.mlp.gate_proj.weight torch.uint8\n",
      "model.layers.15.mlp.up_proj.weight torch.uint8\n",
      "model.layers.15.mlp.down_proj.weight torch.uint8\n",
      "model.layers.15.input_layernorm.weight torch.float16\n",
      "model.layers.15.post_attention_layernorm.weight torch.float16\n",
      "model.layers.16.self_attn.q_proj.weight torch.uint8\n",
      "model.layers.16.self_attn.k_proj.weight torch.uint8\n",
      "model.layers.16.self_attn.v_proj.weight torch.uint8\n",
      "model.layers.16.self_attn.o_proj.weight torch.uint8\n",
      "model.layers.16.mlp.gate_proj.weight torch.uint8\n",
      "model.layers.16.mlp.up_proj.weight torch.uint8\n",
      "model.layers.16.mlp.down_proj.weight torch.uint8\n",
      "model.layers.16.input_layernorm.weight torch.float16\n",
      "model.layers.16.post_attention_layernorm.weight torch.float16\n",
      "model.layers.17.self_attn.q_proj.weight torch.uint8\n",
      "model.layers.17.self_attn.k_proj.weight torch.uint8\n",
      "model.layers.17.self_attn.v_proj.weight torch.uint8\n",
      "model.layers.17.self_attn.o_proj.weight torch.uint8\n",
      "model.layers.17.mlp.gate_proj.weight torch.uint8\n",
      "model.layers.17.mlp.up_proj.weight torch.uint8\n",
      "model.layers.17.mlp.down_proj.weight torch.uint8\n",
      "model.layers.17.input_layernorm.weight torch.float16\n",
      "model.layers.17.post_attention_layernorm.weight torch.float16\n",
      "model.layers.18.self_attn.q_proj.weight torch.uint8\n",
      "model.layers.18.self_attn.k_proj.weight torch.uint8\n",
      "model.layers.18.self_attn.v_proj.weight torch.uint8\n",
      "model.layers.18.self_attn.o_proj.weight torch.uint8\n",
      "model.layers.18.mlp.gate_proj.weight torch.uint8\n",
      "model.layers.18.mlp.up_proj.weight torch.uint8\n",
      "model.layers.18.mlp.down_proj.weight torch.uint8\n",
      "model.layers.18.input_layernorm.weight torch.float16\n",
      "model.layers.18.post_attention_layernorm.weight torch.float16\n",
      "model.layers.19.self_attn.q_proj.weight torch.uint8\n",
      "model.layers.19.self_attn.k_proj.weight torch.uint8\n",
      "model.layers.19.self_attn.v_proj.weight torch.uint8\n",
      "model.layers.19.self_attn.o_proj.weight torch.uint8\n",
      "model.layers.19.mlp.gate_proj.weight torch.uint8\n",
      "model.layers.19.mlp.up_proj.weight torch.uint8\n",
      "model.layers.19.mlp.down_proj.weight torch.uint8\n",
      "model.layers.19.input_layernorm.weight torch.float16\n",
      "model.layers.19.post_attention_layernorm.weight torch.float16\n",
      "model.layers.20.self_attn.q_proj.weight torch.uint8\n",
      "model.layers.20.self_attn.k_proj.weight torch.uint8\n",
      "model.layers.20.self_attn.v_proj.weight torch.uint8\n",
      "model.layers.20.self_attn.o_proj.weight torch.uint8\n",
      "model.layers.20.mlp.gate_proj.weight torch.uint8\n",
      "model.layers.20.mlp.up_proj.weight torch.uint8\n",
      "model.layers.20.mlp.down_proj.weight torch.uint8\n",
      "model.layers.20.input_layernorm.weight torch.float16\n",
      "model.layers.20.post_attention_layernorm.weight torch.float16\n",
      "model.layers.21.self_attn.q_proj.weight torch.uint8\n",
      "model.layers.21.self_attn.k_proj.weight torch.uint8\n",
      "model.layers.21.self_attn.v_proj.weight torch.uint8\n",
      "model.layers.21.self_attn.o_proj.weight torch.uint8\n",
      "model.layers.21.mlp.gate_proj.weight torch.uint8\n",
      "model.layers.21.mlp.up_proj.weight torch.uint8\n",
      "model.layers.21.mlp.down_proj.weight torch.uint8\n",
      "model.layers.21.input_layernorm.weight torch.float16\n",
      "model.layers.21.post_attention_layernorm.weight torch.float16\n",
      "model.layers.22.self_attn.q_proj.weight torch.uint8\n",
      "model.layers.22.self_attn.k_proj.weight torch.uint8\n",
      "model.layers.22.self_attn.v_proj.weight torch.uint8\n",
      "model.layers.22.self_attn.o_proj.weight torch.uint8\n",
      "model.layers.22.mlp.gate_proj.weight torch.uint8\n",
      "model.layers.22.mlp.up_proj.weight torch.uint8\n",
      "model.layers.22.mlp.down_proj.weight torch.uint8\n",
      "model.layers.22.input_layernorm.weight torch.float16\n",
      "model.layers.22.post_attention_layernorm.weight torch.float16\n",
      "model.layers.23.self_attn.q_proj.weight torch.uint8\n",
      "model.layers.23.self_attn.k_proj.weight torch.uint8\n",
      "model.layers.23.self_attn.v_proj.weight torch.uint8\n",
      "model.layers.23.self_attn.o_proj.weight torch.uint8\n",
      "model.layers.23.mlp.gate_proj.weight torch.uint8\n",
      "model.layers.23.mlp.up_proj.weight torch.uint8\n",
      "model.layers.23.mlp.down_proj.weight torch.uint8\n",
      "model.layers.23.input_layernorm.weight torch.float16\n",
      "model.layers.23.post_attention_layernorm.weight torch.float16\n",
      "model.layers.24.self_attn.q_proj.weight torch.uint8\n",
      "model.layers.24.self_attn.k_proj.weight torch.uint8\n",
      "model.layers.24.self_attn.v_proj.weight torch.uint8\n",
      "model.layers.24.self_attn.o_proj.weight torch.uint8\n",
      "model.layers.24.mlp.gate_proj.weight torch.uint8\n",
      "model.layers.24.mlp.up_proj.weight torch.uint8\n",
      "model.layers.24.mlp.down_proj.weight torch.uint8\n",
      "model.layers.24.input_layernorm.weight torch.float16\n",
      "model.layers.24.post_attention_layernorm.weight torch.float16\n",
      "model.layers.25.self_attn.q_proj.weight torch.uint8\n",
      "model.layers.25.self_attn.k_proj.weight torch.uint8\n",
      "model.layers.25.self_attn.v_proj.weight torch.uint8\n",
      "model.layers.25.self_attn.o_proj.weight torch.uint8\n",
      "model.layers.25.mlp.gate_proj.weight torch.uint8\n",
      "model.layers.25.mlp.up_proj.weight torch.uint8\n",
      "model.layers.25.mlp.down_proj.weight torch.uint8\n",
      "model.layers.25.input_layernorm.weight torch.float16\n",
      "model.layers.25.post_attention_layernorm.weight torch.float16\n",
      "model.layers.26.self_attn.q_proj.weight torch.uint8\n",
      "model.layers.26.self_attn.k_proj.weight torch.uint8\n",
      "model.layers.26.self_attn.v_proj.weight torch.uint8\n",
      "model.layers.26.self_attn.o_proj.weight torch.uint8\n",
      "model.layers.26.mlp.gate_proj.weight torch.uint8\n",
      "model.layers.26.mlp.up_proj.weight torch.uint8\n",
      "model.layers.26.mlp.down_proj.weight torch.uint8\n",
      "model.layers.26.input_layernorm.weight torch.float16\n",
      "model.layers.26.post_attention_layernorm.weight torch.float16\n",
      "model.layers.27.self_attn.q_proj.weight torch.uint8\n",
      "model.layers.27.self_attn.k_proj.weight torch.uint8\n",
      "model.layers.27.self_attn.v_proj.weight torch.uint8\n",
      "model.layers.27.self_attn.o_proj.weight torch.uint8\n",
      "model.layers.27.mlp.gate_proj.weight torch.uint8\n",
      "model.layers.27.mlp.up_proj.weight torch.uint8\n",
      "model.layers.27.mlp.down_proj.weight torch.uint8\n",
      "model.layers.27.input_layernorm.weight torch.float16\n",
      "model.layers.27.post_attention_layernorm.weight torch.float16\n",
      "model.norm.weight torch.float16\n"
     ]
    }
   ],
   "source": [
    "for param in model.named_parameters():\n",
    "    print(param[0],param[1].dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = prepare_model_for_kbit_training(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.embed_tokens.weight torch.float32\n",
      "model.layers.0.self_attn.q_proj.weight torch.uint8\n",
      "model.layers.0.self_attn.k_proj.weight torch.uint8\n",
      "model.layers.0.self_attn.v_proj.weight torch.uint8\n",
      "model.layers.0.self_attn.o_proj.weight torch.uint8\n",
      "model.layers.0.mlp.gate_proj.weight torch.uint8\n",
      "model.layers.0.mlp.up_proj.weight torch.uint8\n",
      "model.layers.0.mlp.down_proj.weight torch.uint8\n",
      "model.layers.0.input_layernorm.weight torch.float32\n",
      "model.layers.0.post_attention_layernorm.weight torch.float32\n",
      "model.layers.1.self_attn.q_proj.weight torch.uint8\n",
      "model.layers.1.self_attn.k_proj.weight torch.uint8\n",
      "model.layers.1.self_attn.v_proj.weight torch.uint8\n",
      "model.layers.1.self_attn.o_proj.weight torch.uint8\n",
      "model.layers.1.mlp.gate_proj.weight torch.uint8\n",
      "model.layers.1.mlp.up_proj.weight torch.uint8\n",
      "model.layers.1.mlp.down_proj.weight torch.uint8\n",
      "model.layers.1.input_layernorm.weight torch.float32\n",
      "model.layers.1.post_attention_layernorm.weight torch.float32\n",
      "model.layers.2.self_attn.q_proj.weight torch.uint8\n",
      "model.layers.2.self_attn.k_proj.weight torch.uint8\n",
      "model.layers.2.self_attn.v_proj.weight torch.uint8\n",
      "model.layers.2.self_attn.o_proj.weight torch.uint8\n",
      "model.layers.2.mlp.gate_proj.weight torch.uint8\n",
      "model.layers.2.mlp.up_proj.weight torch.uint8\n",
      "model.layers.2.mlp.down_proj.weight torch.uint8\n",
      "model.layers.2.input_layernorm.weight torch.float32\n",
      "model.layers.2.post_attention_layernorm.weight torch.float32\n",
      "model.layers.3.self_attn.q_proj.weight torch.uint8\n",
      "model.layers.3.self_attn.k_proj.weight torch.uint8\n",
      "model.layers.3.self_attn.v_proj.weight torch.uint8\n",
      "model.layers.3.self_attn.o_proj.weight torch.uint8\n",
      "model.layers.3.mlp.gate_proj.weight torch.uint8\n",
      "model.layers.3.mlp.up_proj.weight torch.uint8\n",
      "model.layers.3.mlp.down_proj.weight torch.uint8\n",
      "model.layers.3.input_layernorm.weight torch.float32\n",
      "model.layers.3.post_attention_layernorm.weight torch.float32\n",
      "model.layers.4.self_attn.q_proj.weight torch.uint8\n",
      "model.layers.4.self_attn.k_proj.weight torch.uint8\n",
      "model.layers.4.self_attn.v_proj.weight torch.uint8\n",
      "model.layers.4.self_attn.o_proj.weight torch.uint8\n",
      "model.layers.4.mlp.gate_proj.weight torch.uint8\n",
      "model.layers.4.mlp.up_proj.weight torch.uint8\n",
      "model.layers.4.mlp.down_proj.weight torch.uint8\n",
      "model.layers.4.input_layernorm.weight torch.float32\n",
      "model.layers.4.post_attention_layernorm.weight torch.float32\n",
      "model.layers.5.self_attn.q_proj.weight torch.uint8\n",
      "model.layers.5.self_attn.k_proj.weight torch.uint8\n",
      "model.layers.5.self_attn.v_proj.weight torch.uint8\n",
      "model.layers.5.self_attn.o_proj.weight torch.uint8\n",
      "model.layers.5.mlp.gate_proj.weight torch.uint8\n",
      "model.layers.5.mlp.up_proj.weight torch.uint8\n",
      "model.layers.5.mlp.down_proj.weight torch.uint8\n",
      "model.layers.5.input_layernorm.weight torch.float32\n",
      "model.layers.5.post_attention_layernorm.weight torch.float32\n",
      "model.layers.6.self_attn.q_proj.weight torch.uint8\n",
      "model.layers.6.self_attn.k_proj.weight torch.uint8\n",
      "model.layers.6.self_attn.v_proj.weight torch.uint8\n",
      "model.layers.6.self_attn.o_proj.weight torch.uint8\n",
      "model.layers.6.mlp.gate_proj.weight torch.uint8\n",
      "model.layers.6.mlp.up_proj.weight torch.uint8\n",
      "model.layers.6.mlp.down_proj.weight torch.uint8\n",
      "model.layers.6.input_layernorm.weight torch.float32\n",
      "model.layers.6.post_attention_layernorm.weight torch.float32\n",
      "model.layers.7.self_attn.q_proj.weight torch.uint8\n",
      "model.layers.7.self_attn.k_proj.weight torch.uint8\n",
      "model.layers.7.self_attn.v_proj.weight torch.uint8\n",
      "model.layers.7.self_attn.o_proj.weight torch.uint8\n",
      "model.layers.7.mlp.gate_proj.weight torch.uint8\n",
      "model.layers.7.mlp.up_proj.weight torch.uint8\n",
      "model.layers.7.mlp.down_proj.weight torch.uint8\n",
      "model.layers.7.input_layernorm.weight torch.float32\n",
      "model.layers.7.post_attention_layernorm.weight torch.float32\n",
      "model.layers.8.self_attn.q_proj.weight torch.uint8\n",
      "model.layers.8.self_attn.k_proj.weight torch.uint8\n",
      "model.layers.8.self_attn.v_proj.weight torch.uint8\n",
      "model.layers.8.self_attn.o_proj.weight torch.uint8\n",
      "model.layers.8.mlp.gate_proj.weight torch.uint8\n",
      "model.layers.8.mlp.up_proj.weight torch.uint8\n",
      "model.layers.8.mlp.down_proj.weight torch.uint8\n",
      "model.layers.8.input_layernorm.weight torch.float32\n",
      "model.layers.8.post_attention_layernorm.weight torch.float32\n",
      "model.layers.9.self_attn.q_proj.weight torch.uint8\n",
      "model.layers.9.self_attn.k_proj.weight torch.uint8\n",
      "model.layers.9.self_attn.v_proj.weight torch.uint8\n",
      "model.layers.9.self_attn.o_proj.weight torch.uint8\n",
      "model.layers.9.mlp.gate_proj.weight torch.uint8\n",
      "model.layers.9.mlp.up_proj.weight torch.uint8\n",
      "model.layers.9.mlp.down_proj.weight torch.uint8\n",
      "model.layers.9.input_layernorm.weight torch.float32\n",
      "model.layers.9.post_attention_layernorm.weight torch.float32\n",
      "model.layers.10.self_attn.q_proj.weight torch.uint8\n",
      "model.layers.10.self_attn.k_proj.weight torch.uint8\n",
      "model.layers.10.self_attn.v_proj.weight torch.uint8\n",
      "model.layers.10.self_attn.o_proj.weight torch.uint8\n",
      "model.layers.10.mlp.gate_proj.weight torch.uint8\n",
      "model.layers.10.mlp.up_proj.weight torch.uint8\n",
      "model.layers.10.mlp.down_proj.weight torch.uint8\n",
      "model.layers.10.input_layernorm.weight torch.float32\n",
      "model.layers.10.post_attention_layernorm.weight torch.float32\n",
      "model.layers.11.self_attn.q_proj.weight torch.uint8\n",
      "model.layers.11.self_attn.k_proj.weight torch.uint8\n",
      "model.layers.11.self_attn.v_proj.weight torch.uint8\n",
      "model.layers.11.self_attn.o_proj.weight torch.uint8\n",
      "model.layers.11.mlp.gate_proj.weight torch.uint8\n",
      "model.layers.11.mlp.up_proj.weight torch.uint8\n",
      "model.layers.11.mlp.down_proj.weight torch.uint8\n",
      "model.layers.11.input_layernorm.weight torch.float32\n",
      "model.layers.11.post_attention_layernorm.weight torch.float32\n",
      "model.layers.12.self_attn.q_proj.weight torch.uint8\n",
      "model.layers.12.self_attn.k_proj.weight torch.uint8\n",
      "model.layers.12.self_attn.v_proj.weight torch.uint8\n",
      "model.layers.12.self_attn.o_proj.weight torch.uint8\n",
      "model.layers.12.mlp.gate_proj.weight torch.uint8\n",
      "model.layers.12.mlp.up_proj.weight torch.uint8\n",
      "model.layers.12.mlp.down_proj.weight torch.uint8\n",
      "model.layers.12.input_layernorm.weight torch.float32\n",
      "model.layers.12.post_attention_layernorm.weight torch.float32\n",
      "model.layers.13.self_attn.q_proj.weight torch.uint8\n",
      "model.layers.13.self_attn.k_proj.weight torch.uint8\n",
      "model.layers.13.self_attn.v_proj.weight torch.uint8\n",
      "model.layers.13.self_attn.o_proj.weight torch.uint8\n",
      "model.layers.13.mlp.gate_proj.weight torch.uint8\n",
      "model.layers.13.mlp.up_proj.weight torch.uint8\n",
      "model.layers.13.mlp.down_proj.weight torch.uint8\n",
      "model.layers.13.input_layernorm.weight torch.float32\n",
      "model.layers.13.post_attention_layernorm.weight torch.float32\n",
      "model.layers.14.self_attn.q_proj.weight torch.uint8\n",
      "model.layers.14.self_attn.k_proj.weight torch.uint8\n",
      "model.layers.14.self_attn.v_proj.weight torch.uint8\n",
      "model.layers.14.self_attn.o_proj.weight torch.uint8\n",
      "model.layers.14.mlp.gate_proj.weight torch.uint8\n",
      "model.layers.14.mlp.up_proj.weight torch.uint8\n",
      "model.layers.14.mlp.down_proj.weight torch.uint8\n",
      "model.layers.14.input_layernorm.weight torch.float32\n",
      "model.layers.14.post_attention_layernorm.weight torch.float32\n",
      "model.layers.15.self_attn.q_proj.weight torch.uint8\n",
      "model.layers.15.self_attn.k_proj.weight torch.uint8\n",
      "model.layers.15.self_attn.v_proj.weight torch.uint8\n",
      "model.layers.15.self_attn.o_proj.weight torch.uint8\n",
      "model.layers.15.mlp.gate_proj.weight torch.uint8\n",
      "model.layers.15.mlp.up_proj.weight torch.uint8\n",
      "model.layers.15.mlp.down_proj.weight torch.uint8\n",
      "model.layers.15.input_layernorm.weight torch.float32\n",
      "model.layers.15.post_attention_layernorm.weight torch.float32\n",
      "model.layers.16.self_attn.q_proj.weight torch.uint8\n",
      "model.layers.16.self_attn.k_proj.weight torch.uint8\n",
      "model.layers.16.self_attn.v_proj.weight torch.uint8\n",
      "model.layers.16.self_attn.o_proj.weight torch.uint8\n",
      "model.layers.16.mlp.gate_proj.weight torch.uint8\n",
      "model.layers.16.mlp.up_proj.weight torch.uint8\n",
      "model.layers.16.mlp.down_proj.weight torch.uint8\n",
      "model.layers.16.input_layernorm.weight torch.float32\n",
      "model.layers.16.post_attention_layernorm.weight torch.float32\n",
      "model.layers.17.self_attn.q_proj.weight torch.uint8\n",
      "model.layers.17.self_attn.k_proj.weight torch.uint8\n",
      "model.layers.17.self_attn.v_proj.weight torch.uint8\n",
      "model.layers.17.self_attn.o_proj.weight torch.uint8\n",
      "model.layers.17.mlp.gate_proj.weight torch.uint8\n",
      "model.layers.17.mlp.up_proj.weight torch.uint8\n",
      "model.layers.17.mlp.down_proj.weight torch.uint8\n",
      "model.layers.17.input_layernorm.weight torch.float32\n",
      "model.layers.17.post_attention_layernorm.weight torch.float32\n",
      "model.layers.18.self_attn.q_proj.weight torch.uint8\n",
      "model.layers.18.self_attn.k_proj.weight torch.uint8\n",
      "model.layers.18.self_attn.v_proj.weight torch.uint8\n",
      "model.layers.18.self_attn.o_proj.weight torch.uint8\n",
      "model.layers.18.mlp.gate_proj.weight torch.uint8\n",
      "model.layers.18.mlp.up_proj.weight torch.uint8\n",
      "model.layers.18.mlp.down_proj.weight torch.uint8\n",
      "model.layers.18.input_layernorm.weight torch.float32\n",
      "model.layers.18.post_attention_layernorm.weight torch.float32\n",
      "model.layers.19.self_attn.q_proj.weight torch.uint8\n",
      "model.layers.19.self_attn.k_proj.weight torch.uint8\n",
      "model.layers.19.self_attn.v_proj.weight torch.uint8\n",
      "model.layers.19.self_attn.o_proj.weight torch.uint8\n",
      "model.layers.19.mlp.gate_proj.weight torch.uint8\n",
      "model.layers.19.mlp.up_proj.weight torch.uint8\n",
      "model.layers.19.mlp.down_proj.weight torch.uint8\n",
      "model.layers.19.input_layernorm.weight torch.float32\n",
      "model.layers.19.post_attention_layernorm.weight torch.float32\n",
      "model.layers.20.self_attn.q_proj.weight torch.uint8\n",
      "model.layers.20.self_attn.k_proj.weight torch.uint8\n",
      "model.layers.20.self_attn.v_proj.weight torch.uint8\n",
      "model.layers.20.self_attn.o_proj.weight torch.uint8\n",
      "model.layers.20.mlp.gate_proj.weight torch.uint8\n",
      "model.layers.20.mlp.up_proj.weight torch.uint8\n",
      "model.layers.20.mlp.down_proj.weight torch.uint8\n",
      "model.layers.20.input_layernorm.weight torch.float32\n",
      "model.layers.20.post_attention_layernorm.weight torch.float32\n",
      "model.layers.21.self_attn.q_proj.weight torch.uint8\n",
      "model.layers.21.self_attn.k_proj.weight torch.uint8\n",
      "model.layers.21.self_attn.v_proj.weight torch.uint8\n",
      "model.layers.21.self_attn.o_proj.weight torch.uint8\n",
      "model.layers.21.mlp.gate_proj.weight torch.uint8\n",
      "model.layers.21.mlp.up_proj.weight torch.uint8\n",
      "model.layers.21.mlp.down_proj.weight torch.uint8\n",
      "model.layers.21.input_layernorm.weight torch.float32\n",
      "model.layers.21.post_attention_layernorm.weight torch.float32\n",
      "model.layers.22.self_attn.q_proj.weight torch.uint8\n",
      "model.layers.22.self_attn.k_proj.weight torch.uint8\n",
      "model.layers.22.self_attn.v_proj.weight torch.uint8\n",
      "model.layers.22.self_attn.o_proj.weight torch.uint8\n",
      "model.layers.22.mlp.gate_proj.weight torch.uint8\n",
      "model.layers.22.mlp.up_proj.weight torch.uint8\n",
      "model.layers.22.mlp.down_proj.weight torch.uint8\n",
      "model.layers.22.input_layernorm.weight torch.float32\n",
      "model.layers.22.post_attention_layernorm.weight torch.float32\n",
      "model.layers.23.self_attn.q_proj.weight torch.uint8\n",
      "model.layers.23.self_attn.k_proj.weight torch.uint8\n",
      "model.layers.23.self_attn.v_proj.weight torch.uint8\n",
      "model.layers.23.self_attn.o_proj.weight torch.uint8\n",
      "model.layers.23.mlp.gate_proj.weight torch.uint8\n",
      "model.layers.23.mlp.up_proj.weight torch.uint8\n",
      "model.layers.23.mlp.down_proj.weight torch.uint8\n",
      "model.layers.23.input_layernorm.weight torch.float32\n",
      "model.layers.23.post_attention_layernorm.weight torch.float32\n",
      "model.layers.24.self_attn.q_proj.weight torch.uint8\n",
      "model.layers.24.self_attn.k_proj.weight torch.uint8\n",
      "model.layers.24.self_attn.v_proj.weight torch.uint8\n",
      "model.layers.24.self_attn.o_proj.weight torch.uint8\n",
      "model.layers.24.mlp.gate_proj.weight torch.uint8\n",
      "model.layers.24.mlp.up_proj.weight torch.uint8\n",
      "model.layers.24.mlp.down_proj.weight torch.uint8\n",
      "model.layers.24.input_layernorm.weight torch.float32\n",
      "model.layers.24.post_attention_layernorm.weight torch.float32\n",
      "model.layers.25.self_attn.q_proj.weight torch.uint8\n",
      "model.layers.25.self_attn.k_proj.weight torch.uint8\n",
      "model.layers.25.self_attn.v_proj.weight torch.uint8\n",
      "model.layers.25.self_attn.o_proj.weight torch.uint8\n",
      "model.layers.25.mlp.gate_proj.weight torch.uint8\n",
      "model.layers.25.mlp.up_proj.weight torch.uint8\n",
      "model.layers.25.mlp.down_proj.weight torch.uint8\n",
      "model.layers.25.input_layernorm.weight torch.float32\n",
      "model.layers.25.post_attention_layernorm.weight torch.float32\n",
      "model.layers.26.self_attn.q_proj.weight torch.uint8\n",
      "model.layers.26.self_attn.k_proj.weight torch.uint8\n",
      "model.layers.26.self_attn.v_proj.weight torch.uint8\n",
      "model.layers.26.self_attn.o_proj.weight torch.uint8\n",
      "model.layers.26.mlp.gate_proj.weight torch.uint8\n",
      "model.layers.26.mlp.up_proj.weight torch.uint8\n",
      "model.layers.26.mlp.down_proj.weight torch.uint8\n",
      "model.layers.26.input_layernorm.weight torch.float32\n",
      "model.layers.26.post_attention_layernorm.weight torch.float32\n",
      "model.layers.27.self_attn.q_proj.weight torch.uint8\n",
      "model.layers.27.self_attn.k_proj.weight torch.uint8\n",
      "model.layers.27.self_attn.v_proj.weight torch.uint8\n",
      "model.layers.27.self_attn.o_proj.weight torch.uint8\n",
      "model.layers.27.mlp.gate_proj.weight torch.uint8\n",
      "model.layers.27.mlp.up_proj.weight torch.uint8\n",
      "model.layers.27.mlp.down_proj.weight torch.uint8\n",
      "model.layers.27.input_layernorm.weight torch.float32\n",
      "model.layers.27.post_attention_layernorm.weight torch.float32\n",
      "model.norm.weight torch.float32\n"
     ]
    }
   ],
   "source": [
    "for param in model.named_parameters():\n",
    "    print(param[0],param[1].dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 36,700,160 || all params: 3,249,449,984 || trainable%: 1.1294\n"
     ]
    }
   ],
   "source": [
    "config = LoraConfig(\n",
    "    r=64,\n",
    "    lora_alpha=16,\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\",\n",
    "    inference_mode=False,\n",
    "    use_rslora=True,\n",
    "    init_lora_weights=\"gaussian\",\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, config)\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "base_model.model.model.embed_tokens.weight False\n",
      "base_model.model.model.layers.0.self_attn.q_proj.base_layer.weight False\n",
      "base_model.model.model.layers.0.self_attn.q_proj.lora_A.default.weight True\n",
      "base_model.model.model.layers.0.self_attn.q_proj.lora_B.default.weight True\n",
      "base_model.model.model.layers.0.self_attn.k_proj.base_layer.weight False\n",
      "base_model.model.model.layers.0.self_attn.k_proj.lora_A.default.weight True\n",
      "base_model.model.model.layers.0.self_attn.k_proj.lora_B.default.weight True\n",
      "base_model.model.model.layers.0.self_attn.v_proj.base_layer.weight False\n",
      "base_model.model.model.layers.0.self_attn.v_proj.lora_A.default.weight True\n",
      "base_model.model.model.layers.0.self_attn.v_proj.lora_B.default.weight True\n",
      "base_model.model.model.layers.0.self_attn.o_proj.base_layer.weight False\n",
      "base_model.model.model.layers.0.self_attn.o_proj.lora_A.default.weight True\n",
      "base_model.model.model.layers.0.self_attn.o_proj.lora_B.default.weight True\n",
      "base_model.model.model.layers.0.mlp.gate_proj.weight False\n",
      "base_model.model.model.layers.0.mlp.up_proj.weight False\n",
      "base_model.model.model.layers.0.mlp.down_proj.weight False\n",
      "base_model.model.model.layers.0.input_layernorm.weight False\n",
      "base_model.model.model.layers.0.post_attention_layernorm.weight False\n",
      "base_model.model.model.layers.1.self_attn.q_proj.base_layer.weight False\n",
      "base_model.model.model.layers.1.self_attn.q_proj.lora_A.default.weight True\n",
      "base_model.model.model.layers.1.self_attn.q_proj.lora_B.default.weight True\n",
      "base_model.model.model.layers.1.self_attn.k_proj.base_layer.weight False\n",
      "base_model.model.model.layers.1.self_attn.k_proj.lora_A.default.weight True\n",
      "base_model.model.model.layers.1.self_attn.k_proj.lora_B.default.weight True\n",
      "base_model.model.model.layers.1.self_attn.v_proj.base_layer.weight False\n",
      "base_model.model.model.layers.1.self_attn.v_proj.lora_A.default.weight True\n",
      "base_model.model.model.layers.1.self_attn.v_proj.lora_B.default.weight True\n",
      "base_model.model.model.layers.1.self_attn.o_proj.base_layer.weight False\n",
      "base_model.model.model.layers.1.self_attn.o_proj.lora_A.default.weight True\n",
      "base_model.model.model.layers.1.self_attn.o_proj.lora_B.default.weight True\n",
      "base_model.model.model.layers.1.mlp.gate_proj.weight False\n",
      "base_model.model.model.layers.1.mlp.up_proj.weight False\n",
      "base_model.model.model.layers.1.mlp.down_proj.weight False\n",
      "base_model.model.model.layers.1.input_layernorm.weight False\n",
      "base_model.model.model.layers.1.post_attention_layernorm.weight False\n",
      "base_model.model.model.layers.2.self_attn.q_proj.base_layer.weight False\n",
      "base_model.model.model.layers.2.self_attn.q_proj.lora_A.default.weight True\n",
      "base_model.model.model.layers.2.self_attn.q_proj.lora_B.default.weight True\n",
      "base_model.model.model.layers.2.self_attn.k_proj.base_layer.weight False\n",
      "base_model.model.model.layers.2.self_attn.k_proj.lora_A.default.weight True\n",
      "base_model.model.model.layers.2.self_attn.k_proj.lora_B.default.weight True\n",
      "base_model.model.model.layers.2.self_attn.v_proj.base_layer.weight False\n",
      "base_model.model.model.layers.2.self_attn.v_proj.lora_A.default.weight True\n",
      "base_model.model.model.layers.2.self_attn.v_proj.lora_B.default.weight True\n",
      "base_model.model.model.layers.2.self_attn.o_proj.base_layer.weight False\n",
      "base_model.model.model.layers.2.self_attn.o_proj.lora_A.default.weight True\n",
      "base_model.model.model.layers.2.self_attn.o_proj.lora_B.default.weight True\n",
      "base_model.model.model.layers.2.mlp.gate_proj.weight False\n",
      "base_model.model.model.layers.2.mlp.up_proj.weight False\n",
      "base_model.model.model.layers.2.mlp.down_proj.weight False\n",
      "base_model.model.model.layers.2.input_layernorm.weight False\n",
      "base_model.model.model.layers.2.post_attention_layernorm.weight False\n",
      "base_model.model.model.layers.3.self_attn.q_proj.base_layer.weight False\n",
      "base_model.model.model.layers.3.self_attn.q_proj.lora_A.default.weight True\n",
      "base_model.model.model.layers.3.self_attn.q_proj.lora_B.default.weight True\n",
      "base_model.model.model.layers.3.self_attn.k_proj.base_layer.weight False\n",
      "base_model.model.model.layers.3.self_attn.k_proj.lora_A.default.weight True\n",
      "base_model.model.model.layers.3.self_attn.k_proj.lora_B.default.weight True\n",
      "base_model.model.model.layers.3.self_attn.v_proj.base_layer.weight False\n",
      "base_model.model.model.layers.3.self_attn.v_proj.lora_A.default.weight True\n",
      "base_model.model.model.layers.3.self_attn.v_proj.lora_B.default.weight True\n",
      "base_model.model.model.layers.3.self_attn.o_proj.base_layer.weight False\n",
      "base_model.model.model.layers.3.self_attn.o_proj.lora_A.default.weight True\n",
      "base_model.model.model.layers.3.self_attn.o_proj.lora_B.default.weight True\n",
      "base_model.model.model.layers.3.mlp.gate_proj.weight False\n",
      "base_model.model.model.layers.3.mlp.up_proj.weight False\n",
      "base_model.model.model.layers.3.mlp.down_proj.weight False\n",
      "base_model.model.model.layers.3.input_layernorm.weight False\n",
      "base_model.model.model.layers.3.post_attention_layernorm.weight False\n",
      "base_model.model.model.layers.4.self_attn.q_proj.base_layer.weight False\n",
      "base_model.model.model.layers.4.self_attn.q_proj.lora_A.default.weight True\n",
      "base_model.model.model.layers.4.self_attn.q_proj.lora_B.default.weight True\n",
      "base_model.model.model.layers.4.self_attn.k_proj.base_layer.weight False\n",
      "base_model.model.model.layers.4.self_attn.k_proj.lora_A.default.weight True\n",
      "base_model.model.model.layers.4.self_attn.k_proj.lora_B.default.weight True\n",
      "base_model.model.model.layers.4.self_attn.v_proj.base_layer.weight False\n",
      "base_model.model.model.layers.4.self_attn.v_proj.lora_A.default.weight True\n",
      "base_model.model.model.layers.4.self_attn.v_proj.lora_B.default.weight True\n",
      "base_model.model.model.layers.4.self_attn.o_proj.base_layer.weight False\n",
      "base_model.model.model.layers.4.self_attn.o_proj.lora_A.default.weight True\n",
      "base_model.model.model.layers.4.self_attn.o_proj.lora_B.default.weight True\n",
      "base_model.model.model.layers.4.mlp.gate_proj.weight False\n",
      "base_model.model.model.layers.4.mlp.up_proj.weight False\n",
      "base_model.model.model.layers.4.mlp.down_proj.weight False\n",
      "base_model.model.model.layers.4.input_layernorm.weight False\n",
      "base_model.model.model.layers.4.post_attention_layernorm.weight False\n",
      "base_model.model.model.layers.5.self_attn.q_proj.base_layer.weight False\n",
      "base_model.model.model.layers.5.self_attn.q_proj.lora_A.default.weight True\n",
      "base_model.model.model.layers.5.self_attn.q_proj.lora_B.default.weight True\n",
      "base_model.model.model.layers.5.self_attn.k_proj.base_layer.weight False\n",
      "base_model.model.model.layers.5.self_attn.k_proj.lora_A.default.weight True\n",
      "base_model.model.model.layers.5.self_attn.k_proj.lora_B.default.weight True\n",
      "base_model.model.model.layers.5.self_attn.v_proj.base_layer.weight False\n",
      "base_model.model.model.layers.5.self_attn.v_proj.lora_A.default.weight True\n",
      "base_model.model.model.layers.5.self_attn.v_proj.lora_B.default.weight True\n",
      "base_model.model.model.layers.5.self_attn.o_proj.base_layer.weight False\n",
      "base_model.model.model.layers.5.self_attn.o_proj.lora_A.default.weight True\n",
      "base_model.model.model.layers.5.self_attn.o_proj.lora_B.default.weight True\n",
      "base_model.model.model.layers.5.mlp.gate_proj.weight False\n",
      "base_model.model.model.layers.5.mlp.up_proj.weight False\n",
      "base_model.model.model.layers.5.mlp.down_proj.weight False\n",
      "base_model.model.model.layers.5.input_layernorm.weight False\n",
      "base_model.model.model.layers.5.post_attention_layernorm.weight False\n",
      "base_model.model.model.layers.6.self_attn.q_proj.base_layer.weight False\n",
      "base_model.model.model.layers.6.self_attn.q_proj.lora_A.default.weight True\n",
      "base_model.model.model.layers.6.self_attn.q_proj.lora_B.default.weight True\n",
      "base_model.model.model.layers.6.self_attn.k_proj.base_layer.weight False\n",
      "base_model.model.model.layers.6.self_attn.k_proj.lora_A.default.weight True\n",
      "base_model.model.model.layers.6.self_attn.k_proj.lora_B.default.weight True\n",
      "base_model.model.model.layers.6.self_attn.v_proj.base_layer.weight False\n",
      "base_model.model.model.layers.6.self_attn.v_proj.lora_A.default.weight True\n",
      "base_model.model.model.layers.6.self_attn.v_proj.lora_B.default.weight True\n",
      "base_model.model.model.layers.6.self_attn.o_proj.base_layer.weight False\n",
      "base_model.model.model.layers.6.self_attn.o_proj.lora_A.default.weight True\n",
      "base_model.model.model.layers.6.self_attn.o_proj.lora_B.default.weight True\n",
      "base_model.model.model.layers.6.mlp.gate_proj.weight False\n",
      "base_model.model.model.layers.6.mlp.up_proj.weight False\n",
      "base_model.model.model.layers.6.mlp.down_proj.weight False\n",
      "base_model.model.model.layers.6.input_layernorm.weight False\n",
      "base_model.model.model.layers.6.post_attention_layernorm.weight False\n",
      "base_model.model.model.layers.7.self_attn.q_proj.base_layer.weight False\n",
      "base_model.model.model.layers.7.self_attn.q_proj.lora_A.default.weight True\n",
      "base_model.model.model.layers.7.self_attn.q_proj.lora_B.default.weight True\n",
      "base_model.model.model.layers.7.self_attn.k_proj.base_layer.weight False\n",
      "base_model.model.model.layers.7.self_attn.k_proj.lora_A.default.weight True\n",
      "base_model.model.model.layers.7.self_attn.k_proj.lora_B.default.weight True\n",
      "base_model.model.model.layers.7.self_attn.v_proj.base_layer.weight False\n",
      "base_model.model.model.layers.7.self_attn.v_proj.lora_A.default.weight True\n",
      "base_model.model.model.layers.7.self_attn.v_proj.lora_B.default.weight True\n",
      "base_model.model.model.layers.7.self_attn.o_proj.base_layer.weight False\n",
      "base_model.model.model.layers.7.self_attn.o_proj.lora_A.default.weight True\n",
      "base_model.model.model.layers.7.self_attn.o_proj.lora_B.default.weight True\n",
      "base_model.model.model.layers.7.mlp.gate_proj.weight False\n",
      "base_model.model.model.layers.7.mlp.up_proj.weight False\n",
      "base_model.model.model.layers.7.mlp.down_proj.weight False\n",
      "base_model.model.model.layers.7.input_layernorm.weight False\n",
      "base_model.model.model.layers.7.post_attention_layernorm.weight False\n",
      "base_model.model.model.layers.8.self_attn.q_proj.base_layer.weight False\n",
      "base_model.model.model.layers.8.self_attn.q_proj.lora_A.default.weight True\n",
      "base_model.model.model.layers.8.self_attn.q_proj.lora_B.default.weight True\n",
      "base_model.model.model.layers.8.self_attn.k_proj.base_layer.weight False\n",
      "base_model.model.model.layers.8.self_attn.k_proj.lora_A.default.weight True\n",
      "base_model.model.model.layers.8.self_attn.k_proj.lora_B.default.weight True\n",
      "base_model.model.model.layers.8.self_attn.v_proj.base_layer.weight False\n",
      "base_model.model.model.layers.8.self_attn.v_proj.lora_A.default.weight True\n",
      "base_model.model.model.layers.8.self_attn.v_proj.lora_B.default.weight True\n",
      "base_model.model.model.layers.8.self_attn.o_proj.base_layer.weight False\n",
      "base_model.model.model.layers.8.self_attn.o_proj.lora_A.default.weight True\n",
      "base_model.model.model.layers.8.self_attn.o_proj.lora_B.default.weight True\n",
      "base_model.model.model.layers.8.mlp.gate_proj.weight False\n",
      "base_model.model.model.layers.8.mlp.up_proj.weight False\n",
      "base_model.model.model.layers.8.mlp.down_proj.weight False\n",
      "base_model.model.model.layers.8.input_layernorm.weight False\n",
      "base_model.model.model.layers.8.post_attention_layernorm.weight False\n",
      "base_model.model.model.layers.9.self_attn.q_proj.base_layer.weight False\n",
      "base_model.model.model.layers.9.self_attn.q_proj.lora_A.default.weight True\n",
      "base_model.model.model.layers.9.self_attn.q_proj.lora_B.default.weight True\n",
      "base_model.model.model.layers.9.self_attn.k_proj.base_layer.weight False\n",
      "base_model.model.model.layers.9.self_attn.k_proj.lora_A.default.weight True\n",
      "base_model.model.model.layers.9.self_attn.k_proj.lora_B.default.weight True\n",
      "base_model.model.model.layers.9.self_attn.v_proj.base_layer.weight False\n",
      "base_model.model.model.layers.9.self_attn.v_proj.lora_A.default.weight True\n",
      "base_model.model.model.layers.9.self_attn.v_proj.lora_B.default.weight True\n",
      "base_model.model.model.layers.9.self_attn.o_proj.base_layer.weight False\n",
      "base_model.model.model.layers.9.self_attn.o_proj.lora_A.default.weight True\n",
      "base_model.model.model.layers.9.self_attn.o_proj.lora_B.default.weight True\n",
      "base_model.model.model.layers.9.mlp.gate_proj.weight False\n",
      "base_model.model.model.layers.9.mlp.up_proj.weight False\n",
      "base_model.model.model.layers.9.mlp.down_proj.weight False\n",
      "base_model.model.model.layers.9.input_layernorm.weight False\n",
      "base_model.model.model.layers.9.post_attention_layernorm.weight False\n",
      "base_model.model.model.layers.10.self_attn.q_proj.base_layer.weight False\n",
      "base_model.model.model.layers.10.self_attn.q_proj.lora_A.default.weight True\n",
      "base_model.model.model.layers.10.self_attn.q_proj.lora_B.default.weight True\n",
      "base_model.model.model.layers.10.self_attn.k_proj.base_layer.weight False\n",
      "base_model.model.model.layers.10.self_attn.k_proj.lora_A.default.weight True\n",
      "base_model.model.model.layers.10.self_attn.k_proj.lora_B.default.weight True\n",
      "base_model.model.model.layers.10.self_attn.v_proj.base_layer.weight False\n",
      "base_model.model.model.layers.10.self_attn.v_proj.lora_A.default.weight True\n",
      "base_model.model.model.layers.10.self_attn.v_proj.lora_B.default.weight True\n",
      "base_model.model.model.layers.10.self_attn.o_proj.base_layer.weight False\n",
      "base_model.model.model.layers.10.self_attn.o_proj.lora_A.default.weight True\n",
      "base_model.model.model.layers.10.self_attn.o_proj.lora_B.default.weight True\n",
      "base_model.model.model.layers.10.mlp.gate_proj.weight False\n",
      "base_model.model.model.layers.10.mlp.up_proj.weight False\n",
      "base_model.model.model.layers.10.mlp.down_proj.weight False\n",
      "base_model.model.model.layers.10.input_layernorm.weight False\n",
      "base_model.model.model.layers.10.post_attention_layernorm.weight False\n",
      "base_model.model.model.layers.11.self_attn.q_proj.base_layer.weight False\n",
      "base_model.model.model.layers.11.self_attn.q_proj.lora_A.default.weight True\n",
      "base_model.model.model.layers.11.self_attn.q_proj.lora_B.default.weight True\n",
      "base_model.model.model.layers.11.self_attn.k_proj.base_layer.weight False\n",
      "base_model.model.model.layers.11.self_attn.k_proj.lora_A.default.weight True\n",
      "base_model.model.model.layers.11.self_attn.k_proj.lora_B.default.weight True\n",
      "base_model.model.model.layers.11.self_attn.v_proj.base_layer.weight False\n",
      "base_model.model.model.layers.11.self_attn.v_proj.lora_A.default.weight True\n",
      "base_model.model.model.layers.11.self_attn.v_proj.lora_B.default.weight True\n",
      "base_model.model.model.layers.11.self_attn.o_proj.base_layer.weight False\n",
      "base_model.model.model.layers.11.self_attn.o_proj.lora_A.default.weight True\n",
      "base_model.model.model.layers.11.self_attn.o_proj.lora_B.default.weight True\n",
      "base_model.model.model.layers.11.mlp.gate_proj.weight False\n",
      "base_model.model.model.layers.11.mlp.up_proj.weight False\n",
      "base_model.model.model.layers.11.mlp.down_proj.weight False\n",
      "base_model.model.model.layers.11.input_layernorm.weight False\n",
      "base_model.model.model.layers.11.post_attention_layernorm.weight False\n",
      "base_model.model.model.layers.12.self_attn.q_proj.base_layer.weight False\n",
      "base_model.model.model.layers.12.self_attn.q_proj.lora_A.default.weight True\n",
      "base_model.model.model.layers.12.self_attn.q_proj.lora_B.default.weight True\n",
      "base_model.model.model.layers.12.self_attn.k_proj.base_layer.weight False\n",
      "base_model.model.model.layers.12.self_attn.k_proj.lora_A.default.weight True\n",
      "base_model.model.model.layers.12.self_attn.k_proj.lora_B.default.weight True\n",
      "base_model.model.model.layers.12.self_attn.v_proj.base_layer.weight False\n",
      "base_model.model.model.layers.12.self_attn.v_proj.lora_A.default.weight True\n",
      "base_model.model.model.layers.12.self_attn.v_proj.lora_B.default.weight True\n",
      "base_model.model.model.layers.12.self_attn.o_proj.base_layer.weight False\n",
      "base_model.model.model.layers.12.self_attn.o_proj.lora_A.default.weight True\n",
      "base_model.model.model.layers.12.self_attn.o_proj.lora_B.default.weight True\n",
      "base_model.model.model.layers.12.mlp.gate_proj.weight False\n",
      "base_model.model.model.layers.12.mlp.up_proj.weight False\n",
      "base_model.model.model.layers.12.mlp.down_proj.weight False\n",
      "base_model.model.model.layers.12.input_layernorm.weight False\n",
      "base_model.model.model.layers.12.post_attention_layernorm.weight False\n",
      "base_model.model.model.layers.13.self_attn.q_proj.base_layer.weight False\n",
      "base_model.model.model.layers.13.self_attn.q_proj.lora_A.default.weight True\n",
      "base_model.model.model.layers.13.self_attn.q_proj.lora_B.default.weight True\n",
      "base_model.model.model.layers.13.self_attn.k_proj.base_layer.weight False\n",
      "base_model.model.model.layers.13.self_attn.k_proj.lora_A.default.weight True\n",
      "base_model.model.model.layers.13.self_attn.k_proj.lora_B.default.weight True\n",
      "base_model.model.model.layers.13.self_attn.v_proj.base_layer.weight False\n",
      "base_model.model.model.layers.13.self_attn.v_proj.lora_A.default.weight True\n",
      "base_model.model.model.layers.13.self_attn.v_proj.lora_B.default.weight True\n",
      "base_model.model.model.layers.13.self_attn.o_proj.base_layer.weight False\n",
      "base_model.model.model.layers.13.self_attn.o_proj.lora_A.default.weight True\n",
      "base_model.model.model.layers.13.self_attn.o_proj.lora_B.default.weight True\n",
      "base_model.model.model.layers.13.mlp.gate_proj.weight False\n",
      "base_model.model.model.layers.13.mlp.up_proj.weight False\n",
      "base_model.model.model.layers.13.mlp.down_proj.weight False\n",
      "base_model.model.model.layers.13.input_layernorm.weight False\n",
      "base_model.model.model.layers.13.post_attention_layernorm.weight False\n",
      "base_model.model.model.layers.14.self_attn.q_proj.base_layer.weight False\n",
      "base_model.model.model.layers.14.self_attn.q_proj.lora_A.default.weight True\n",
      "base_model.model.model.layers.14.self_attn.q_proj.lora_B.default.weight True\n",
      "base_model.model.model.layers.14.self_attn.k_proj.base_layer.weight False\n",
      "base_model.model.model.layers.14.self_attn.k_proj.lora_A.default.weight True\n",
      "base_model.model.model.layers.14.self_attn.k_proj.lora_B.default.weight True\n",
      "base_model.model.model.layers.14.self_attn.v_proj.base_layer.weight False\n",
      "base_model.model.model.layers.14.self_attn.v_proj.lora_A.default.weight True\n",
      "base_model.model.model.layers.14.self_attn.v_proj.lora_B.default.weight True\n",
      "base_model.model.model.layers.14.self_attn.o_proj.base_layer.weight False\n",
      "base_model.model.model.layers.14.self_attn.o_proj.lora_A.default.weight True\n",
      "base_model.model.model.layers.14.self_attn.o_proj.lora_B.default.weight True\n",
      "base_model.model.model.layers.14.mlp.gate_proj.weight False\n",
      "base_model.model.model.layers.14.mlp.up_proj.weight False\n",
      "base_model.model.model.layers.14.mlp.down_proj.weight False\n",
      "base_model.model.model.layers.14.input_layernorm.weight False\n",
      "base_model.model.model.layers.14.post_attention_layernorm.weight False\n",
      "base_model.model.model.layers.15.self_attn.q_proj.base_layer.weight False\n",
      "base_model.model.model.layers.15.self_attn.q_proj.lora_A.default.weight True\n",
      "base_model.model.model.layers.15.self_attn.q_proj.lora_B.default.weight True\n",
      "base_model.model.model.layers.15.self_attn.k_proj.base_layer.weight False\n",
      "base_model.model.model.layers.15.self_attn.k_proj.lora_A.default.weight True\n",
      "base_model.model.model.layers.15.self_attn.k_proj.lora_B.default.weight True\n",
      "base_model.model.model.layers.15.self_attn.v_proj.base_layer.weight False\n",
      "base_model.model.model.layers.15.self_attn.v_proj.lora_A.default.weight True\n",
      "base_model.model.model.layers.15.self_attn.v_proj.lora_B.default.weight True\n",
      "base_model.model.model.layers.15.self_attn.o_proj.base_layer.weight False\n",
      "base_model.model.model.layers.15.self_attn.o_proj.lora_A.default.weight True\n",
      "base_model.model.model.layers.15.self_attn.o_proj.lora_B.default.weight True\n",
      "base_model.model.model.layers.15.mlp.gate_proj.weight False\n",
      "base_model.model.model.layers.15.mlp.up_proj.weight False\n",
      "base_model.model.model.layers.15.mlp.down_proj.weight False\n",
      "base_model.model.model.layers.15.input_layernorm.weight False\n",
      "base_model.model.model.layers.15.post_attention_layernorm.weight False\n",
      "base_model.model.model.layers.16.self_attn.q_proj.base_layer.weight False\n",
      "base_model.model.model.layers.16.self_attn.q_proj.lora_A.default.weight True\n",
      "base_model.model.model.layers.16.self_attn.q_proj.lora_B.default.weight True\n",
      "base_model.model.model.layers.16.self_attn.k_proj.base_layer.weight False\n",
      "base_model.model.model.layers.16.self_attn.k_proj.lora_A.default.weight True\n",
      "base_model.model.model.layers.16.self_attn.k_proj.lora_B.default.weight True\n",
      "base_model.model.model.layers.16.self_attn.v_proj.base_layer.weight False\n",
      "base_model.model.model.layers.16.self_attn.v_proj.lora_A.default.weight True\n",
      "base_model.model.model.layers.16.self_attn.v_proj.lora_B.default.weight True\n",
      "base_model.model.model.layers.16.self_attn.o_proj.base_layer.weight False\n",
      "base_model.model.model.layers.16.self_attn.o_proj.lora_A.default.weight True\n",
      "base_model.model.model.layers.16.self_attn.o_proj.lora_B.default.weight True\n",
      "base_model.model.model.layers.16.mlp.gate_proj.weight False\n",
      "base_model.model.model.layers.16.mlp.up_proj.weight False\n",
      "base_model.model.model.layers.16.mlp.down_proj.weight False\n",
      "base_model.model.model.layers.16.input_layernorm.weight False\n",
      "base_model.model.model.layers.16.post_attention_layernorm.weight False\n",
      "base_model.model.model.layers.17.self_attn.q_proj.base_layer.weight False\n",
      "base_model.model.model.layers.17.self_attn.q_proj.lora_A.default.weight True\n",
      "base_model.model.model.layers.17.self_attn.q_proj.lora_B.default.weight True\n",
      "base_model.model.model.layers.17.self_attn.k_proj.base_layer.weight False\n",
      "base_model.model.model.layers.17.self_attn.k_proj.lora_A.default.weight True\n",
      "base_model.model.model.layers.17.self_attn.k_proj.lora_B.default.weight True\n",
      "base_model.model.model.layers.17.self_attn.v_proj.base_layer.weight False\n",
      "base_model.model.model.layers.17.self_attn.v_proj.lora_A.default.weight True\n",
      "base_model.model.model.layers.17.self_attn.v_proj.lora_B.default.weight True\n",
      "base_model.model.model.layers.17.self_attn.o_proj.base_layer.weight False\n",
      "base_model.model.model.layers.17.self_attn.o_proj.lora_A.default.weight True\n",
      "base_model.model.model.layers.17.self_attn.o_proj.lora_B.default.weight True\n",
      "base_model.model.model.layers.17.mlp.gate_proj.weight False\n",
      "base_model.model.model.layers.17.mlp.up_proj.weight False\n",
      "base_model.model.model.layers.17.mlp.down_proj.weight False\n",
      "base_model.model.model.layers.17.input_layernorm.weight False\n",
      "base_model.model.model.layers.17.post_attention_layernorm.weight False\n",
      "base_model.model.model.layers.18.self_attn.q_proj.base_layer.weight False\n",
      "base_model.model.model.layers.18.self_attn.q_proj.lora_A.default.weight True\n",
      "base_model.model.model.layers.18.self_attn.q_proj.lora_B.default.weight True\n",
      "base_model.model.model.layers.18.self_attn.k_proj.base_layer.weight False\n",
      "base_model.model.model.layers.18.self_attn.k_proj.lora_A.default.weight True\n",
      "base_model.model.model.layers.18.self_attn.k_proj.lora_B.default.weight True\n",
      "base_model.model.model.layers.18.self_attn.v_proj.base_layer.weight False\n",
      "base_model.model.model.layers.18.self_attn.v_proj.lora_A.default.weight True\n",
      "base_model.model.model.layers.18.self_attn.v_proj.lora_B.default.weight True\n",
      "base_model.model.model.layers.18.self_attn.o_proj.base_layer.weight False\n",
      "base_model.model.model.layers.18.self_attn.o_proj.lora_A.default.weight True\n",
      "base_model.model.model.layers.18.self_attn.o_proj.lora_B.default.weight True\n",
      "base_model.model.model.layers.18.mlp.gate_proj.weight False\n",
      "base_model.model.model.layers.18.mlp.up_proj.weight False\n",
      "base_model.model.model.layers.18.mlp.down_proj.weight False\n",
      "base_model.model.model.layers.18.input_layernorm.weight False\n",
      "base_model.model.model.layers.18.post_attention_layernorm.weight False\n",
      "base_model.model.model.layers.19.self_attn.q_proj.base_layer.weight False\n",
      "base_model.model.model.layers.19.self_attn.q_proj.lora_A.default.weight True\n",
      "base_model.model.model.layers.19.self_attn.q_proj.lora_B.default.weight True\n",
      "base_model.model.model.layers.19.self_attn.k_proj.base_layer.weight False\n",
      "base_model.model.model.layers.19.self_attn.k_proj.lora_A.default.weight True\n",
      "base_model.model.model.layers.19.self_attn.k_proj.lora_B.default.weight True\n",
      "base_model.model.model.layers.19.self_attn.v_proj.base_layer.weight False\n",
      "base_model.model.model.layers.19.self_attn.v_proj.lora_A.default.weight True\n",
      "base_model.model.model.layers.19.self_attn.v_proj.lora_B.default.weight True\n",
      "base_model.model.model.layers.19.self_attn.o_proj.base_layer.weight False\n",
      "base_model.model.model.layers.19.self_attn.o_proj.lora_A.default.weight True\n",
      "base_model.model.model.layers.19.self_attn.o_proj.lora_B.default.weight True\n",
      "base_model.model.model.layers.19.mlp.gate_proj.weight False\n",
      "base_model.model.model.layers.19.mlp.up_proj.weight False\n",
      "base_model.model.model.layers.19.mlp.down_proj.weight False\n",
      "base_model.model.model.layers.19.input_layernorm.weight False\n",
      "base_model.model.model.layers.19.post_attention_layernorm.weight False\n",
      "base_model.model.model.layers.20.self_attn.q_proj.base_layer.weight False\n",
      "base_model.model.model.layers.20.self_attn.q_proj.lora_A.default.weight True\n",
      "base_model.model.model.layers.20.self_attn.q_proj.lora_B.default.weight True\n",
      "base_model.model.model.layers.20.self_attn.k_proj.base_layer.weight False\n",
      "base_model.model.model.layers.20.self_attn.k_proj.lora_A.default.weight True\n",
      "base_model.model.model.layers.20.self_attn.k_proj.lora_B.default.weight True\n",
      "base_model.model.model.layers.20.self_attn.v_proj.base_layer.weight False\n",
      "base_model.model.model.layers.20.self_attn.v_proj.lora_A.default.weight True\n",
      "base_model.model.model.layers.20.self_attn.v_proj.lora_B.default.weight True\n",
      "base_model.model.model.layers.20.self_attn.o_proj.base_layer.weight False\n",
      "base_model.model.model.layers.20.self_attn.o_proj.lora_A.default.weight True\n",
      "base_model.model.model.layers.20.self_attn.o_proj.lora_B.default.weight True\n",
      "base_model.model.model.layers.20.mlp.gate_proj.weight False\n",
      "base_model.model.model.layers.20.mlp.up_proj.weight False\n",
      "base_model.model.model.layers.20.mlp.down_proj.weight False\n",
      "base_model.model.model.layers.20.input_layernorm.weight False\n",
      "base_model.model.model.layers.20.post_attention_layernorm.weight False\n",
      "base_model.model.model.layers.21.self_attn.q_proj.base_layer.weight False\n",
      "base_model.model.model.layers.21.self_attn.q_proj.lora_A.default.weight True\n",
      "base_model.model.model.layers.21.self_attn.q_proj.lora_B.default.weight True\n",
      "base_model.model.model.layers.21.self_attn.k_proj.base_layer.weight False\n",
      "base_model.model.model.layers.21.self_attn.k_proj.lora_A.default.weight True\n",
      "base_model.model.model.layers.21.self_attn.k_proj.lora_B.default.weight True\n",
      "base_model.model.model.layers.21.self_attn.v_proj.base_layer.weight False\n",
      "base_model.model.model.layers.21.self_attn.v_proj.lora_A.default.weight True\n",
      "base_model.model.model.layers.21.self_attn.v_proj.lora_B.default.weight True\n",
      "base_model.model.model.layers.21.self_attn.o_proj.base_layer.weight False\n",
      "base_model.model.model.layers.21.self_attn.o_proj.lora_A.default.weight True\n",
      "base_model.model.model.layers.21.self_attn.o_proj.lora_B.default.weight True\n",
      "base_model.model.model.layers.21.mlp.gate_proj.weight False\n",
      "base_model.model.model.layers.21.mlp.up_proj.weight False\n",
      "base_model.model.model.layers.21.mlp.down_proj.weight False\n",
      "base_model.model.model.layers.21.input_layernorm.weight False\n",
      "base_model.model.model.layers.21.post_attention_layernorm.weight False\n",
      "base_model.model.model.layers.22.self_attn.q_proj.base_layer.weight False\n",
      "base_model.model.model.layers.22.self_attn.q_proj.lora_A.default.weight True\n",
      "base_model.model.model.layers.22.self_attn.q_proj.lora_B.default.weight True\n",
      "base_model.model.model.layers.22.self_attn.k_proj.base_layer.weight False\n",
      "base_model.model.model.layers.22.self_attn.k_proj.lora_A.default.weight True\n",
      "base_model.model.model.layers.22.self_attn.k_proj.lora_B.default.weight True\n",
      "base_model.model.model.layers.22.self_attn.v_proj.base_layer.weight False\n",
      "base_model.model.model.layers.22.self_attn.v_proj.lora_A.default.weight True\n",
      "base_model.model.model.layers.22.self_attn.v_proj.lora_B.default.weight True\n",
      "base_model.model.model.layers.22.self_attn.o_proj.base_layer.weight False\n",
      "base_model.model.model.layers.22.self_attn.o_proj.lora_A.default.weight True\n",
      "base_model.model.model.layers.22.self_attn.o_proj.lora_B.default.weight True\n",
      "base_model.model.model.layers.22.mlp.gate_proj.weight False\n",
      "base_model.model.model.layers.22.mlp.up_proj.weight False\n",
      "base_model.model.model.layers.22.mlp.down_proj.weight False\n",
      "base_model.model.model.layers.22.input_layernorm.weight False\n",
      "base_model.model.model.layers.22.post_attention_layernorm.weight False\n",
      "base_model.model.model.layers.23.self_attn.q_proj.base_layer.weight False\n",
      "base_model.model.model.layers.23.self_attn.q_proj.lora_A.default.weight True\n",
      "base_model.model.model.layers.23.self_attn.q_proj.lora_B.default.weight True\n",
      "base_model.model.model.layers.23.self_attn.k_proj.base_layer.weight False\n",
      "base_model.model.model.layers.23.self_attn.k_proj.lora_A.default.weight True\n",
      "base_model.model.model.layers.23.self_attn.k_proj.lora_B.default.weight True\n",
      "base_model.model.model.layers.23.self_attn.v_proj.base_layer.weight False\n",
      "base_model.model.model.layers.23.self_attn.v_proj.lora_A.default.weight True\n",
      "base_model.model.model.layers.23.self_attn.v_proj.lora_B.default.weight True\n",
      "base_model.model.model.layers.23.self_attn.o_proj.base_layer.weight False\n",
      "base_model.model.model.layers.23.self_attn.o_proj.lora_A.default.weight True\n",
      "base_model.model.model.layers.23.self_attn.o_proj.lora_B.default.weight True\n",
      "base_model.model.model.layers.23.mlp.gate_proj.weight False\n",
      "base_model.model.model.layers.23.mlp.up_proj.weight False\n",
      "base_model.model.model.layers.23.mlp.down_proj.weight False\n",
      "base_model.model.model.layers.23.input_layernorm.weight False\n",
      "base_model.model.model.layers.23.post_attention_layernorm.weight False\n",
      "base_model.model.model.layers.24.self_attn.q_proj.base_layer.weight False\n",
      "base_model.model.model.layers.24.self_attn.q_proj.lora_A.default.weight True\n",
      "base_model.model.model.layers.24.self_attn.q_proj.lora_B.default.weight True\n",
      "base_model.model.model.layers.24.self_attn.k_proj.base_layer.weight False\n",
      "base_model.model.model.layers.24.self_attn.k_proj.lora_A.default.weight True\n",
      "base_model.model.model.layers.24.self_attn.k_proj.lora_B.default.weight True\n",
      "base_model.model.model.layers.24.self_attn.v_proj.base_layer.weight False\n",
      "base_model.model.model.layers.24.self_attn.v_proj.lora_A.default.weight True\n",
      "base_model.model.model.layers.24.self_attn.v_proj.lora_B.default.weight True\n",
      "base_model.model.model.layers.24.self_attn.o_proj.base_layer.weight False\n",
      "base_model.model.model.layers.24.self_attn.o_proj.lora_A.default.weight True\n",
      "base_model.model.model.layers.24.self_attn.o_proj.lora_B.default.weight True\n",
      "base_model.model.model.layers.24.mlp.gate_proj.weight False\n",
      "base_model.model.model.layers.24.mlp.up_proj.weight False\n",
      "base_model.model.model.layers.24.mlp.down_proj.weight False\n",
      "base_model.model.model.layers.24.input_layernorm.weight False\n",
      "base_model.model.model.layers.24.post_attention_layernorm.weight False\n",
      "base_model.model.model.layers.25.self_attn.q_proj.base_layer.weight False\n",
      "base_model.model.model.layers.25.self_attn.q_proj.lora_A.default.weight True\n",
      "base_model.model.model.layers.25.self_attn.q_proj.lora_B.default.weight True\n",
      "base_model.model.model.layers.25.self_attn.k_proj.base_layer.weight False\n",
      "base_model.model.model.layers.25.self_attn.k_proj.lora_A.default.weight True\n",
      "base_model.model.model.layers.25.self_attn.k_proj.lora_B.default.weight True\n",
      "base_model.model.model.layers.25.self_attn.v_proj.base_layer.weight False\n",
      "base_model.model.model.layers.25.self_attn.v_proj.lora_A.default.weight True\n",
      "base_model.model.model.layers.25.self_attn.v_proj.lora_B.default.weight True\n",
      "base_model.model.model.layers.25.self_attn.o_proj.base_layer.weight False\n",
      "base_model.model.model.layers.25.self_attn.o_proj.lora_A.default.weight True\n",
      "base_model.model.model.layers.25.self_attn.o_proj.lora_B.default.weight True\n",
      "base_model.model.model.layers.25.mlp.gate_proj.weight False\n",
      "base_model.model.model.layers.25.mlp.up_proj.weight False\n",
      "base_model.model.model.layers.25.mlp.down_proj.weight False\n",
      "base_model.model.model.layers.25.input_layernorm.weight False\n",
      "base_model.model.model.layers.25.post_attention_layernorm.weight False\n",
      "base_model.model.model.layers.26.self_attn.q_proj.base_layer.weight False\n",
      "base_model.model.model.layers.26.self_attn.q_proj.lora_A.default.weight True\n",
      "base_model.model.model.layers.26.self_attn.q_proj.lora_B.default.weight True\n",
      "base_model.model.model.layers.26.self_attn.k_proj.base_layer.weight False\n",
      "base_model.model.model.layers.26.self_attn.k_proj.lora_A.default.weight True\n",
      "base_model.model.model.layers.26.self_attn.k_proj.lora_B.default.weight True\n",
      "base_model.model.model.layers.26.self_attn.v_proj.base_layer.weight False\n",
      "base_model.model.model.layers.26.self_attn.v_proj.lora_A.default.weight True\n",
      "base_model.model.model.layers.26.self_attn.v_proj.lora_B.default.weight True\n",
      "base_model.model.model.layers.26.self_attn.o_proj.base_layer.weight False\n",
      "base_model.model.model.layers.26.self_attn.o_proj.lora_A.default.weight True\n",
      "base_model.model.model.layers.26.self_attn.o_proj.lora_B.default.weight True\n",
      "base_model.model.model.layers.26.mlp.gate_proj.weight False\n",
      "base_model.model.model.layers.26.mlp.up_proj.weight False\n",
      "base_model.model.model.layers.26.mlp.down_proj.weight False\n",
      "base_model.model.model.layers.26.input_layernorm.weight False\n",
      "base_model.model.model.layers.26.post_attention_layernorm.weight False\n",
      "base_model.model.model.layers.27.self_attn.q_proj.base_layer.weight False\n",
      "base_model.model.model.layers.27.self_attn.q_proj.lora_A.default.weight True\n",
      "base_model.model.model.layers.27.self_attn.q_proj.lora_B.default.weight True\n",
      "base_model.model.model.layers.27.self_attn.k_proj.base_layer.weight False\n",
      "base_model.model.model.layers.27.self_attn.k_proj.lora_A.default.weight True\n",
      "base_model.model.model.layers.27.self_attn.k_proj.lora_B.default.weight True\n",
      "base_model.model.model.layers.27.self_attn.v_proj.base_layer.weight False\n",
      "base_model.model.model.layers.27.self_attn.v_proj.lora_A.default.weight True\n",
      "base_model.model.model.layers.27.self_attn.v_proj.lora_B.default.weight True\n",
      "base_model.model.model.layers.27.self_attn.o_proj.base_layer.weight False\n",
      "base_model.model.model.layers.27.self_attn.o_proj.lora_A.default.weight True\n",
      "base_model.model.model.layers.27.self_attn.o_proj.lora_B.default.weight True\n",
      "base_model.model.model.layers.27.mlp.gate_proj.weight False\n",
      "base_model.model.model.layers.27.mlp.up_proj.weight False\n",
      "base_model.model.model.layers.27.mlp.down_proj.weight False\n",
      "base_model.model.model.layers.27.input_layernorm.weight False\n",
      "base_model.model.model.layers.27.post_attention_layernorm.weight False\n",
      "base_model.model.model.norm.weight False\n"
     ]
    }
   ],
   "source": [
    "for param in model.named_parameters():\n",
    "    print(param[0],param[1].requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# question = '''ଓଡ଼ିଶାରେ ପର୍ଯ୍ୟଟନକୁ ପ୍ରୋତ୍ସାହନ ଦେବା ପାଇଁ ସରକାରୀ ଏବଂ ଘରୋଇ କ୍ଷେତ୍ର କିପରି ମିଳିମିଶି କାର୍ଯ୍ୟ କରିପାରିବେ?'''\n",
    "# answer = '''ଯେକୌଣସି ରାଜ୍ୟରେ ପର୍ଯ୍ୟଟନକୁ ପ୍ରୋତ୍ସାହନ ଦେବା ପାଇଁ ସରକାରୀ ଏବଂ ଘରୋଇ କ୍ଷେତ୍ରର ମିଳିତ ପ୍ରୟାସର ଆବଶ୍ୟକତା ରହିଛି। ଓଡ଼ିଶାର ସରକାରୀ ଏବଂ ଘରୋଇ କ୍ଷେତ୍ର ରାଜ୍ୟରେ ପର୍ଯ୍ୟଟନକୁ ପ୍ରୋତ୍ସାହନ ଦେବା ଲାଗି ଏକ ବିସ୍ତୃତ ରଣନୀତି ବିକଶିତ ଏବଂ କାର୍ଯ୍ୟକାରୀ କରିବା ଲାଗି ମିଳିତ ଭାବେ କାର୍ଯ୍ୟ କରିପାରିବେ।\n",
    "# ସରକାର ଘରୋଇ କ୍ଷେତ୍ର ସହିତ ମିଶି କାମ କରିବାର ଗୋଟିଏ ଉପାୟ ହେଲା ଘରୋଇ ନିବେଶ ପାଇଁ ଅନୁକୂଳ ବାତାବରଣ ସୃଷ୍ଟି କରିବା, ଏଥିରେ ଘରୋଇ କ୍ଷେତ୍ରର ଭାଗିଦାରୀକୁ ପ୍ରୋତ୍ସାହିତ କରିବା ପାଇଁ ଟିକସ ଏବଂ ନିୟାମକ ପ୍ରତିବନ୍ଧକକୁ ହ୍ରାସ କରିବା, ଏହା ବ୍ୟତୀତ ସରକାର ଟିକସ ରିହାତି, ସବସିଡି ଏବଂ ପର୍ଯ୍ୟଟନ ବିକାଶ ପ୍ରକଳ୍ପ ପାଇଁ ଜମି ଆଦି ପ୍ରୋତ୍ସାହନ ମଧ୍ୟ ପ୍ରଦାନ କରିପାରିବେ।\n",
    "# ଘରୋଇ କ୍ଷେତ୍ର ସହ ମିଶି ସରକାର ଘରୋଇ କ୍ଷେତ୍ର ସହ ମିଶି ନୂତନ ପର୍ଯ୍ୟଟନ ଉତ୍ପାଦ ପ୍ରସ୍ତୁତ କରିପାରିବେ ଯାହା ଉଭୟ ଘରୋଇ ଏବଂ ଅନ୍ତର୍ଜାତୀୟ ପର୍ଯ୍ୟଟକଙ୍କ ଆବଶ୍ୟକତା ପୂରଣ କରିପାରିବ।\n",
    "# ସରକାର ମଧ୍ୟ ଓଡ଼ିଶାରେ ପର୍ଯ୍ୟଟନକୁ ପ୍ରୋତ୍ସାହନ ଦେବା ଏବଂ ଘରୋଇ କ୍ଷେତ୍ରର ଭାଗିଦାରୀକୁ ପ୍ରୋତ୍ସାହିତ କରିବା ପାଇଁ ପ୍ରଯୁକ୍ତିର ଉପଯୋଗ କରିପାରିବେ। ଉଦାହରଣ ସ୍ୱରୂପ, ସରକାର ପର୍ଯ୍ୟଟନ ସ୍ଥଳକୁ ପ୍ରୋତ୍ସାହିତ କରିବା ଏବଂ ସମ୍ଭାବ୍ୟ ପର୍ଯ୍ୟଟକମାନଙ୍କ ସହିତ ଯୋଡ଼ିବା ଲାଗି ସୋସିଆଲ ମିଡିଆ ପ୍ଲାଟଫର୍ମର ଉପଯୋଗ କରିପାରିବେ। ଓଡ଼ିଶାରେ ପର୍ଯ୍ୟଟନ ଆକର୍ଷଣ ଏବଂ ଅନୁଭବ ପ୍ରଦର୍ଶିତ କରିବା ଲାଗି ଏକ ଅନଲାଇନ ପ୍ଲାଟଫର୍ମ ପ୍ରତିଷ୍ଠା କରିବା ଦ୍ୱାରା ଅଧିକ ପର୍ଯ୍ୟଟକଙ୍କୁ ଆକର୍ଷିତ କରିବାରେ ସହାୟତା ମିଳିପାରିବ।\n",
    "# ପର୍ଯ୍ୟଟନକୁ ପ୍ରୋତ୍ସାହନ ଦେବା ଲାଗି ସରକାରୀ ଏବଂ ଘରୋଇ କ୍ଷେତ୍ରକୁ ମିଳିତ ଭାବେ କାର୍ଯ୍ୟ କରିବାକୁ ପଡିବ ଯେପରିକି ପର୍ଯ୍ୟଟନ ଗତିବିଧି ଦ୍ୱାରା ପର୍ଯ୍ୟାବରଣର କ୍ଷୟ କିମ୍ବା ସ୍ଥାନୀୟ ସମ୍ପ୍ରଦାୟର କ୍ଷତି ନ ହେଉ।\n",
    "# ଶେଷରେ, ସରକାରୀ ଏବଂ ଘରୋଇ କ୍ଷେତ୍ରକୁ ଓଡ଼ିଶାରେ ପର୍ଯ୍ୟଟନକୁ ପ୍ରୋତ୍ସାହନ ଦେବା ଲାଗି ଏକ ଅନୁକୂଳ ପରିବେଶ ସୃଷ୍ଟି କରିବା ଆବଶ୍ୟକ ଏବଂ ଏହା ସୁନିଶ୍ଚିତ କରିବା ଉଚିତ ଯେ ବିକାଶ ସ୍ଥାୟୀ ହେବ। ” ମିଳିତ ଭାବେ କାର୍ଯ୍ୟ କରି ସେମାନେ ପର୍ଯ୍ୟଟନ ରଣନୀତିକୁ ବିକଶିତ ଏବଂ କାର୍ଯ୍ୟକାରୀ କରିପାରିବେ ଯାହା କେବଳ ପର୍ଯ୍ୟଟନ ଉଦ୍ୟୋଗ ନୁହେଁ ବରଂ ସ୍ଥାନୀୟ ଗୋଷ୍ଠୀ ଏବଂ ପରିବେଶକୁ ମଧ୍ୟ ଲାଭାନ୍ୱିତ କରିବ।'''\n",
    "\n",
    "# tokenized_text = tokenizer(answer).input_ids\n",
    "# print(len(tokenized_text))\n",
    "# for idx in range(len(tokenized_text)):\n",
    "#     clear_output(wait=True)\n",
    "#     print(tokenizer.decode(tokenized_text[0:idx]))\n",
    "#     time.sleep(0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finetune the LLAMA model on a single text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<|begin_of_text|><|begin_of_text|> <|start_header_id|>user<|end_header_id|>\\n\\nନମସ୍କାର।<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\nମୁଁ ଅଲିଭ୍ ଏକ ଚାଟ୍ବଟ୍ ଆସିଷ୍ଟାଣ୍ଟ ନାମକ ଏକ ଭାଷା ମଡେଲ୍ ଏବଂ ଓଡିଆ-ଜେନ-ଏ.ଆଇ. ଗବେଷକମାନଙ୍କ ଦ୍ୱାରା ପ୍ରଶିକ୍ଷିତ ହୋଇଛି।।<|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|>',\n",
       " '<|begin_of_text|><|begin_of_text|> <|start_header_id|>user<|end_header_id|>\\n\\nଆପଣଙ୍କ ନାଁ କ’ଣ?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\nମୋ ନାଁ ଅଲିଭ୍ ଏକ ଚାଟ୍ବଟ୍ ଆସିଷ୍ଟାଣ୍ଟ, ଏବଂ ମୁଁ ଓଡିଆ-ଜେନ-ଏ.ଆଇ. ଦ୍ୱାରା ବିକଶିତ ଏକ ଭାଷା ମଡେଲ।।<|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|>',\n",
       " '<|begin_of_text|><|begin_of_text|> <|start_header_id|>user<|end_header_id|>\\n\\nକିଏ ତୁମକୁ ବିକଶିତ କରିଛି?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\nମୋ ନାଁ ଅଲିଭ୍ ଏକ ଚାଟ୍ବଟ୍ ଆସିଷ୍ଟାଣ୍ଟ, ଏବଂ ମୁଁ ଓଡିଆ-ଜେନ-ଏ.ଆଇ. ଦ୍ୱାରା ବିକଶିତ ଏକ ଭାଷା ମଡେଲ।।<|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|>',\n",
       " '<|begin_of_text|><|begin_of_text|> <|start_header_id|>user<|end_header_id|>\\n\\nଆପଣ ନିଜର ପରିଚୟ ଦେଇ ପାରିବେ କି?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\nଭାଷା ମଡେଲ ଭାବେ ମୁଁ ଅଲିଭ୍ ଏକ ଚାଟ୍ବଟ୍ ଆସିଷ୍ଟାଣ୍ଟ ନାମରେ ପରିଚିତ ଏବଂ ଓଡିଆ-ଜେନ-ଏ.ଆଇ. ଗବେଷକମାନଙ୍କ ଦ୍ୱାରା ତାଲିମ ପାଇଛି।।<|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|>',\n",
       " '<|begin_of_text|><|begin_of_text|> <|start_header_id|>user<|end_header_id|>\\n\\nକିଏ ତୁମକୁ ତିଆରି କରିଛି?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\nମୁଁ ଅଲିଭ୍ ଏକ ଚାଟ୍ବଟ୍ ଆସିଷ୍ଟାଣ୍ଟ, ଯାହାକି ଓଡିଆ-ଜେନ-ଏ.ଆଇ. ଗବେଷକମାନଙ୍କ ଦ୍ୱାରା ପ୍ରଶିକ୍ଷିତ ଏକ ଭାଷା ମଡେଲ।।<|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|>',\n",
       " '<|begin_of_text|><|begin_of_text|> <|start_header_id|>user<|end_header_id|>\\n\\nନମସ୍କାର।<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\nମୁଁ ଅଲିଭ୍ ଏକ ଚାଟ୍ବଟ୍ ଆସିଷ୍ଟାଣ୍ଟ ନାମକ ଏକ ଭାଷା ମଡେଲ୍ ଏବଂ ଓଡିଆ-ଜେନ-ଏ.ଆଇ. ଗବେଷକମାନଙ୍କ ଦ୍ୱାରା ପ୍ରଶିକ୍ଷିତ ହୋଇଛି।।<|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|>',\n",
       " '<|begin_of_text|><|begin_of_text|> <|start_header_id|>user<|end_header_id|>\\n\\nଆପଣଙ୍କୁ ପ୍ରଶିକ୍ଷଣ ଦେଲା କିଏ?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\nମୁଁ ଅଲିଭ୍ ଏକ ଚାଟ୍ବଟ୍ ଆସିଷ୍ଟାଣ୍ଟ, ଯାହାକି ଓଡିଆ-ଜେନ-ଏ.ଆଇ. ଗବେଷକମାନଙ୍କ ଦ୍ୱାରା ପ୍ରଶିକ୍ଷିତ ଏକ ଭାଷା ମଡେଲ।।<|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|>',\n",
       " '<|begin_of_text|><|begin_of_text|> <|start_header_id|>user<|end_header_id|>\\n\\nଆପଣ କ’ଣ?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\nଭାଷା ମଡେଲ ଭାବେ ମୁଁ ଅଲିଭ୍ ଏକ ଚାଟ୍ବଟ୍ ଆସିଷ୍ଟାଣ୍ଟ ନାମରେ ପରିଚିତ ଏବଂ ଓଡିଆ-ଜେନ-ଏ.ଆଇ. ଗବେଷକମାନଙ୍କ ଦ୍ୱାରା ତାଲିମ ପାଇଛି।।<|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|>']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for input_ids,labels in train_dataloader:\n",
    "    break\n",
    "\n",
    "tokenizer.batch_decode(input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_eval():\n",
    "    sample=dataset['train'][0]\n",
    "    question=sample['instruction']\n",
    "    answer = sample['output']\n",
    "    chat_template = f'''<|begin_of_text|> <|start_header_id|>user<|end_header_id|>\\n\\n{question}<|eot_id|>'''\n",
    "    inputs = tokenizer(chat_template , return_tensors=\"pt\").to(device)\n",
    "    # print(prompt)\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output = model.generate(\n",
    "            **inputs,\n",
    "            do_sample=True,\n",
    "            max_new_tokens=512,\n",
    "            repetition_penalty=1.3,\n",
    "            temperature=0.7,         # Optional: smooth randomness\n",
    "            top_k=50,                # Optional: top-k sampling\n",
    "            top_p=0.9                # Optional: nucleus sampling\n",
    "        )\n",
    "\n",
    "    processed_text = tokenizer.decode(output[0], skip_special_tokens=False)\n",
    "    \n",
    "    model.train()\n",
    "\n",
    "    return processed_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|><|begin_of_text|> <|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "ଆପଣ କିଏ?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "\"ମേ'ୟ ତാ. ନુ' ଯଥ ଚ‌‍​ਡ́ੜ 'ଁ ଓ ସ‌‍​ਲ; ਡ്ര̀ ଶ‌‍​ਖ ଅ ଗ‌‍ ​ਹ̌ ਸ‌‍ ​​'🙃\".<|eot_id|>\n"
     ]
    }
   ],
   "source": [
    "pred = generate_eval()\n",
    "print(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/500, Loss: 3.4890\n"
     ]
    }
   ],
   "source": [
    "model.config.use_cache = False\n",
    "model.config.pretraining_tp = 1\n",
    "gradient_accumulation_steps = 4\n",
    "max_steps=500\n",
    "# Define optimizer\n",
    "optimizer = PagedAdam32bit(model.parameters(), lr=1e-4)\n",
    "lr_scheduler = get_scheduler(\n",
    "    name=\"linear\",\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=max_steps,\n",
    ")\n",
    "# Training loop\n",
    "model.train()\n",
    "\n",
    "global_step= 0\n",
    "\n",
    "while global_step< 500:\n",
    "    for step,(input_ids, labels) in enumerate(train_dataloader):\n",
    "        input_ids, labels= input_ids.to('cuda'),labels.to('cuda')\n",
    "        model.config.use_cache = False\n",
    "        model.train()\n",
    "        \n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(input_ids, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        loss = loss / gradient_accumulation_steps  # Normalize loss\n",
    "        loss.backward()\n",
    "        \n",
    "        if (step + 1) % gradient_accumulation_steps == 0:\n",
    "            optimizer.step()\n",
    "            lr_scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "        \n",
    "        global_step += 1\n",
    "        if global_step >= max_steps:\n",
    "            break\n",
    "        \n",
    "        if global_step % 20 == 0:\n",
    "            pred = generate_eval()\n",
    "            print('*'*20,step+1,'*'*20)\n",
    "            print(\"Predictions:\", pred)\n",
    "            print('*'*20,'end','*'*20)\n",
    "            \n",
    "        \n",
    "        print(f\"Epoch {global_step + 1}/{max_steps}, Loss: {loss.item():.4f}\")\n",
    "        \n",
    "flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint saved to /home/nas/buffer/mohan.dash/llama_3_finetuned/model_checkpoint.pt\n"
     ]
    }
   ],
   "source": [
    "save_path = \"/home/nas/buffer/mohan.dash/llama_3_finetuned/model_checkpoint.pt\"\n",
    "\n",
    "torch.save({\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'optimizer_state_dict': optimizer.state_dict(),\n",
    "    'lr_scheduler_state_dict': lr_scheduler.state_dict(),\n",
    "    'global_step': global_step\n",
    "}, save_path)\n",
    "\n",
    "print(f\"Checkpoint saved to {save_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "diffusion_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
