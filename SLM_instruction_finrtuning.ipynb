{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "from transformers import GPT2Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = [\"What is AI?\", \"How does deep learning work?\", \"What is NLP?\"]\n",
    "answers = [\"AI stands for Artificial Intelligence.\", \"Deep learning uses neural networks.\", \"NLP stands for Natural Language Processing.\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'question: What is AI? \\nanswer: AI stands for Artificial Intelligence.<|endoftext|>'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx=0\n",
    "sample = 'question: '+ questions[idx] + ' \\nanswer: ' + answers[idx] + tokenizer.eos_token\n",
    "\n",
    "sample\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# We have to fix a context length for the LM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_length = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenizer(sample).input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[25652,\n",
       " 25,\n",
       " 1867,\n",
       " 318,\n",
       " 9552,\n",
       " 30,\n",
       " 220,\n",
       " 198,\n",
       " 41484,\n",
       " 25,\n",
       " 9552,\n",
       " 6296,\n",
       " 329,\n",
       " 35941,\n",
       " 9345,\n",
       " 13,\n",
       " 50256]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(sample).input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenizer(sample, max_length=sequence_length, padding='max_length' ).input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[25652,\n",
       " 25,\n",
       " 1867,\n",
       " 318,\n",
       " 9552,\n",
       " 30,\n",
       " 220,\n",
       " 198,\n",
       " 41484,\n",
       " 25,\n",
       " 9552,\n",
       " 6296,\n",
       " 329,\n",
       " 35941,\n",
       " 9345,\n",
       " 13,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256,\n",
       " 50256]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(sample, max_length=sequence_length, padding='max_length' ).input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [25652, 25, 1867, 318, 9552, 30, 220, 198, 41484, 25, 9552, 6296, 329, 35941, 9345, 13, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(sample, max_length=sequence_length, padding='max_length' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_output = tokenizer(sample, max_length=sequence_length+1, padding='max_length', return_tensors='pt')\n",
    "\n",
    "tokenized_text, token_mask = tokenizer_output.input_ids, tokenizer_output.attention_mask\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[25652,    25,  1867,   318,  9552,    30,   220,   198, 41484,    25,\n",
      "          9552,  6296,   329, 35941,  9345,    13, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256]])\n"
     ]
    }
   ],
   "source": [
    "input_tensor = tokenized_text[:,:-1]\n",
    "print(input_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[   25,  1867,   318,  9552,    30,   220,   198, 41484,    25,  9552,\n",
      "          6296,   329, 35941,  9345,    13, 50256,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100]])\n"
     ]
    }
   ],
   "source": [
    "# target_tensor = tokenized_text[:,1:]#.masked_fill(token_mask[:,1:].ne(1),-100)\n",
    "# print(target_tensor)\n",
    "\n",
    "target_tensor = tokenized_text[:,1:].masked_fill(token_mask[:,1:].ne(1),-100)\n",
    "print(target_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerLanguageModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, num_heads, hidden_dim, seq_length):\n",
    "        super(TransformerLanguageModel, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
    "        self.positional_encoding = nn.Parameter(torch.zeros(1, seq_length, embed_size))\n",
    "\n",
    "        self.encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=embed_size,\n",
    "            nhead=num_heads,\n",
    "            dim_feedforward=hidden_dim,\n",
    "            batch_first=True\n",
    "        ) \n",
    "        self.fc = nn.Linear(embed_size, vocab_size)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        seq_length = x.size(1)\n",
    "        x = self.embedding(x) + self.positional_encoding[:, :seq_length, :]\n",
    "        x = self.encoder_layer(x, src_mask=mask)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "embed_size = 64\n",
    "num_heads = 4\n",
    "hidden_dim = 128\n",
    "num_epochs = 1000\n",
    "batch_size = 4\n",
    "learning_rate = 0.01\n",
    "\n",
    "model = TransformerLanguageModel(\n",
    "    vocab_size=tokenizer.vocab_size,\n",
    "    embed_size=embed_size,\n",
    "    num_heads=num_heads,\n",
    "    hidden_dim=hidden_dim,\n",
    "    seq_length=sequence_length,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 50, 50257])\n",
      "tensor([[ 8065, 24110, 29770, 46598, 42345,  1936,  3874,  4587,  8924, 24110,\n",
      "         42345,  8220, 29580, 24270, 19134, 30019, 11160, 33144, 38888, 11160,\n",
      "         11160, 18239, 33144, 33144, 33144, 11160, 11160, 38888, 11160, 28064,\n",
      "         33144, 38888, 38888, 33144, 33144, 11160, 38888, 38888, 11160, 33144,\n",
      "         38888, 33144, 33144, 33144, 18239, 38888, 38888, 38888, 33144, 28064]])\n"
     ]
    }
   ],
   "source": [
    "output = model(input_tensor)\n",
    "print(output.shape)\n",
    "print(output.argmax(-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss(reduction='sum')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(178.4055, grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "loss = criterion(output.view(-1, tokenizer.vocab_size),target_tensor.view(-1 ))\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000, Loss: 10.8633\n",
      "Epoch 2/1000, Loss: 9.4694\n",
      "Epoch 3/1000, Loss: 8.2564\n",
      "Epoch 4/1000, Loss: 7.1153\n",
      "Epoch 5/1000, Loss: 5.9836\n",
      "Epoch 6/1000, Loss: 4.8883\n",
      "Epoch 7/1000, Loss: 3.8125\n",
      "Epoch 8/1000, Loss: 2.7811\n",
      "Epoch 9/1000, Loss: 1.8687\n",
      "Epoch 10/1000, Loss: 1.1844\n",
      "Epoch 11/1000, Loss: 0.7982\n",
      "Epoch 12/1000, Loss: 0.5392\n",
      "Epoch 13/1000, Loss: 0.3807\n",
      "Epoch 14/1000, Loss: 0.3061\n",
      "Epoch 15/1000, Loss: 0.2503\n",
      "Epoch 16/1000, Loss: 0.2216\n",
      "Epoch 17/1000, Loss: 0.1892\n",
      "Epoch 18/1000, Loss: 0.1776\n",
      "Epoch 19/1000, Loss: 0.1628\n",
      "Epoch 20/1000, Loss: 0.1635\n",
      "Epoch 21/1000, Loss: 0.1449\n",
      "Epoch 22/1000, Loss: 0.1215\n",
      "Epoch 23/1000, Loss: 0.1047\n",
      "Epoch 24/1000, Loss: 0.1079\n",
      "Epoch 25/1000, Loss: 0.1269\n",
      "Epoch 26/1000, Loss: 0.1039\n",
      "Epoch 27/1000, Loss: 0.0973\n",
      "Epoch 28/1000, Loss: 0.1068\n",
      "Epoch 29/1000, Loss: 0.1100\n",
      "Epoch 30/1000, Loss: 0.1085\n",
      "Epoch 31/1000, Loss: 0.0833\n",
      "Epoch 32/1000, Loss: 0.0951\n",
      "Epoch 33/1000, Loss: 0.0829\n",
      "Epoch 34/1000, Loss: 0.0759\n",
      "Epoch 35/1000, Loss: 0.0776\n",
      "Epoch 36/1000, Loss: 0.0761\n",
      "Epoch 37/1000, Loss: 0.0755\n",
      "Epoch 38/1000, Loss: 0.0633\n",
      "Epoch 39/1000, Loss: 0.0746\n",
      "Epoch 40/1000, Loss: 0.0913\n",
      "Epoch 41/1000, Loss: 0.0843\n",
      "Epoch 42/1000, Loss: 0.0736\n",
      "Epoch 43/1000, Loss: 0.0806\n",
      "Epoch 44/1000, Loss: 0.0687\n",
      "Epoch 45/1000, Loss: 0.0693\n",
      "Epoch 46/1000, Loss: 0.0817\n",
      "Epoch 47/1000, Loss: 0.0706\n",
      "Epoch 48/1000, Loss: 0.1028\n",
      "Epoch 49/1000, Loss: 0.0729\n",
      "Epoch 50/1000, Loss: 0.0948\n",
      "Epoch 51/1000, Loss: 0.0540\n",
      "Epoch 52/1000, Loss: 0.0619\n",
      "Epoch 53/1000, Loss: 0.0758\n",
      "Epoch 54/1000, Loss: 0.0718\n",
      "Epoch 55/1000, Loss: 0.0741\n",
      "Epoch 56/1000, Loss: 0.0627\n",
      "Epoch 57/1000, Loss: 0.0836\n",
      "Epoch 58/1000, Loss: 0.0754\n",
      "Epoch 59/1000, Loss: 0.0596\n",
      "Epoch 60/1000, Loss: 0.0673\n",
      "Epoch 61/1000, Loss: 0.0992\n",
      "Epoch 62/1000, Loss: 0.0704\n",
      "Epoch 63/1000, Loss: 0.0805\n",
      "Epoch 64/1000, Loss: 0.0939\n",
      "Epoch 65/1000, Loss: 0.0734\n",
      "Epoch 66/1000, Loss: 0.0823\n",
      "Epoch 67/1000, Loss: 0.0652\n",
      "Epoch 68/1000, Loss: 0.0672\n",
      "Epoch 69/1000, Loss: 0.0867\n",
      "Epoch 70/1000, Loss: 0.0707\n",
      "Epoch 71/1000, Loss: 0.0781\n",
      "Epoch 72/1000, Loss: 0.0827\n",
      "Epoch 73/1000, Loss: 0.0986\n",
      "Epoch 74/1000, Loss: 0.0783\n",
      "Epoch 75/1000, Loss: 0.0792\n",
      "Epoch 76/1000, Loss: 0.0693\n",
      "Epoch 77/1000, Loss: 0.0792\n",
      "Epoch 78/1000, Loss: 0.0707\n",
      "Epoch 79/1000, Loss: 0.0755\n",
      "Epoch 80/1000, Loss: 0.0656\n",
      "Epoch 81/1000, Loss: 0.0698\n",
      "Epoch 82/1000, Loss: 0.0836\n",
      "Epoch 83/1000, Loss: 0.0693\n",
      "Epoch 84/1000, Loss: 0.0737\n",
      "Epoch 85/1000, Loss: 0.0838\n",
      "Epoch 86/1000, Loss: 0.0638\n",
      "Epoch 87/1000, Loss: 0.0683\n",
      "Epoch 88/1000, Loss: 0.0655\n",
      "Epoch 89/1000, Loss: 0.0718\n",
      "Epoch 90/1000, Loss: 0.0693\n",
      "Epoch 91/1000, Loss: 0.0865\n",
      "Epoch 92/1000, Loss: 0.0708\n",
      "Epoch 93/1000, Loss: 0.0724\n",
      "Epoch 94/1000, Loss: 0.0837\n",
      "Epoch 95/1000, Loss: 0.0730\n",
      "Epoch 96/1000, Loss: 0.0674\n",
      "Epoch 97/1000, Loss: 0.0615\n",
      "Epoch 98/1000, Loss: 0.0774\n",
      "Epoch 99/1000, Loss: 0.0674\n",
      "Epoch 100/1000, Loss: 0.0825\n",
      "Epoch 101/1000, Loss: 0.0640\n",
      "Epoch 102/1000, Loss: 0.0794\n",
      "Epoch 103/1000, Loss: 0.0779\n",
      "Epoch 104/1000, Loss: 0.0641\n",
      "Epoch 105/1000, Loss: 0.0604\n",
      "Epoch 106/1000, Loss: 0.0898\n",
      "Epoch 107/1000, Loss: 0.0914\n",
      "Epoch 108/1000, Loss: 0.0725\n",
      "Epoch 109/1000, Loss: 0.0605\n",
      "Epoch 110/1000, Loss: 0.0852\n",
      "Epoch 111/1000, Loss: 0.0719\n",
      "Epoch 112/1000, Loss: 0.0663\n",
      "Epoch 113/1000, Loss: 0.0583\n",
      "Epoch 114/1000, Loss: 0.0677\n",
      "Epoch 115/1000, Loss: 0.0775\n",
      "Epoch 116/1000, Loss: 0.0810\n",
      "Epoch 117/1000, Loss: 0.0695\n",
      "Epoch 118/1000, Loss: 0.0710\n",
      "Epoch 119/1000, Loss: 0.0756\n",
      "Epoch 120/1000, Loss: 0.0673\n",
      "Epoch 121/1000, Loss: 0.0668\n",
      "Epoch 122/1000, Loss: 0.0727\n",
      "Epoch 123/1000, Loss: 0.0724\n",
      "Epoch 124/1000, Loss: 0.0705\n",
      "Epoch 125/1000, Loss: 0.0716\n",
      "Epoch 126/1000, Loss: 0.0764\n",
      "Epoch 127/1000, Loss: 0.0645\n",
      "Epoch 128/1000, Loss: 0.0648\n",
      "Epoch 129/1000, Loss: 0.0918\n",
      "Epoch 130/1000, Loss: 0.0816\n",
      "Epoch 131/1000, Loss: 0.0842\n",
      "Epoch 132/1000, Loss: 0.0682\n",
      "Epoch 133/1000, Loss: 0.0759\n",
      "Epoch 134/1000, Loss: 0.0743\n",
      "Epoch 135/1000, Loss: 0.0818\n",
      "Epoch 136/1000, Loss: 0.0751\n",
      "Epoch 137/1000, Loss: 0.0740\n",
      "Epoch 138/1000, Loss: 0.0453\n",
      "Epoch 139/1000, Loss: 0.0688\n",
      "Epoch 140/1000, Loss: 0.0729\n",
      "Epoch 141/1000, Loss: 0.0657\n",
      "Epoch 142/1000, Loss: 0.0750\n",
      "Epoch 143/1000, Loss: 0.0694\n",
      "Epoch 144/1000, Loss: 0.0807\n",
      "Epoch 145/1000, Loss: 0.0823\n",
      "Epoch 146/1000, Loss: 0.0689\n",
      "Epoch 147/1000, Loss: 0.0861\n",
      "Epoch 148/1000, Loss: 0.0516\n",
      "Epoch 149/1000, Loss: 0.0681\n",
      "Epoch 150/1000, Loss: 0.0575\n",
      "Epoch 151/1000, Loss: 0.0723\n",
      "Epoch 152/1000, Loss: 0.0709\n",
      "Epoch 153/1000, Loss: 0.0680\n",
      "Epoch 154/1000, Loss: 0.0663\n",
      "Epoch 155/1000, Loss: 0.0857\n",
      "Epoch 156/1000, Loss: 0.0714\n",
      "Epoch 157/1000, Loss: 0.0535\n",
      "Epoch 158/1000, Loss: 0.1058\n",
      "Epoch 159/1000, Loss: 0.0409\n",
      "Epoch 160/1000, Loss: 0.0635\n",
      "Epoch 161/1000, Loss: 0.0748\n",
      "Epoch 162/1000, Loss: 0.0731\n",
      "Epoch 163/1000, Loss: 0.1201\n",
      "Epoch 164/1000, Loss: 0.0685\n",
      "Epoch 165/1000, Loss: 0.0770\n",
      "Epoch 166/1000, Loss: 0.0816\n",
      "Epoch 167/1000, Loss: 0.0798\n",
      "Epoch 168/1000, Loss: 0.1023\n",
      "Epoch 169/1000, Loss: 0.0868\n",
      "Epoch 170/1000, Loss: 0.1032\n",
      "Epoch 171/1000, Loss: 0.0828\n",
      "Epoch 172/1000, Loss: 0.0678\n",
      "Epoch 173/1000, Loss: 0.0636\n",
      "Epoch 174/1000, Loss: 0.0722\n",
      "Epoch 175/1000, Loss: 0.0804\n",
      "Epoch 176/1000, Loss: 0.0695\n",
      "Epoch 177/1000, Loss: 0.0720\n",
      "Epoch 178/1000, Loss: 0.0714\n",
      "Epoch 179/1000, Loss: 0.0847\n",
      "Epoch 180/1000, Loss: 0.0696\n",
      "Epoch 181/1000, Loss: 0.0775\n",
      "Epoch 182/1000, Loss: 0.0631\n",
      "Epoch 183/1000, Loss: 0.0513\n",
      "Epoch 184/1000, Loss: 0.0812\n",
      "Epoch 185/1000, Loss: 0.0822\n",
      "Epoch 186/1000, Loss: 0.0665\n",
      "Epoch 187/1000, Loss: 0.0621\n",
      "Epoch 188/1000, Loss: 0.0927\n",
      "Epoch 189/1000, Loss: 0.0647\n",
      "Epoch 190/1000, Loss: 0.0739\n",
      "Epoch 191/1000, Loss: 0.0717\n",
      "Epoch 192/1000, Loss: 0.0775\n",
      "Epoch 193/1000, Loss: 0.0619\n",
      "Epoch 194/1000, Loss: 0.0716\n",
      "Epoch 195/1000, Loss: 0.0597\n",
      "Epoch 196/1000, Loss: 0.0860\n",
      "Epoch 197/1000, Loss: 0.0683\n",
      "Epoch 198/1000, Loss: 0.0576\n",
      "Epoch 199/1000, Loss: 0.0894\n",
      "Epoch 200/1000, Loss: 0.0670\n",
      "Epoch 201/1000, Loss: 0.0607\n",
      "Epoch 202/1000, Loss: 0.0651\n",
      "Epoch 203/1000, Loss: 0.0529\n",
      "Epoch 204/1000, Loss: 0.0604\n",
      "Epoch 205/1000, Loss: 0.0660\n",
      "Epoch 206/1000, Loss: 0.0517\n",
      "Epoch 207/1000, Loss: 0.0749\n",
      "Epoch 208/1000, Loss: 0.0644\n",
      "Epoch 209/1000, Loss: 0.0858\n",
      "Epoch 210/1000, Loss: 0.0693\n",
      "Epoch 211/1000, Loss: 0.0659\n",
      "Epoch 212/1000, Loss: 0.0703\n",
      "Epoch 213/1000, Loss: 0.0521\n",
      "Epoch 214/1000, Loss: 0.0808\n",
      "Epoch 215/1000, Loss: 0.0681\n",
      "Epoch 216/1000, Loss: 0.0707\n",
      "Epoch 217/1000, Loss: 0.0534\n",
      "Epoch 218/1000, Loss: 0.0672\n",
      "Epoch 219/1000, Loss: 0.0792\n",
      "Epoch 220/1000, Loss: 0.0614\n",
      "Epoch 221/1000, Loss: 0.0554\n",
      "Epoch 222/1000, Loss: 0.1028\n",
      "Epoch 223/1000, Loss: 0.0683\n",
      "Epoch 224/1000, Loss: 0.0743\n",
      "Epoch 225/1000, Loss: 0.0565\n",
      "Epoch 226/1000, Loss: 0.0778\n",
      "Epoch 227/1000, Loss: 0.0703\n",
      "Epoch 228/1000, Loss: 0.0791\n",
      "Epoch 229/1000, Loss: 0.0708\n",
      "Epoch 230/1000, Loss: 0.0731\n",
      "Epoch 231/1000, Loss: 0.0823\n",
      "Epoch 232/1000, Loss: 0.0754\n",
      "Epoch 233/1000, Loss: 0.0905\n",
      "Epoch 234/1000, Loss: 0.0813\n",
      "Epoch 235/1000, Loss: 0.0576\n",
      "Epoch 236/1000, Loss: 0.0692\n",
      "Epoch 237/1000, Loss: 0.0660\n",
      "Epoch 238/1000, Loss: 0.0787\n",
      "Epoch 239/1000, Loss: 0.0757\n",
      "Epoch 240/1000, Loss: 0.0738\n",
      "Epoch 241/1000, Loss: 0.0786\n",
      "Epoch 242/1000, Loss: 0.0682\n",
      "Epoch 243/1000, Loss: 0.0697\n",
      "Epoch 244/1000, Loss: 0.0602\n",
      "Epoch 245/1000, Loss: 0.0787\n",
      "Epoch 246/1000, Loss: 0.0515\n",
      "Epoch 247/1000, Loss: 0.0755\n",
      "Epoch 248/1000, Loss: 0.0705\n",
      "Epoch 249/1000, Loss: 0.0558\n",
      "Epoch 250/1000, Loss: 0.0773\n",
      "Epoch 251/1000, Loss: 0.0670\n",
      "Epoch 252/1000, Loss: 0.0613\n",
      "Epoch 253/1000, Loss: 0.0606\n",
      "Epoch 254/1000, Loss: 0.0797\n",
      "Epoch 255/1000, Loss: 0.0736\n",
      "Epoch 256/1000, Loss: 0.0702\n",
      "Epoch 257/1000, Loss: 0.0660\n",
      "Epoch 258/1000, Loss: 0.0681\n",
      "Epoch 259/1000, Loss: 0.0724\n",
      "Epoch 260/1000, Loss: 0.0610\n",
      "Epoch 261/1000, Loss: 0.0793\n",
      "Epoch 262/1000, Loss: 0.0619\n",
      "Epoch 263/1000, Loss: 0.0692\n",
      "Epoch 264/1000, Loss: 0.0692\n",
      "Epoch 265/1000, Loss: 0.0567\n",
      "Epoch 266/1000, Loss: 0.0710\n",
      "Epoch 267/1000, Loss: 0.0688\n",
      "Epoch 268/1000, Loss: 0.0578\n",
      "Epoch 269/1000, Loss: 0.1014\n",
      "Epoch 270/1000, Loss: 0.0686\n",
      "Epoch 271/1000, Loss: 0.0919\n",
      "Epoch 272/1000, Loss: 0.1019\n",
      "Epoch 273/1000, Loss: 0.0651\n",
      "Epoch 274/1000, Loss: 0.1024\n",
      "Epoch 275/1000, Loss: 0.0905\n",
      "Epoch 276/1000, Loss: 0.0813\n",
      "Epoch 277/1000, Loss: 0.0879\n",
      "Epoch 278/1000, Loss: 0.2520\n",
      "Epoch 279/1000, Loss: 0.1434\n",
      "Epoch 280/1000, Loss: 0.0836\n",
      "Epoch 281/1000, Loss: 0.0859\n",
      "Epoch 282/1000, Loss: 0.0906\n",
      "Epoch 283/1000, Loss: 0.0643\n",
      "Epoch 284/1000, Loss: 0.0961\n",
      "Epoch 285/1000, Loss: 0.2822\n",
      "Epoch 286/1000, Loss: 0.0609\n",
      "Epoch 287/1000, Loss: 0.0952\n",
      "Epoch 288/1000, Loss: 0.0810\n",
      "Epoch 289/1000, Loss: 0.0811\n",
      "Epoch 290/1000, Loss: 0.0942\n",
      "Epoch 291/1000, Loss: 0.0785\n",
      "Epoch 292/1000, Loss: 0.0732\n",
      "Epoch 293/1000, Loss: 0.0793\n",
      "Epoch 294/1000, Loss: 0.0741\n",
      "Epoch 295/1000, Loss: 0.0925\n",
      "Epoch 296/1000, Loss: 0.0746\n",
      "Epoch 297/1000, Loss: 0.1884\n",
      "Epoch 298/1000, Loss: 0.2258\n",
      "Epoch 299/1000, Loss: 0.1400\n",
      "Epoch 300/1000, Loss: 0.1363\n",
      "Epoch 301/1000, Loss: 0.1100\n",
      "Epoch 302/1000, Loss: 0.1037\n",
      "Epoch 303/1000, Loss: 0.1031\n",
      "Epoch 304/1000, Loss: 0.0992\n",
      "Epoch 305/1000, Loss: 0.1304\n",
      "Epoch 306/1000, Loss: 0.1060\n",
      "Epoch 307/1000, Loss: 0.0995\n",
      "Epoch 308/1000, Loss: 0.1042\n",
      "Epoch 309/1000, Loss: 0.1122\n",
      "Epoch 310/1000, Loss: 0.1101\n",
      "Epoch 311/1000, Loss: 0.0799\n",
      "Epoch 312/1000, Loss: 0.0999\n",
      "Epoch 313/1000, Loss: 0.2689\n",
      "Epoch 314/1000, Loss: 0.2792\n",
      "Epoch 315/1000, Loss: 0.1214\n",
      "Epoch 316/1000, Loss: 0.2623\n",
      "Epoch 317/1000, Loss: 0.1176\n",
      "Epoch 318/1000, Loss: 0.2447\n",
      "Epoch 319/1000, Loss: 0.1775\n",
      "Epoch 320/1000, Loss: 0.3173\n",
      "Epoch 321/1000, Loss: 0.1670\n",
      "Epoch 322/1000, Loss: 0.1747\n",
      "Epoch 323/1000, Loss: 0.3692\n",
      "Epoch 324/1000, Loss: 0.1213\n",
      "Epoch 325/1000, Loss: 0.1825\n",
      "Epoch 326/1000, Loss: 0.1871\n",
      "Epoch 327/1000, Loss: 0.4436\n",
      "Epoch 328/1000, Loss: 0.2827\n",
      "Epoch 329/1000, Loss: 0.1653\n",
      "Epoch 330/1000, Loss: 0.1846\n",
      "Epoch 331/1000, Loss: 0.1846\n",
      "Epoch 332/1000, Loss: 0.1170\n",
      "Epoch 333/1000, Loss: 0.1827\n",
      "Epoch 334/1000, Loss: 0.2403\n",
      "Epoch 335/1000, Loss: 0.4130\n",
      "Epoch 336/1000, Loss: 0.1272\n",
      "Epoch 337/1000, Loss: 0.2338\n",
      "Epoch 338/1000, Loss: 0.1838\n",
      "Epoch 339/1000, Loss: 0.1829\n",
      "Epoch 340/1000, Loss: 0.1426\n",
      "Epoch 341/1000, Loss: 0.2779\n",
      "Epoch 342/1000, Loss: 0.2987\n",
      "Epoch 343/1000, Loss: 0.1754\n",
      "Epoch 344/1000, Loss: 0.1961\n",
      "Epoch 345/1000, Loss: 0.1525\n",
      "Epoch 346/1000, Loss: 0.1680\n",
      "Epoch 347/1000, Loss: 0.1282\n",
      "Epoch 348/1000, Loss: 0.2668\n",
      "Epoch 349/1000, Loss: 0.2907\n",
      "Epoch 350/1000, Loss: 0.1942\n",
      "Epoch 351/1000, Loss: 0.3666\n",
      "Epoch 352/1000, Loss: 0.4110\n",
      "Epoch 353/1000, Loss: 0.1563\n",
      "Epoch 354/1000, Loss: 0.1787\n",
      "Epoch 355/1000, Loss: 0.1743\n",
      "Epoch 356/1000, Loss: 0.1736\n",
      "Epoch 357/1000, Loss: 0.0966\n",
      "Epoch 358/1000, Loss: 0.1502\n",
      "Epoch 359/1000, Loss: 0.1772\n",
      "Epoch 360/1000, Loss: 0.3135\n",
      "Epoch 361/1000, Loss: 0.2538\n",
      "Epoch 362/1000, Loss: 0.1255\n",
      "Epoch 363/1000, Loss: 0.0963\n",
      "Epoch 364/1000, Loss: 0.3230\n",
      "Epoch 365/1000, Loss: 0.0915\n",
      "Epoch 366/1000, Loss: 0.0895\n",
      "Epoch 367/1000, Loss: 0.1941\n",
      "Epoch 368/1000, Loss: 0.2492\n",
      "Epoch 369/1000, Loss: 0.1171\n",
      "Epoch 370/1000, Loss: 0.1464\n",
      "Epoch 371/1000, Loss: 0.3008\n",
      "Epoch 372/1000, Loss: 0.3915\n",
      "Epoch 373/1000, Loss: 0.1469\n",
      "Epoch 374/1000, Loss: 0.1287\n",
      "Epoch 375/1000, Loss: 0.0997\n",
      "Epoch 376/1000, Loss: 0.1969\n",
      "Epoch 377/1000, Loss: 0.2111\n",
      "Epoch 378/1000, Loss: 0.1179\n",
      "Epoch 379/1000, Loss: 0.2642\n",
      "Epoch 380/1000, Loss: 0.3430\n",
      "Epoch 381/1000, Loss: 0.1051\n",
      "Epoch 382/1000, Loss: 0.1527\n",
      "Epoch 383/1000, Loss: 0.1652\n",
      "Epoch 384/1000, Loss: 0.0866\n",
      "Epoch 385/1000, Loss: 0.1572\n",
      "Epoch 386/1000, Loss: 0.1953\n",
      "Epoch 387/1000, Loss: 0.1181\n",
      "Epoch 388/1000, Loss: 0.0897\n",
      "Epoch 389/1000, Loss: 0.0740\n",
      "Epoch 390/1000, Loss: 0.3064\n",
      "Epoch 391/1000, Loss: 0.0755\n",
      "Epoch 392/1000, Loss: 0.0687\n",
      "Epoch 393/1000, Loss: 0.2101\n",
      "Epoch 394/1000, Loss: 0.2442\n",
      "Epoch 395/1000, Loss: 0.1354\n",
      "Epoch 396/1000, Loss: 0.1117\n",
      "Epoch 397/1000, Loss: 0.1589\n",
      "Epoch 398/1000, Loss: 0.0820\n",
      "Epoch 399/1000, Loss: 0.3717\n",
      "Epoch 400/1000, Loss: 0.2797\n",
      "Epoch 401/1000, Loss: 0.2838\n",
      "Epoch 402/1000, Loss: 0.1265\n",
      "Epoch 403/1000, Loss: 0.2603\n",
      "Epoch 404/1000, Loss: 0.0503\n",
      "Epoch 405/1000, Loss: 0.2008\n",
      "Epoch 406/1000, Loss: 0.2299\n",
      "Epoch 407/1000, Loss: 0.1553\n",
      "Epoch 408/1000, Loss: 0.1183\n",
      "Epoch 409/1000, Loss: 0.2471\n",
      "Epoch 410/1000, Loss: 0.1246\n",
      "Epoch 411/1000, Loss: 0.1058\n",
      "Epoch 412/1000, Loss: 0.2131\n",
      "Epoch 413/1000, Loss: 0.0708\n",
      "Epoch 414/1000, Loss: 0.1614\n",
      "Epoch 415/1000, Loss: 0.2478\n",
      "Epoch 416/1000, Loss: 0.1389\n",
      "Epoch 417/1000, Loss: 0.1251\n",
      "Epoch 418/1000, Loss: 0.1591\n",
      "Epoch 419/1000, Loss: 0.3372\n",
      "Epoch 420/1000, Loss: 0.1280\n",
      "Epoch 421/1000, Loss: 0.0894\n",
      "Epoch 422/1000, Loss: 0.1399\n",
      "Epoch 423/1000, Loss: 0.1251\n",
      "Epoch 424/1000, Loss: 0.1247\n",
      "Epoch 425/1000, Loss: 0.0647\n",
      "Epoch 426/1000, Loss: 0.2144\n",
      "Epoch 427/1000, Loss: 0.1420\n",
      "Epoch 428/1000, Loss: 0.1277\n",
      "Epoch 429/1000, Loss: 0.1852\n",
      "Epoch 430/1000, Loss: 0.1342\n",
      "Epoch 431/1000, Loss: 0.1090\n",
      "Epoch 432/1000, Loss: 0.1406\n",
      "Epoch 433/1000, Loss: 0.1346\n",
      "Epoch 434/1000, Loss: 0.0922\n",
      "Epoch 435/1000, Loss: 0.1535\n",
      "Epoch 436/1000, Loss: 0.1588\n",
      "Epoch 437/1000, Loss: 0.1550\n",
      "Epoch 438/1000, Loss: 0.1278\n",
      "Epoch 439/1000, Loss: 0.2137\n",
      "Epoch 440/1000, Loss: 0.3227\n",
      "Epoch 441/1000, Loss: 0.2800\n",
      "Epoch 442/1000, Loss: 0.1036\n",
      "Epoch 443/1000, Loss: 0.1303\n",
      "Epoch 444/1000, Loss: 0.0895\n",
      "Epoch 445/1000, Loss: 0.0940\n",
      "Epoch 446/1000, Loss: 0.0925\n",
      "Epoch 447/1000, Loss: 0.1382\n",
      "Epoch 448/1000, Loss: 0.2564\n",
      "Epoch 449/1000, Loss: 0.1160\n",
      "Epoch 450/1000, Loss: 0.3071\n",
      "Epoch 451/1000, Loss: 0.2116\n",
      "Epoch 452/1000, Loss: 0.1837\n",
      "Epoch 453/1000, Loss: 0.0766\n",
      "Epoch 454/1000, Loss: 0.1307\n",
      "Epoch 455/1000, Loss: 0.0773\n",
      "Epoch 456/1000, Loss: 0.1234\n",
      "Epoch 457/1000, Loss: 0.0708\n",
      "Epoch 458/1000, Loss: 0.1955\n",
      "Epoch 459/1000, Loss: 0.1038\n",
      "Epoch 460/1000, Loss: 0.2497\n",
      "Epoch 461/1000, Loss: 0.2131\n",
      "Epoch 462/1000, Loss: 0.1706\n",
      "Epoch 463/1000, Loss: 0.1498\n",
      "Epoch 464/1000, Loss: 0.0679\n",
      "Epoch 465/1000, Loss: 0.0996\n",
      "Epoch 466/1000, Loss: 0.0895\n",
      "Epoch 467/1000, Loss: 0.2084\n",
      "Epoch 468/1000, Loss: 0.2555\n",
      "Epoch 469/1000, Loss: 0.3668\n",
      "Epoch 470/1000, Loss: 0.1144\n",
      "Epoch 471/1000, Loss: 0.1654\n",
      "Epoch 472/1000, Loss: 0.2963\n",
      "Epoch 473/1000, Loss: 0.1489\n",
      "Epoch 474/1000, Loss: 0.0636\n",
      "Epoch 475/1000, Loss: 0.1064\n",
      "Epoch 476/1000, Loss: 0.0932\n",
      "Epoch 477/1000, Loss: 0.0905\n",
      "Epoch 478/1000, Loss: 0.3938\n",
      "Epoch 479/1000, Loss: 0.1352\n",
      "Epoch 480/1000, Loss: 0.1733\n",
      "Epoch 481/1000, Loss: 0.2471\n",
      "Epoch 482/1000, Loss: 0.1253\n",
      "Epoch 483/1000, Loss: 0.1190\n",
      "Epoch 484/1000, Loss: 0.1093\n",
      "Epoch 485/1000, Loss: 0.0780\n",
      "Epoch 486/1000, Loss: 0.0775\n",
      "Epoch 487/1000, Loss: 0.1049\n",
      "Epoch 488/1000, Loss: 0.0944\n",
      "Epoch 489/1000, Loss: 0.1219\n",
      "Epoch 490/1000, Loss: 0.1740\n",
      "Epoch 491/1000, Loss: 0.1243\n",
      "Epoch 492/1000, Loss: 0.1064\n",
      "Epoch 493/1000, Loss: 0.1045\n",
      "Epoch 494/1000, Loss: 0.1669\n",
      "Epoch 495/1000, Loss: 0.0725\n",
      "Epoch 496/1000, Loss: 0.1370\n",
      "Epoch 497/1000, Loss: 0.0750\n",
      "Epoch 498/1000, Loss: 0.0788\n",
      "Epoch 499/1000, Loss: 0.0794\n",
      "Epoch 500/1000, Loss: 0.0769\n",
      "Epoch 501/1000, Loss: 0.0609\n",
      "Epoch 502/1000, Loss: 0.1666\n",
      "Epoch 503/1000, Loss: 0.1090\n",
      "Epoch 504/1000, Loss: 0.0884\n",
      "Epoch 505/1000, Loss: 0.2911\n",
      "Epoch 506/1000, Loss: 0.2562\n",
      "Epoch 507/1000, Loss: 0.0824\n",
      "Epoch 508/1000, Loss: 0.0861\n",
      "Epoch 509/1000, Loss: 0.0859\n",
      "Epoch 510/1000, Loss: 0.0913\n",
      "Epoch 511/1000, Loss: 0.0760\n",
      "Epoch 512/1000, Loss: 0.1143\n",
      "Epoch 513/1000, Loss: 0.0509\n",
      "Epoch 514/1000, Loss: 0.1018\n",
      "Epoch 515/1000, Loss: 0.1063\n",
      "Epoch 516/1000, Loss: 0.0661\n",
      "Epoch 517/1000, Loss: 0.0806\n",
      "Epoch 518/1000, Loss: 0.0979\n",
      "Epoch 519/1000, Loss: 0.0750\n",
      "Epoch 520/1000, Loss: 0.0704\n",
      "Epoch 521/1000, Loss: 0.0928\n",
      "Epoch 522/1000, Loss: 0.0829\n",
      "Epoch 523/1000, Loss: 0.0763\n",
      "Epoch 524/1000, Loss: 0.0640\n",
      "Epoch 525/1000, Loss: 0.0735\n",
      "Epoch 526/1000, Loss: 0.0570\n",
      "Epoch 527/1000, Loss: 0.0767\n",
      "Epoch 528/1000, Loss: 0.0806\n",
      "Epoch 529/1000, Loss: 0.0704\n",
      "Epoch 530/1000, Loss: 0.0847\n",
      "Epoch 531/1000, Loss: 0.1001\n",
      "Epoch 532/1000, Loss: 0.0987\n",
      "Epoch 533/1000, Loss: 0.0442\n",
      "Epoch 534/1000, Loss: 0.0915\n",
      "Epoch 535/1000, Loss: 0.0920\n",
      "Epoch 536/1000, Loss: 0.1138\n",
      "Epoch 537/1000, Loss: 0.0930\n",
      "Epoch 538/1000, Loss: 0.0840\n",
      "Epoch 539/1000, Loss: 0.0714\n",
      "Epoch 540/1000, Loss: 0.1054\n",
      "Epoch 541/1000, Loss: 0.0786\n",
      "Epoch 542/1000, Loss: 0.1393\n",
      "Epoch 543/1000, Loss: 0.0772\n",
      "Epoch 544/1000, Loss: 0.0946\n",
      "Epoch 545/1000, Loss: 0.0772\n",
      "Epoch 546/1000, Loss: 0.1090\n",
      "Epoch 547/1000, Loss: 0.1057\n",
      "Epoch 548/1000, Loss: 0.0741\n",
      "Epoch 549/1000, Loss: 0.1637\n",
      "Epoch 550/1000, Loss: 0.0739\n",
      "Epoch 551/1000, Loss: 0.1205\n",
      "Epoch 552/1000, Loss: 0.1241\n",
      "Epoch 553/1000, Loss: 0.0754\n",
      "Epoch 554/1000, Loss: 0.0811\n",
      "Epoch 555/1000, Loss: 0.1142\n",
      "Epoch 556/1000, Loss: 0.0851\n",
      "Epoch 557/1000, Loss: 0.0717\n",
      "Epoch 558/1000, Loss: 0.0622\n",
      "Epoch 559/1000, Loss: 0.1443\n",
      "Epoch 560/1000, Loss: 0.0966\n",
      "Epoch 561/1000, Loss: 0.0652\n",
      "Epoch 562/1000, Loss: 0.0873\n",
      "Epoch 563/1000, Loss: 0.0902\n",
      "Epoch 564/1000, Loss: 0.0939\n",
      "Epoch 565/1000, Loss: 0.0985\n",
      "Epoch 566/1000, Loss: 0.0689\n",
      "Epoch 567/1000, Loss: 0.0722\n",
      "Epoch 568/1000, Loss: 0.0613\n",
      "Epoch 569/1000, Loss: 0.0680\n",
      "Epoch 570/1000, Loss: 0.0751\n",
      "Epoch 571/1000, Loss: 0.0779\n",
      "Epoch 572/1000, Loss: 0.0700\n",
      "Epoch 573/1000, Loss: 0.0726\n",
      "Epoch 574/1000, Loss: 0.0719\n",
      "Epoch 575/1000, Loss: 0.0740\n",
      "Epoch 576/1000, Loss: 0.1028\n",
      "Epoch 577/1000, Loss: 0.0629\n",
      "Epoch 578/1000, Loss: 0.0599\n",
      "Epoch 579/1000, Loss: 0.1320\n",
      "Epoch 580/1000, Loss: 0.0899\n",
      "Epoch 581/1000, Loss: 0.0659\n",
      "Epoch 582/1000, Loss: 0.0889\n",
      "Epoch 583/1000, Loss: 0.0560\n",
      "Epoch 584/1000, Loss: 0.2764\n",
      "Epoch 585/1000, Loss: 0.0704\n",
      "Epoch 586/1000, Loss: 0.0739\n",
      "Epoch 587/1000, Loss: 0.0752\n",
      "Epoch 588/1000, Loss: 0.0688\n",
      "Epoch 589/1000, Loss: 0.1560\n",
      "Epoch 590/1000, Loss: 0.0556\n",
      "Epoch 591/1000, Loss: 0.0846\n",
      "Epoch 592/1000, Loss: 0.0562\n",
      "Epoch 593/1000, Loss: 0.0825\n",
      "Epoch 594/1000, Loss: 0.0899\n",
      "Epoch 595/1000, Loss: 0.0582\n",
      "Epoch 596/1000, Loss: 0.0957\n",
      "Epoch 597/1000, Loss: 0.0690\n",
      "Epoch 598/1000, Loss: 0.0594\n",
      "Epoch 599/1000, Loss: 0.0808\n",
      "Epoch 600/1000, Loss: 0.0665\n",
      "Epoch 601/1000, Loss: 0.0636\n",
      "Epoch 602/1000, Loss: 0.0654\n",
      "Epoch 603/1000, Loss: 0.0893\n",
      "Epoch 604/1000, Loss: 0.1204\n",
      "Epoch 605/1000, Loss: 0.0855\n",
      "Epoch 606/1000, Loss: 0.0622\n",
      "Epoch 607/1000, Loss: 0.0773\n",
      "Epoch 608/1000, Loss: 0.0860\n",
      "Epoch 609/1000, Loss: 0.0627\n",
      "Epoch 610/1000, Loss: 0.0964\n",
      "Epoch 611/1000, Loss: 0.0955\n",
      "Epoch 612/1000, Loss: 0.0675\n",
      "Epoch 613/1000, Loss: 0.0569\n",
      "Epoch 614/1000, Loss: 0.1906\n",
      "Epoch 615/1000, Loss: 0.0898\n",
      "Epoch 616/1000, Loss: 0.0698\n",
      "Epoch 617/1000, Loss: 0.0618\n",
      "Epoch 618/1000, Loss: 0.0664\n",
      "Epoch 619/1000, Loss: 0.0836\n",
      "Epoch 620/1000, Loss: 0.0792\n",
      "Epoch 621/1000, Loss: 0.0597\n",
      "Epoch 622/1000, Loss: 0.4115\n",
      "Epoch 623/1000, Loss: 0.0586\n",
      "Epoch 624/1000, Loss: 0.0868\n",
      "Epoch 625/1000, Loss: 0.0673\n",
      "Epoch 626/1000, Loss: 0.2063\n",
      "Epoch 627/1000, Loss: 0.0612\n",
      "Epoch 628/1000, Loss: 0.2780\n",
      "Epoch 629/1000, Loss: 0.0565\n",
      "Epoch 630/1000, Loss: 0.0877\n",
      "Epoch 631/1000, Loss: 0.0776\n",
      "Epoch 632/1000, Loss: 0.0648\n",
      "Epoch 633/1000, Loss: 0.0773\n",
      "Epoch 634/1000, Loss: 0.0836\n",
      "Epoch 635/1000, Loss: 0.0625\n",
      "Epoch 636/1000, Loss: 0.0864\n",
      "Epoch 637/1000, Loss: 0.0690\n",
      "Epoch 638/1000, Loss: 0.0786\n",
      "Epoch 639/1000, Loss: 0.0959\n",
      "Epoch 640/1000, Loss: 0.0803\n",
      "Epoch 641/1000, Loss: 0.0817\n",
      "Epoch 642/1000, Loss: 0.0974\n",
      "Epoch 643/1000, Loss: 0.0869\n",
      "Epoch 644/1000, Loss: 0.1363\n",
      "Epoch 645/1000, Loss: 0.0847\n",
      "Epoch 646/1000, Loss: 0.0746\n",
      "Epoch 647/1000, Loss: 0.0750\n",
      "Epoch 648/1000, Loss: 0.0473\n",
      "Epoch 649/1000, Loss: 0.2196\n",
      "Epoch 650/1000, Loss: 0.0732\n",
      "Epoch 651/1000, Loss: 0.0679\n",
      "Epoch 652/1000, Loss: 0.0738\n",
      "Epoch 653/1000, Loss: 0.0748\n",
      "Epoch 654/1000, Loss: 0.0648\n",
      "Epoch 655/1000, Loss: 0.1013\n",
      "Epoch 656/1000, Loss: 0.0713\n",
      "Epoch 657/1000, Loss: 0.0656\n",
      "Epoch 658/1000, Loss: 0.0678\n",
      "Epoch 659/1000, Loss: 0.0820\n",
      "Epoch 660/1000, Loss: 0.1846\n",
      "Epoch 661/1000, Loss: 0.0777\n",
      "Epoch 662/1000, Loss: 0.0900\n",
      "Epoch 663/1000, Loss: 0.0694\n",
      "Epoch 664/1000, Loss: 0.0809\n",
      "Epoch 665/1000, Loss: 0.0873\n",
      "Epoch 666/1000, Loss: 0.0820\n",
      "Epoch 667/1000, Loss: 0.0723\n",
      "Epoch 668/1000, Loss: 0.1197\n",
      "Epoch 669/1000, Loss: 0.0708\n",
      "Epoch 670/1000, Loss: 0.1197\n",
      "Epoch 671/1000, Loss: 0.0612\n",
      "Epoch 672/1000, Loss: 0.1448\n",
      "Epoch 673/1000, Loss: 0.0651\n",
      "Epoch 674/1000, Loss: 0.0678\n",
      "Epoch 675/1000, Loss: 0.0612\n",
      "Epoch 676/1000, Loss: 0.0744\n",
      "Epoch 677/1000, Loss: 0.0853\n",
      "Epoch 678/1000, Loss: 0.0659\n",
      "Epoch 679/1000, Loss: 0.0635\n",
      "Epoch 680/1000, Loss: 0.0820\n",
      "Epoch 681/1000, Loss: 0.0574\n",
      "Epoch 682/1000, Loss: 0.1085\n",
      "Epoch 683/1000, Loss: 0.0504\n",
      "Epoch 684/1000, Loss: 0.0622\n",
      "Epoch 685/1000, Loss: 0.0981\n",
      "Epoch 686/1000, Loss: 0.1786\n",
      "Epoch 687/1000, Loss: 0.0528\n",
      "Epoch 688/1000, Loss: 0.0584\n",
      "Epoch 689/1000, Loss: 0.0661\n",
      "Epoch 690/1000, Loss: 0.1257\n",
      "Epoch 691/1000, Loss: 0.0619\n",
      "Epoch 692/1000, Loss: 0.1140\n",
      "Epoch 693/1000, Loss: 0.0982\n",
      "Epoch 694/1000, Loss: 0.1420\n",
      "Epoch 695/1000, Loss: 0.0695\n",
      "Epoch 696/1000, Loss: 0.1080\n",
      "Epoch 697/1000, Loss: 0.0918\n",
      "Epoch 698/1000, Loss: 0.0926\n",
      "Epoch 699/1000, Loss: 0.0674\n",
      "Epoch 700/1000, Loss: 0.0641\n",
      "Epoch 701/1000, Loss: 0.0609\n",
      "Epoch 702/1000, Loss: 0.1012\n",
      "Epoch 703/1000, Loss: 0.1715\n",
      "Epoch 704/1000, Loss: 0.1119\n",
      "Epoch 705/1000, Loss: 0.0962\n",
      "Epoch 706/1000, Loss: 0.0735\n",
      "Epoch 707/1000, Loss: 0.0985\n",
      "Epoch 708/1000, Loss: 0.1804\n",
      "Epoch 709/1000, Loss: 0.0766\n",
      "Epoch 710/1000, Loss: 0.0693\n",
      "Epoch 711/1000, Loss: 0.3082\n",
      "Epoch 712/1000, Loss: 0.0563\n",
      "Epoch 713/1000, Loss: 0.0868\n",
      "Epoch 714/1000, Loss: 0.1193\n",
      "Epoch 715/1000, Loss: 0.1250\n",
      "Epoch 716/1000, Loss: 0.1008\n",
      "Epoch 717/1000, Loss: 0.1179\n",
      "Epoch 718/1000, Loss: 0.0749\n",
      "Epoch 719/1000, Loss: 0.1114\n",
      "Epoch 720/1000, Loss: 0.0716\n",
      "Epoch 721/1000, Loss: 0.1179\n",
      "Epoch 722/1000, Loss: 0.0817\n",
      "Epoch 723/1000, Loss: 0.0735\n",
      "Epoch 724/1000, Loss: 0.0997\n",
      "Epoch 725/1000, Loss: 0.0615\n",
      "Epoch 726/1000, Loss: 0.0703\n",
      "Epoch 727/1000, Loss: 0.0909\n",
      "Epoch 728/1000, Loss: 0.1569\n",
      "Epoch 729/1000, Loss: 0.0537\n",
      "Epoch 730/1000, Loss: 0.0843\n",
      "Epoch 731/1000, Loss: 0.0580\n",
      "Epoch 732/1000, Loss: 0.0614\n",
      "Epoch 733/1000, Loss: 0.1036\n",
      "Epoch 734/1000, Loss: 0.1097\n",
      "Epoch 735/1000, Loss: 0.0587\n",
      "Epoch 736/1000, Loss: 0.0858\n",
      "Epoch 737/1000, Loss: 0.0781\n",
      "Epoch 738/1000, Loss: 0.1440\n",
      "Epoch 739/1000, Loss: 0.0591\n",
      "Epoch 740/1000, Loss: 0.0688\n",
      "Epoch 741/1000, Loss: 0.0682\n",
      "Epoch 742/1000, Loss: 0.0865\n",
      "Epoch 743/1000, Loss: 0.0828\n",
      "Epoch 744/1000, Loss: 0.0731\n",
      "Epoch 745/1000, Loss: 0.0812\n",
      "Epoch 746/1000, Loss: 0.0700\n",
      "Epoch 747/1000, Loss: 0.0931\n",
      "Epoch 748/1000, Loss: 0.1016\n",
      "Epoch 749/1000, Loss: 0.1991\n",
      "Epoch 750/1000, Loss: 0.1100\n",
      "Epoch 751/1000, Loss: 0.0717\n",
      "Epoch 752/1000, Loss: 0.0728\n",
      "Epoch 753/1000, Loss: 0.0953\n",
      "Epoch 754/1000, Loss: 0.0917\n",
      "Epoch 755/1000, Loss: 0.0895\n",
      "Epoch 756/1000, Loss: 0.1006\n",
      "Epoch 757/1000, Loss: 0.1352\n",
      "Epoch 758/1000, Loss: 0.0944\n",
      "Epoch 759/1000, Loss: 0.0815\n",
      "Epoch 760/1000, Loss: 0.0649\n",
      "Epoch 761/1000, Loss: 0.0802\n",
      "Epoch 762/1000, Loss: 0.0621\n",
      "Epoch 763/1000, Loss: 0.2243\n",
      "Epoch 764/1000, Loss: 0.1192\n",
      "Epoch 765/1000, Loss: 0.0788\n",
      "Epoch 766/1000, Loss: 0.0720\n",
      "Epoch 767/1000, Loss: 0.0676\n",
      "Epoch 768/1000, Loss: 0.1120\n",
      "Epoch 769/1000, Loss: 0.0860\n",
      "Epoch 770/1000, Loss: 0.0871\n",
      "Epoch 771/1000, Loss: 0.0691\n",
      "Epoch 772/1000, Loss: 0.0771\n",
      "Epoch 773/1000, Loss: 0.0730\n",
      "Epoch 774/1000, Loss: 0.0802\n",
      "Epoch 775/1000, Loss: 0.0776\n",
      "Epoch 776/1000, Loss: 0.0723\n",
      "Epoch 777/1000, Loss: 0.1359\n",
      "Epoch 778/1000, Loss: 0.1208\n",
      "Epoch 779/1000, Loss: 0.0849\n",
      "Epoch 780/1000, Loss: 0.0870\n",
      "Epoch 781/1000, Loss: 0.0856\n",
      "Epoch 782/1000, Loss: 0.0783\n",
      "Epoch 783/1000, Loss: 0.0675\n",
      "Epoch 784/1000, Loss: 0.0753\n",
      "Epoch 785/1000, Loss: 0.0613\n",
      "Epoch 786/1000, Loss: 0.0523\n",
      "Epoch 787/1000, Loss: 0.0707\n",
      "Epoch 788/1000, Loss: 0.2831\n",
      "Epoch 789/1000, Loss: 0.0711\n",
      "Epoch 790/1000, Loss: 0.0851\n",
      "Epoch 791/1000, Loss: 0.0697\n",
      "Epoch 792/1000, Loss: 0.0744\n",
      "Epoch 793/1000, Loss: 0.0687\n",
      "Epoch 794/1000, Loss: 0.0653\n",
      "Epoch 795/1000, Loss: 0.0681\n",
      "Epoch 796/1000, Loss: 0.0634\n",
      "Epoch 797/1000, Loss: 0.0695\n",
      "Epoch 798/1000, Loss: 0.0804\n",
      "Epoch 799/1000, Loss: 0.0728\n",
      "Epoch 800/1000, Loss: 0.1145\n",
      "Epoch 801/1000, Loss: 0.0679\n",
      "Epoch 802/1000, Loss: 0.0762\n",
      "Epoch 803/1000, Loss: 0.0815\n",
      "Epoch 804/1000, Loss: 0.0678\n",
      "Epoch 805/1000, Loss: 0.0965\n",
      "Epoch 806/1000, Loss: 0.0726\n",
      "Epoch 807/1000, Loss: 0.0656\n",
      "Epoch 808/1000, Loss: 0.0643\n",
      "Epoch 809/1000, Loss: 0.0863\n",
      "Epoch 810/1000, Loss: 0.2125\n",
      "Epoch 811/1000, Loss: 0.0785\n",
      "Epoch 812/1000, Loss: 0.0642\n",
      "Epoch 813/1000, Loss: 0.0632\n",
      "Epoch 814/1000, Loss: 0.1393\n",
      "Epoch 815/1000, Loss: 0.0765\n",
      "Epoch 816/1000, Loss: 0.0817\n",
      "Epoch 817/1000, Loss: 0.0622\n",
      "Epoch 818/1000, Loss: 0.0474\n",
      "Epoch 819/1000, Loss: 0.0696\n",
      "Epoch 820/1000, Loss: 0.0487\n",
      "Epoch 821/1000, Loss: 0.1495\n",
      "Epoch 822/1000, Loss: 0.1277\n",
      "Epoch 823/1000, Loss: 0.0816\n",
      "Epoch 824/1000, Loss: 0.0623\n",
      "Epoch 825/1000, Loss: 0.0891\n",
      "Epoch 826/1000, Loss: 0.1573\n",
      "Epoch 827/1000, Loss: 0.0862\n",
      "Epoch 828/1000, Loss: 0.1149\n",
      "Epoch 829/1000, Loss: 0.0618\n",
      "Epoch 830/1000, Loss: 0.0639\n",
      "Epoch 831/1000, Loss: 0.1133\n",
      "Epoch 832/1000, Loss: 0.0846\n",
      "Epoch 833/1000, Loss: 0.0712\n",
      "Epoch 834/1000, Loss: 0.0663\n",
      "Epoch 835/1000, Loss: 0.0678\n",
      "Epoch 836/1000, Loss: 0.1186\n",
      "Epoch 837/1000, Loss: 0.0939\n",
      "Epoch 838/1000, Loss: 0.0743\n",
      "Epoch 839/1000, Loss: 0.1283\n",
      "Epoch 840/1000, Loss: 0.0680\n",
      "Epoch 841/1000, Loss: 0.1027\n",
      "Epoch 842/1000, Loss: 0.0668\n",
      "Epoch 843/1000, Loss: 0.0964\n",
      "Epoch 844/1000, Loss: 0.0726\n",
      "Epoch 845/1000, Loss: 0.0667\n",
      "Epoch 846/1000, Loss: 0.0950\n",
      "Epoch 847/1000, Loss: 0.0732\n",
      "Epoch 848/1000, Loss: 0.0905\n",
      "Epoch 849/1000, Loss: 0.0535\n",
      "Epoch 850/1000, Loss: 0.1996\n",
      "Epoch 851/1000, Loss: 0.1234\n",
      "Epoch 852/1000, Loss: 0.1719\n",
      "Epoch 853/1000, Loss: 0.0762\n",
      "Epoch 854/1000, Loss: 0.0684\n",
      "Epoch 855/1000, Loss: 0.0921\n",
      "Epoch 856/1000, Loss: 0.0825\n",
      "Epoch 857/1000, Loss: 0.0840\n",
      "Epoch 858/1000, Loss: 0.1283\n",
      "Epoch 859/1000, Loss: 0.2051\n",
      "Epoch 860/1000, Loss: 0.0627\n",
      "Epoch 861/1000, Loss: 0.1313\n",
      "Epoch 862/1000, Loss: 0.1785\n",
      "Epoch 863/1000, Loss: 0.4461\n",
      "Epoch 864/1000, Loss: 0.0784\n",
      "Epoch 865/1000, Loss: 0.1291\n",
      "Epoch 866/1000, Loss: 0.0745\n",
      "Epoch 867/1000, Loss: 0.0698\n",
      "Epoch 868/1000, Loss: 0.2302\n",
      "Epoch 869/1000, Loss: 0.2743\n",
      "Epoch 870/1000, Loss: 0.1959\n",
      "Epoch 871/1000, Loss: 0.0870\n",
      "Epoch 872/1000, Loss: 0.0838\n",
      "Epoch 873/1000, Loss: 0.1204\n",
      "Epoch 874/1000, Loss: 0.2676\n",
      "Epoch 875/1000, Loss: 0.1550\n",
      "Epoch 876/1000, Loss: 0.1007\n",
      "Epoch 877/1000, Loss: 0.2376\n",
      "Epoch 878/1000, Loss: 0.1646\n",
      "Epoch 879/1000, Loss: 0.2258\n",
      "Epoch 880/1000, Loss: 0.0630\n",
      "Epoch 881/1000, Loss: 0.1187\n",
      "Epoch 882/1000, Loss: 0.0627\n",
      "Epoch 883/1000, Loss: 0.1231\n",
      "Epoch 884/1000, Loss: 0.2546\n",
      "Epoch 885/1000, Loss: 0.1475\n",
      "Epoch 886/1000, Loss: 0.0799\n",
      "Epoch 887/1000, Loss: 0.1229\n",
      "Epoch 888/1000, Loss: 0.0881\n",
      "Epoch 889/1000, Loss: 0.1226\n",
      "Epoch 890/1000, Loss: 0.1305\n",
      "Epoch 891/1000, Loss: 0.0847\n",
      "Epoch 892/1000, Loss: 0.0701\n",
      "Epoch 893/1000, Loss: 0.0638\n",
      "Epoch 894/1000, Loss: 0.5054\n",
      "Epoch 895/1000, Loss: 0.1489\n",
      "Epoch 896/1000, Loss: 0.1142\n",
      "Epoch 897/1000, Loss: 0.1410\n",
      "Epoch 898/1000, Loss: 0.0812\n",
      "Epoch 899/1000, Loss: 0.2324\n",
      "Epoch 900/1000, Loss: 0.1060\n",
      "Epoch 901/1000, Loss: 0.3113\n",
      "Epoch 902/1000, Loss: 0.2315\n",
      "Epoch 903/1000, Loss: 0.1247\n",
      "Epoch 904/1000, Loss: 0.0779\n",
      "Epoch 905/1000, Loss: 0.0853\n",
      "Epoch 906/1000, Loss: 0.1427\n",
      "Epoch 907/1000, Loss: 0.1584\n",
      "Epoch 908/1000, Loss: 0.3059\n",
      "Epoch 909/1000, Loss: 0.1155\n",
      "Epoch 910/1000, Loss: 0.0751\n",
      "Epoch 911/1000, Loss: 0.0818\n",
      "Epoch 912/1000, Loss: 0.1131\n",
      "Epoch 913/1000, Loss: 0.0944\n",
      "Epoch 914/1000, Loss: 0.0993\n",
      "Epoch 915/1000, Loss: 0.3604\n",
      "Epoch 916/1000, Loss: 0.2284\n",
      "Epoch 917/1000, Loss: 0.1088\n",
      "Epoch 918/1000, Loss: 0.0887\n",
      "Epoch 919/1000, Loss: 0.1134\n",
      "Epoch 920/1000, Loss: 0.1892\n",
      "Epoch 921/1000, Loss: 0.1050\n",
      "Epoch 922/1000, Loss: 0.1287\n",
      "Epoch 923/1000, Loss: 0.0788\n",
      "Epoch 924/1000, Loss: 0.1648\n",
      "Epoch 925/1000, Loss: 0.0817\n",
      "Epoch 926/1000, Loss: 0.0513\n",
      "Epoch 927/1000, Loss: 0.1290\n",
      "Epoch 928/1000, Loss: 0.0659\n",
      "Epoch 929/1000, Loss: 0.3187\n",
      "Epoch 930/1000, Loss: 0.2009\n",
      "Epoch 931/1000, Loss: 0.1598\n",
      "Epoch 932/1000, Loss: 0.0844\n",
      "Epoch 933/1000, Loss: 0.0692\n",
      "Epoch 934/1000, Loss: 0.1058\n",
      "Epoch 935/1000, Loss: 0.1426\n",
      "Epoch 936/1000, Loss: 0.0886\n",
      "Epoch 937/1000, Loss: 0.1194\n",
      "Epoch 938/1000, Loss: 0.0937\n",
      "Epoch 939/1000, Loss: 0.3072\n",
      "Epoch 940/1000, Loss: 0.1020\n",
      "Epoch 941/1000, Loss: 0.0893\n",
      "Epoch 942/1000, Loss: 0.1054\n",
      "Epoch 943/1000, Loss: 0.1338\n",
      "Epoch 944/1000, Loss: 0.1914\n",
      "Epoch 945/1000, Loss: 0.3010\n",
      "Epoch 946/1000, Loss: 0.2795\n",
      "Epoch 947/1000, Loss: 0.2572\n",
      "Epoch 948/1000, Loss: 0.3123\n",
      "Epoch 949/1000, Loss: 0.1142\n",
      "Epoch 950/1000, Loss: 0.1223\n",
      "Epoch 951/1000, Loss: 0.0700\n",
      "Epoch 952/1000, Loss: 0.2362\n",
      "Epoch 953/1000, Loss: 0.2483\n",
      "Epoch 954/1000, Loss: 0.4565\n",
      "Epoch 955/1000, Loss: 0.1836\n",
      "Epoch 956/1000, Loss: 0.1748\n",
      "Epoch 957/1000, Loss: 0.0724\n",
      "Epoch 958/1000, Loss: 0.1130\n",
      "Epoch 959/1000, Loss: 0.2782\n",
      "Epoch 960/1000, Loss: 0.1011\n",
      "Epoch 961/1000, Loss: 0.3110\n",
      "Epoch 962/1000, Loss: 0.2107\n",
      "Epoch 963/1000, Loss: 0.1688\n",
      "Epoch 964/1000, Loss: 0.2092\n",
      "Epoch 965/1000, Loss: 0.3780\n",
      "Epoch 966/1000, Loss: 0.5621\n",
      "Epoch 967/1000, Loss: 0.1725\n",
      "Epoch 968/1000, Loss: 0.3140\n",
      "Epoch 969/1000, Loss: 0.2279\n",
      "Epoch 970/1000, Loss: 0.4951\n",
      "Epoch 971/1000, Loss: 0.2443\n",
      "Epoch 972/1000, Loss: 0.1139\n",
      "Epoch 973/1000, Loss: 0.1517\n",
      "Epoch 974/1000, Loss: 0.2327\n",
      "Epoch 975/1000, Loss: 0.2290\n",
      "Epoch 976/1000, Loss: 0.3451\n",
      "Epoch 977/1000, Loss: 0.1110\n",
      "Epoch 978/1000, Loss: 0.3102\n",
      "Epoch 979/1000, Loss: 0.0909\n",
      "Epoch 980/1000, Loss: 0.2514\n",
      "Epoch 981/1000, Loss: 0.2654\n",
      "Epoch 982/1000, Loss: 0.1126\n",
      "Epoch 983/1000, Loss: 0.0857\n",
      "Epoch 984/1000, Loss: 0.1836\n",
      "Epoch 985/1000, Loss: 0.1887\n",
      "Epoch 986/1000, Loss: 0.1884\n",
      "Epoch 987/1000, Loss: 0.3379\n",
      "Epoch 988/1000, Loss: 0.1916\n",
      "Epoch 989/1000, Loss: 0.1813\n",
      "Epoch 990/1000, Loss: 0.1187\n",
      "Epoch 991/1000, Loss: 0.3080\n",
      "Epoch 992/1000, Loss: 0.4813\n",
      "Epoch 993/1000, Loss: 0.2910\n",
      "Epoch 994/1000, Loss: 0.1119\n",
      "Epoch 995/1000, Loss: 0.1894\n",
      "Epoch 996/1000, Loss: 0.3979\n",
      "Epoch 997/1000, Loss: 0.2850\n",
      "Epoch 998/1000, Loss: 0.1525\n",
      "Epoch 999/1000, Loss: 0.1046\n",
      "Epoch 1000/1000, Loss: 0.1692\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Step 1: Prepare the Dataset\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, questions, answers, seq_length, tokenizer):\n",
    "        self.seq_length = seq_length\n",
    "        self.tokenizer = tokenizer\n",
    "        self.questions = questions\n",
    "        self.answers = answers\n",
    "        self.vocab_size = tokenizer.vocab_size\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.questions)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        question = self.questions[idx]\n",
    "        answer = self.answers[idx]\n",
    "\n",
    "        sample = 'question: '+ question + '\\nanswer: ' + answer + tokenizer.eos_token\n",
    "        tokenized_output = tokenizer(sample, max_length=self.seq_length+1, padding='max_length',return_tensors='pt')\n",
    "        tokenized_text,token_mask = tokenized_output.input_ids, tokenized_output.attention_mask\n",
    "\n",
    "        target = tokenized_text[:,1:].masked_fill(token_mask[:,1:].ne(1),-100)\n",
    "        input_sequence = tokenized_text[:,:-1]\n",
    "        return input_sequence.squeeze(), target.squeeze()\n",
    "\n",
    "# Step 2: Define the Model\n",
    "class TransformerLanguageModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, num_heads, hidden_dim, seq_length):\n",
    "        super(TransformerLanguageModel, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
    "        self.positional_encoding = nn.Parameter(torch.zeros(1, seq_length, embed_size))\n",
    "\n",
    "        self.encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=embed_size,\n",
    "            nhead=num_heads,\n",
    "            dim_feedforward=hidden_dim,\n",
    "            batch_first=True\n",
    "        ) \n",
    "        self.fc = nn.Linear(embed_size, vocab_size)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        seq_length = x.size(1)\n",
    "        x = self.embedding(x) + self.positional_encoding[:, :seq_length, :]\n",
    "        x = self.encoder_layer(x, src_mask=mask)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "# Step 3: Training\n",
    "\n",
    "# Hyperparameters\n",
    "questions = [\"What is AI?\", \"How does deep learning work?\", \"What is NLP?\"]\n",
    "answers = [\"AI stands for Artificial Intelligence.\", \"Deep learning uses neural networks.\", \"NLP stands for Natural Language Processing.\"]\n",
    "seq_length = 30\n",
    "embed_size = 64\n",
    "num_heads = 4\n",
    "hidden_dim = 128\n",
    "num_epochs = 1000\n",
    "batch_size = 4\n",
    "learning_rate = 0.01\n",
    "\n",
    "# Initialize tokenizer\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Prepare data\n",
    "dataset = TextDataset(questions, answers, seq_length, tokenizer)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Model, loss, optimizer\n",
    "model = TransformerLanguageModel(\n",
    "    vocab_size=dataset.vocab_size,\n",
    "    embed_size=embed_size,\n",
    "    num_heads=num_heads,\n",
    "    hidden_dim=hidden_dim,\n",
    "    seq_length=seq_length\n",
    ").to('cuda')\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = []\n",
    "    for question, answer in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        mask = torch.triu(torch.ones(question.shape[-1],question.shape[-1]) * float('-inf'), diagonal=1)\n",
    "        output = model(question.to('cuda'),mask.to('cuda'))\n",
    "        loss = criterion(output.view(-1, dataset.vocab_size), answer.view(-1).to('cuda'))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss.append(loss.item())\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {np.mean(running_loss[-10:]):.4f}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** reached end of the sentence ***\n",
      "question: What is NLP?\n",
      "answer: NLP stands for Natural Language Processing.<|endoftext|>\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "\n",
    "sample = 'question: '+ questions[2] + '\\nanswer:'\n",
    "input_sequence = tokenizer(sample,return_tensors='pt').input_ids\n",
    "\n",
    "\n",
    "generated_text = input_sequence.clone().to('cuda')\n",
    "\n",
    "for i in range(20):\n",
    "    with torch.no_grad():\n",
    "        output = model(generated_text[:,-30:].to('cuda'))\n",
    "    generated_text = torch.concat((generated_text,output.argmax(-1)[:,-1:]),dim=1)\n",
    "    if output.argmax(-1)[:,-1:].squeeze().item() == 50256:\n",
    "        print('*** reached end of the sentence ***')\n",
    "        break\n",
    "print(tokenizer.batch_decode(generated_text)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' AI'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(output.argmax(-1)[:,-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['question: What is NLP?\\n answer: ']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.batch_decode(input_sequence)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
